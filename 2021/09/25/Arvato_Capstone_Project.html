<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="twitter:card" content="summary_large_image" /><!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Bertelsmann Arvato Capstone Project | ahmedsamirio</title>
<meta name="generator" content="Jekyll v4.1.1" />
<meta property="og:title" content="Bertelsmann Arvato Capstone Project" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="blog" />
<meta property="og:description" content="blog" />
<link rel="canonical" href="https://ahmedsamirio.github.io/blog/2021/09/25/Arvato_Capstone_Project.html" />
<meta property="og:url" content="https://ahmedsamirio.github.io/blog/2021/09/25/Arvato_Capstone_Project.html" />
<meta property="og:site_name" content="ahmedsamirio" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2021-09-25T00:00:00-05:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Bertelsmann Arvato Capstone Project" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","dateModified":"2021-09-25T00:00:00-05:00","datePublished":"2021-09-25T00:00:00-05:00","description":"blog","headline":"Bertelsmann Arvato Capstone Project","mainEntityOfPage":{"@type":"WebPage","@id":"https://ahmedsamirio.github.io/blog/2021/09/25/Arvato_Capstone_Project.html"},"url":"https://ahmedsamirio.github.io/blog/2021/09/25/Arvato_Capstone_Project.html"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/blog/assets/css/style.css"><link type="application/atom+xml" rel="alternate" href="https://ahmedsamirio.github.io/blog/feed.xml" title="ahmedsamirio" /><link rel="shortcut icon" type="image/x-icon" href="/blog/images/favicon.ico"><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/Primer/15.2.0/primer.css" integrity="sha512-xTz2ys4coGAOz8vuV1NcQBkgVmKhsSEtjbqyMJbBHRplFuvKIUo6xhLHpAyPt9mfR6twHJgn9OgVLuqOvjeBhg==" crossorigin="anonymous" />
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css" integrity="sha512-1PKOgIY59xJ8Co8+NE6FZ+LOAZKjy+KY8iq0G4B3CyeY6wYHN3yt9PW0XpSriVlkMXe40PTKnXrLnZ9+fkDaog==" crossorigin="anonymous" />

<script>
function wrap_img(fn) {
    if (document.attachEvent ? document.readyState === "complete" : document.readyState !== "loading") {
        var elements = document.querySelectorAll(".post img");
        Array.prototype.forEach.call(elements, function(el, i) {
            if (el.getAttribute("title") && (el.className != "emoji")) {
                const caption = document.createElement('figcaption');
                var node = document.createTextNode(el.getAttribute("title"));
                caption.appendChild(node);
                const wrapper = document.createElement('figure');
                wrapper.className = 'image';
                el.parentNode.insertBefore(wrapper, el);
                el.parentNode.removeChild(el);
                wrapper.appendChild(el);
                wrapper.appendChild(caption);
            }
        });
    } else { document.addEventListener('DOMContentLoaded', fn); }
}
window.onload = wrap_img;
</script>

<script>
    document.addEventListener("DOMContentLoaded", function(){
    // add link icon to anchor tags
    var elem = document.querySelectorAll(".anchor-link")
    elem.forEach(e => (e.innerHTML = '<i class="fas fa-link fa-xs"></i>'));
    });
</script>
</head>
<body><header class="site-header">

  <div class="wrapper"><a class="site-title" rel="author" href="/blog/">ahmedsamirio</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/blog/about/">About Me</a><a class="page-link" href="/blog/search/">Search</a><a class="page-link" href="/blog/categories/">Tags</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Bertelsmann Arvato Capstone Project</h1><p class="post-meta post-meta-title"><time class="dt-published" datetime="2021-09-25T00:00:00-05:00" itemprop="datePublished">
        Sep 25, 2021
      </time>
       • <span class="read-time" title="Estimated read time">
    
    
      67 min read
    
</span></p>

    

    </header>

  <div class="post-content e-content" itemprop="articleBody">
    <p><img src="/images/arvato/header.jpeg" alt="Header Image" /></p>

<p>First of all, I’m really happy to be writing this blog post as this signals the end of my Data Scientist Nano-degree journey generally, and the capstone project specifically.</p>

<p>I’d like to thank Udacity for providing us with this amazing dataset, which enabled me to test my abilities and to really learn some things that couldn’t be learned through lessons alone.</p>

<h1 id="projet-overview">Projet Overview</h1>

<p>In this project, I role-played that I was a Data Scientist at Bertelsmann Arvato Analytics, where I was handed over a project where the stakeholder was a mail-order sales company in Germany.</p>

<p>According to britannica, a mail order business is a</p>

<p>“method of merchandising in which the seller’s offer is made through mass mailing of a circular or catalog or through an advertisement placed in a newspaper or magazine and in which the buyer places an order by mail.”</p>

<p>And this stakeholder wanted two things from us:</p>

<ol>
  <li>First it wanted to understand which parts of the general population described the core customer base of the company.</li>
  <li>Second it wanted a model which predicts which individuals are most likely to convert new campaigns.</li>
</ol>

<p>The datasets that were provided with this project were:</p>

<ol>
  <li>A dataset which holds demographic information about the general population in Germany.</li>
  <li>A dataset which holds demographic information about the customer base of the stakeholder.</li>
  <li>A dataset which has the demographic information about the individuals involved in their campaigning, and whether they converted or not.</li>
</ol>

<p>The datasets that we had were more than enough to begin addressing the requests of the stakeholder, but before we got to do these tasks we needed do some:</p>

<ol>
  <li>Data Exploration</li>
  <li>Data Cleaning</li>
  <li>
    <p>The datasets that were provided were:</p>
  </li>
  <li><code class="language-plaintext highlighter-rouge">Udacity_AZDIAS_052018.csv</code>: Demographics data for the general population of Germany; 891 211 persons (rows) x 366 features (columns).</li>
  <li><code class="language-plaintext highlighter-rouge">Udacity_CUSTOMERS_052018.csv</code>: Demographics data for customers of a mail-order company; 191 652 persons (rows) x 369 features (columns).</li>
  <li><code class="language-plaintext highlighter-rouge">Udacity_MAILOUT_052018_TRAIN.csv</code>: Demographics data for individuals who were targets of a marketing campaign; 42 982 persons (rows) x 367 (columns).</li>
  <li><code class="language-plaintext highlighter-rouge">Udacity_MAILOUT_052018_TEST.csv</code>: Demographics data for individuals who were targets of a marketing campaign; 42 833 persons (rows) x 366 (columns).</li>
  <li><code class="language-plaintext highlighter-rouge">DIAS Information Levels - Attributes 2017.xlsx</code>: Information about features in demographic data.</li>
  <li><code class="language-plaintext highlighter-rouge">DIAS Attributes - Values 2017.xlsx</code>: Information about values of each feature in demographic data.</li>
</ol>

<h1 id="problem-statement">Problem Statement</h1>

<ol>
  <li>We need to find the segments in the general population which constitute the core customer base of the business.</li>
  <li>We need find out which targeted individuals in the new mailout campaign are more likely to convert.</li>
</ol>

<h2 id="how-can-we-compare-the-populations-of-customers-with-the-general-population">How can we compare the populations of customers with the general population?</h2>
<ol>
  <li>First we shall explore the data to get to know the features within it.</li>
  <li>Then we shall segment the general population.</li>
  <li>Finally we shall figure out which segments do our customers belong to.</li>
</ol>

<h2 id="how-can-we-find-out-which-targeted-individuals-in-the-new-campaign-are-likely-to-convert">How can we find out which targeted individuals in the new campaign are likely to convert?</h2>
<ol>
  <li>We can use the provided features in addition to features we engineered from the unsupervised analysis to make a new dataset.</li>
  <li>Then we can use supervised learning to train a model to predict which individuals are more likely to convert</li>
</ol>

<h1 id="metrics">Metrics</h1>

<h2 id="customer-segmentation">Customer Segmentation</h2>
<p>We can use intertia to judge how many clusters we shall set the K-Means algorithm to find. Where inertia is the average distance between each data point and it’s cluster center. The smaller the intertia, the better the model. However, inertia tends to decrease as we increase the number of clusters, so we will use the elbow method to determine the optimal cluster number where the improvement in inertia starts slowing down.</p>

<h2 id="mailout-campaign">Mailout Campaign</h2>
<p>That depends on margin of error that we are willing to except with our model. So for example, we might want a model that is able to predict all individuals likely to be customers, despite predicting a large sum of individuals that won’t be customers. <strong>In this case we’d want a model with higher Recall</strong>.</p>

<p>On the other hand, sending out to a huge mass might be expensive and counter-intuitive business-wise in some case. <strong>In this case we’d want a model with higher Precision</strong>.</p>

<p>In order to get the best of both worlds, <strong>we can use the F1-Score</strong>, which calculate a harmonic average of Recall and Precision, penalizing a model that has bad scores for either one of the metrics.</p>

<p><strong>Personally I prefer using F1-Score, specifically the macro-averaged F1-Score</strong>, which calculate the average of F1-Score for each class regardless the number of data points belonging to that class, as a weighted average F1-Score wouldn’t account the main class we are concerned with predicting due to it’s extremely low number.</p>

<p>However, I think that getting a model with high precision with this dataset would be a stretch, so my best guess right now is to use Recall, but the other metric to avoid a model with really bad precision.</p>

<p>So I’ll use Macro Recall, which calculates the recall of each class individually, then averages them, which is great for imbalanced classification.</p>

<h1 id="data-exploration">Data Exploration</h1>

<p>The two main datasets that we were dealing with had a setback, they were both in CSV format, and due to there size, they took a really long time to load (around 25 minutes).</p>

<p>Another hurdle was that we couldn’t just pickle them as the Udacity workspace memory couldn’t handle two copies of the two datasets at the same time.</p>

<p>So the first thing I did was to figure out a way to reduce the size of these datasets by manipulating the data-types of their features, and then pickling them.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">read_pickle</span> <span class="o">=</span> <span class="bp">False</span>

<span class="c1"># csv data path
</span><span class="n">data_path</span> <span class="o">=</span> <span class="s">'../../data/Term2/capstone/arvato_data/'</span>

<span class="c1"># csv files
</span><span class="n">azdias_csv</span> <span class="o">=</span> <span class="s">'Udacity_AZDIAS_052018.csv'</span>
<span class="n">customers_csv</span> <span class="o">=</span> <span class="s">'Udacity_CUSTOMERS_052018.csv'</span>

<span class="c1"># csv file paths
</span><span class="n">azdias_csv_path</span> <span class="o">=</span> <span class="n">os</span><span class="p">.</span><span class="n">path</span><span class="p">.</span><span class="n">join</span><span class="p">(</span><span class="n">data_path</span><span class="p">,</span> <span class="n">azdias_csv</span><span class="p">)</span>
<span class="n">customers_csv_path</span> <span class="o">=</span> <span class="n">os</span><span class="p">.</span><span class="n">path</span><span class="p">.</span><span class="n">join</span><span class="p">(</span><span class="n">data_path</span><span class="p">,</span> <span class="n">customers_csv</span><span class="p">)</span>

<span class="c1"># pickle files
</span><span class="n">azdias_pickle</span> <span class="o">=</span> <span class="s">'Udacity_AZDIAS_052018.csv.pandas.pickle'</span>
<span class="n">customers_pickle</span> <span class="o">=</span> <span class="s">'Udacity_CUSTOMERS_052018.csv.pandas.pickle'</span>

<span class="c1"># load pickled datasets if exists
</span><span class="k">if</span> <span class="n">azdias_pickle</span> <span class="ow">in</span> <span class="n">os</span><span class="p">.</span><span class="n">listdir</span><span class="p">()</span> <span class="ow">and</span> <span class="n">customers_pickle</span> <span class="ow">in</span> <span class="n">os</span><span class="p">.</span><span class="n">listdir</span><span class="p">():</span>
    <span class="k">print</span><span class="p">(</span><span class="s">"Loading AZDIAS pickle..."</span><span class="p">)</span>
    <span class="n">azdias</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">read_pickle</span><span class="p">(</span><span class="n">azdias_pickle</span><span class="p">)</span>
    
    <span class="k">print</span><span class="p">(</span><span class="s">"Loading CUSTOMERS pickle..."</span><span class="p">)</span>
    <span class="n">customers</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">read_pickle</span><span class="p">(</span><span class="n">customers_pickle</span><span class="p">)</span>
    
    <span class="n">read_pickle</span> <span class="o">=</span> <span class="bp">True</span>

<span class="c1"># else load csv and save pickles
</span><span class="k">else</span><span class="p">:</span>
    <span class="k">print</span><span class="p">(</span><span class="s">"Loading AZDIAS csv..."</span><span class="p">)</span>
    <span class="n">azdias</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">read_csv</span><span class="p">(</span><span class="n">azdias_csv_path</span><span class="p">,</span> <span class="n">sep</span><span class="o">=</span><span class="s">';'</span><span class="p">)</span>

    <span class="k">print</span><span class="p">(</span><span class="s">"Loading CUSTOMERS csv..."</span><span class="p">)</span>
    <span class="n">customers</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">read_csv</span><span class="p">(</span><span class="n">customers_csv_path</span><span class="p">,</span> <span class="n">sep</span><span class="o">=</span><span class="s">';'</span><span class="p">)</span>

<span class="k">print</span><span class="p">(</span><span class="s">"Loading Attributes Sheet..."</span><span class="p">)</span>
<span class="n">dias_atts</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">read_excel</span><span class="p">(</span><span class="s">"DIAS Information Levels - Attributes 2017.xlsx"</span><span class="p">)</span>
    
<span class="k">print</span><span class="p">(</span><span class="s">"Loading Values Sheet..."</span><span class="p">)</span>
<span class="n">dias_vals</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">read_excel</span><span class="p">(</span><span class="s">"DIAS Attributes - Values 2017.xlsx"</span><span class="p">)</span>
    
<span class="k">print</span><span class="p">(</span><span class="s">"Done."</span><span class="p">)</span>

<span class="k">if</span> <span class="ow">not</span> <span class="n">read_pickle</span><span class="p">:</span>
    <span class="n">azdias_p</span><span class="p">,</span> <span class="n">azdias_na</span> <span class="o">=</span> <span class="n">reduce_mem_usage</span><span class="p">(</span><span class="n">azdias</span><span class="p">)</span>

<span class="k">if</span> <span class="ow">not</span> <span class="n">read_pickle</span><span class="p">:</span>
    <span class="n">customers_p</span><span class="p">,</span> <span class="n">customers_na</span> <span class="o">=</span> <span class="n">reduce_mem_usage</span><span class="p">(</span><span class="n">customers</span><span class="p">)</span>

<span class="c1"># save datasets in pickle format for later usage
</span><span class="k">if</span> <span class="ow">not</span> <span class="n">read_pickle</span><span class="p">:</span>
    <span class="n">pd</span><span class="p">.</span><span class="n">to_pickle</span><span class="p">(</span><span class="n">azdias_p</span><span class="p">,</span> <span class="n">azdias_pickle</span><span class="p">)</span>
    <span class="n">pd</span><span class="p">.</span><span class="n">to_pickle</span><span class="p">(</span><span class="n">customers_p</span><span class="p">,</span> <span class="n">customers_pickle</span><span class="p">)</span>
</code></pre></div></div>

<p>Therefore, I significantly decreased their load time (practically made it in seconds instead of almost half an hour), and made it easy to save them in the workspace.</p>

<p>After taking a quick look into the dimensions of the demographic datasets I found the following:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># looking into the general population dataset
</span><span class="k">print</span><span class="p">(</span><span class="s">'Shape:'</span><span class="p">,</span> <span class="n">azdias</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>  <span class="c1"># (891221, 366)
</span>
<span class="c1"># looking into the customers dataset
</span><span class="k">print</span><span class="p">(</span><span class="s">'Shape:'</span><span class="p">,</span> <span class="n">customers</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>  <span class="c1"># (191652, 369)
</span></code></pre></div></div>

<p>366 features were shared between these two datasets, and 3 additional features were present in <code class="language-plaintext highlighter-rouge">CUSTOMERS</code> relating only to customers which were:</p>

<ol>
  <li><code class="language-plaintext highlighter-rouge">CUSTOMER_GROUP</code></li>
  <li><code class="language-plaintext highlighter-rouge">PRODUCT_GROUP</code></li>
  <li><code class="language-plaintext highlighter-rouge">ONLINE_PURCHASE</code></li>
</ol>

<p>Each row in the demographic data represented a single individual, but it also contained aggregated information about their household, their building, their neighborhood, their community, and more aggregations relating to different perspectives.</p>

<p>The thought of trying to understand these 366 features was overwhelming, as it was the first time I have ever tackled a dataset with this size. So I resorted to the sheets provided about the them.</p>

<p>Information about these features were included in <code class="language-plaintext highlighter-rouge">DIAS Information Levels - Attributes 2017.xlsx</code> and <code class="language-plaintext highlighter-rouge">DIAS Attributes - Values 2017.xlsx</code>, however after looking into the number of features included in these sheets and the features available in the two demographic datasets, I found some features to be missing from the sheets, and some features in the sheets that were missing from the demographic data.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">print</span><span class="p">(</span><span class="s">"Number of features in Values:"</span><span class="p">,</span> <span class="n">dias_vals</span><span class="p">.</span><span class="n">Attribute</span><span class="p">.</span><span class="n">dropna</span><span class="p">().</span><span class="n">nunique</span><span class="p">())</span>  <span class="c1"># 314
</span><span class="k">print</span><span class="p">(</span><span class="s">"Number of features in Attributes:"</span><span class="p">,</span> <span class="n">dias_atts</span><span class="p">.</span><span class="n">Attribute</span><span class="p">.</span><span class="n">dropna</span><span class="p">().</span><span class="n">nunique</span><span class="p">())</span>  <span class="c1"># 313
</span></code></pre></div></div>

<p>It turned out that the demographic data belonged to 2018, while these sheets belonged to the same data source, but one that was aggregated in 2017.</p>

<p>After taking a look at both sheets I found another thing, despite the dataframe having null values, most features had values in <code class="language-plaintext highlighter-rouge">DIAS Attributes - Values 2017.xlsx</code> that encoded unknown or missing values. So that urged me to explore missing values in the datasets, which could get me familiar with the data and break the ice between us before delving into each feature individually.</p>

<p>So I made a function that calculates the distribution of null percentages in a dataset in order to enables easy exploration before and after replacing the unknown values with null.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">get_null_prop</span><span class="p">(</span><span class="n">df</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">plot</span><span class="o">=</span><span class="bp">True</span><span class="p">):</span>
    <span class="s">"""Calculates null proportions in dataframe columns or rows."""</span>
    <span class="c1"># calculate null proportion of each column or row
</span>    <span class="n">null_prop</span> <span class="o">=</span> <span class="p">(</span><span class="n">df</span><span class="p">.</span><span class="n">isnull</span><span class="p">().</span><span class="nb">sum</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="n">axis</span><span class="p">)</span> <span class="o">/</span> <span class="n">df</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="n">axis</span><span class="p">]).</span><span class="n">sort_values</span><span class="p">()</span>
    <span class="k">if</span> <span class="n">plot</span><span class="p">:</span>
        <span class="n">null_prop</span><span class="p">.</span><span class="n">hist</span><span class="p">(</span><span class="n">edgecolor</span><span class="o">=</span><span class="s">'black'</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mf">1.2</span><span class="p">)</span>    
    <span class="k">return</span> <span class="n">null_prop</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">azdias_null_cols</span> <span class="o">=</span> <span class="n">get_null_prop</span><span class="p">(</span><span class="n">azdias</span><span class="p">,</span> <span class="n">plot</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
<span class="n">customers_null_cols</span> <span class="o">=</span> <span class="n">get_null_prop</span><span class="p">(</span><span class="n">customers</span><span class="p">,</span> <span class="n">plot</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>

<span class="n">azdias_null_cols</span><span class="p">.</span><span class="n">hist</span><span class="p">(</span><span class="n">bins</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.7</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">'AZDIAS'</span><span class="p">)</span>
<span class="n">customers_null_cols</span><span class="p">.</span><span class="n">hist</span><span class="p">(</span><span class="n">bins</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.7</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">'CUSTOMERS'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">title</span><span class="p">(</span><span class="s">"Distribution of Null Values in Columns"</span><span class="p">);</span>
<span class="n">plt</span><span class="p">.</span><span class="n">legend</span><span class="p">();</span>
</code></pre></div></div>

<p><img src="/images/arvato/null_columns.png" alt="null columns" /></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">azdias_null_rows</span> <span class="o">=</span> <span class="n">get_null_prop</span><span class="p">(</span><span class="n">azdias</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="bp">False</span><span class="p">)</span>
<span class="n">customers_null_rows</span> <span class="o">=</span> <span class="n">get_null_prop</span><span class="p">(</span><span class="n">customers</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="bp">False</span><span class="p">)</span>

<span class="n">azdias_null_rows</span><span class="p">.</span><span class="n">hist</span><span class="p">(</span><span class="n">bins</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.7</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">'AZDIAS'</span><span class="p">)</span>
<span class="n">customers_null_rows</span><span class="p">.</span><span class="n">hist</span><span class="p">(</span><span class="n">bins</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.7</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">'CUSTOMERS'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">title</span><span class="p">(</span><span class="s">"Distribution of Null Values in Rows"</span><span class="p">);</span>
<span class="n">plt</span><span class="p">.</span><span class="n">legend</span><span class="p">();</span>
</code></pre></div></div>

<p><img src="/images/arvato/null_rows.png" alt="null rows" /></p>

<p>From the above graphs we can see that the null percentages are definitely higher in <code class="language-plaintext highlighter-rouge">AZDIAS</code> , and since we will be modeling the clustering algorithm on it, we will focus our exploration on it, and only look into <code class="language-plaintext highlighter-rouge">CUSTOMER</code> if needed.</p>

<p>Then I made a function to replace the unknown values in each features with null, in order to look into the real percentage of missing values in rows and columns.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">replace_unknown_with_null</span><span class="p">(</span><span class="n">df</span><span class="p">):</span>
    <span class="s">"""Replace unknown values with null in all features that have that info available."""</span>
    <span class="n">df_new</span> <span class="o">=</span> <span class="n">df</span><span class="p">.</span><span class="n">copy</span><span class="p">()</span>
    <span class="n">feat_unknown_vals</span> <span class="o">=</span> <span class="n">dias_vals</span><span class="p">.</span><span class="n">query</span><span class="p">(</span><span class="s">"Meaning == 'unknown'"</span><span class="p">)</span>

    <span class="k">for</span> <span class="n">feat</span> <span class="ow">in</span> <span class="n">feat_unknown_vals</span><span class="p">.</span><span class="n">itertuples</span><span class="p">():</span>
        <span class="c1"># check if feature in df
</span>        <span class="k">if</span> <span class="n">feat</span><span class="p">.</span><span class="n">Attribute</span> <span class="ow">in</span> <span class="n">df_new</span><span class="p">:</span>
            <span class="c1"># if unknown values are more than one
</span>            <span class="k">if</span> <span class="s">','</span> <span class="ow">in</span> <span class="nb">str</span><span class="p">(</span><span class="n">feat</span><span class="p">.</span><span class="n">Value</span><span class="p">):</span>
                <span class="c1"># loop over unknown values
</span>                <span class="k">for</span> <span class="n">val</span> <span class="ow">in</span> <span class="nb">str</span><span class="p">(</span><span class="n">feat</span><span class="p">.</span><span class="n">Value</span><span class="p">).</span><span class="n">split</span><span class="p">(</span><span class="s">','</span><span class="p">):</span>
                    <span class="c1"># replace unknown value with null
</span>                    <span class="n">df_new</span><span class="p">.</span><span class="n">loc</span><span class="p">[:,</span> <span class="n">feat</span><span class="p">.</span><span class="n">Attribute</span><span class="p">]</span> <span class="o">=</span> <span class="n">df_new</span><span class="p">.</span><span class="n">loc</span><span class="p">[:,</span> <span class="n">feat</span><span class="p">.</span><span class="n">Attribute</span><span class="p">].</span><span class="n">replace</span><span class="p">(</span><span class="nb">eval</span><span class="p">(</span><span class="n">val</span><span class="p">),</span> <span class="n">np</span><span class="p">.</span><span class="n">nan</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="c1"># replace unknown value with null
</span>                <span class="n">df_new</span><span class="p">.</span><span class="n">loc</span><span class="p">[:,</span> <span class="n">feat</span><span class="p">.</span><span class="n">Attribute</span><span class="p">]</span> <span class="o">=</span> <span class="n">df_new</span><span class="p">.</span><span class="n">loc</span><span class="p">[:,</span> <span class="n">feat</span><span class="p">.</span><span class="n">Attribute</span><span class="p">].</span><span class="n">replace</span><span class="p">(</span><span class="n">feat</span><span class="p">.</span><span class="n">Value</span><span class="p">,</span> <span class="n">np</span><span class="p">.</span><span class="n">nan</span><span class="p">)</span> 
    <span class="k">return</span> <span class="n">df_new</span>

<span class="c1"># replace unknown values with null
</span><span class="n">azdias_new</span> <span class="o">=</span> <span class="n">replace_unknown_with_null</span><span class="p">(</span><span class="n">azdias</span><span class="p">)</span>
</code></pre></div></div>

<p>After replacing the unknown values with null, I visualized the null percentages in old and new <code class="language-plaintext highlighter-rouge">AZDIAS</code> in order to understand the consequences of this step.</p>

<p><img src="/images/arvato/null_columns_azdias.png" alt="null columns azdias" /></p>

<p><img src="/images/arvato/null_rows_azdias.png" alt="null rows azdias" /></p>

<p>After replacing the unknown values with null, we can see more features having higher percentage of null values than before.</p>

<p>The question then was how to deal with these missing values?</p>

<p>This will depend completely on the type of feature we are dealing with. As missing values can be divided into 3 categories:</p>

<ol>
  <li>Missing completely at random (MCAR): where the absence of such data is completely unrelated to other observed data and unobserved data, which means that there is no pattern to the missing data.</li>
  <li>Missing at random (MAR): where the absence of such data is related to other observed data but not unobserved data, which means that there is a pattern to the missing data.</li>
  <li>Missing not at random: where the missing data is related to unobserved data and it signifies something, like a column about age of first child while the person related to the data point doesn’t have a child.</li>
</ol>

<p>We have a lot of features, and fortunately <code class="language-plaintext highlighter-rouge">DIAS Information Levels - Attributes 2017.xlsx</code> had them divided into categories.</p>

<p>So I decided that the best way is to skim each category’s features with their description, value counts and percentage of null values, and take notes along the way.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">print</span><span class="p">(</span><span class="s">"Number of feature categories:"</span><span class="p">,</span> <span class="n">dias_atts</span><span class="p">[</span><span class="s">"Information level"</span><span class="p">].</span><span class="n">nunique</span><span class="p">())</span>
<span class="k">print</span><span class="p">(</span><span class="s">"</span><span class="se">\n</span><span class="s">Feature Categories:"</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">dias_atts</span><span class="p">[</span><span class="s">"Information level"</span><span class="p">].</span><span class="n">value_counts</span><span class="p">())</span>

<span class="n">Number</span> <span class="n">of</span> <span class="n">feature</span> <span class="n">categories</span><span class="p">:</span> <span class="mi">9</span>

<span class="n">Feature</span> <span class="n">Categories</span><span class="p">:</span>
<span class="n">PLZ8</span>                  <span class="mi">112</span>
<span class="n">Microcell</span> <span class="p">(</span><span class="n">RR3_ID</span><span class="p">)</span>     <span class="mi">54</span>
<span class="n">Person</span>                 <span class="mi">42</span>
<span class="n">Household</span>              <span class="mi">25</span>
<span class="n">Microcell</span> <span class="p">(</span><span class="n">RR4_ID</span><span class="p">)</span>     <span class="mi">11</span>
<span class="n">Building</span>                <span class="mi">9</span>
<span class="n">RR1_ID</span>                  <span class="mi">5</span>
<span class="n">Postcode</span>                <span class="mi">3</span>
<span class="n">Community</span>               <span class="mi">3</span>
<span class="n">Name</span><span class="p">:</span> <span class="n">Information</span> <span class="n">level</span><span class="p">,</span> <span class="n">dtype</span><span class="p">:</span> <span class="n">int64</span>
</code></pre></div></div>

<p>And so I made a function that let’s us inspect the features of each category, and get information about it’s description if it was available.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">explore_category</span><span class="p">(</span><span class="n">category</span><span class="p">,</span> <span class="n">azdias</span><span class="p">):</span>
    <span class="s">"""Prints description, null percentage and value counts of each feature in a specified category."""</span>
    <span class="c1"># query only features in category
</span>    <span class="n">cat_feats</span> <span class="o">=</span> <span class="n">dias_atts</span><span class="p">[</span><span class="n">dias_atts</span><span class="p">[</span><span class="s">"Information level"</span><span class="p">]</span> <span class="o">==</span> <span class="n">category</span><span class="p">]</span>
    <span class="c1"># calcualte null percentage of each features
</span>    <span class="n">cat_feats</span><span class="p">[</span><span class="s">"null_percentage"</span><span class="p">]</span> <span class="o">=</span> <span class="n">cat_feats</span><span class="p">[</span><span class="s">"Attribute"</span><span class="p">].</span><span class="nb">apply</span><span class="p">(</span><span class="k">lambda</span> <span class="n">feat</span><span class="p">:</span> <span class="n">azdias</span><span class="p">[</span><span class="n">feat</span><span class="p">].</span><span class="n">isna</span><span class="p">().</span><span class="nb">sum</span><span class="p">()</span><span class="o">/</span><span class="n">azdias</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
    <span class="c1"># sort by null percentage
</span>    <span class="n">cat_feats</span> <span class="o">=</span> <span class="n">cat_feats</span><span class="p">.</span><span class="n">sort_values</span><span class="p">(</span><span class="s">"null_percentage"</span><span class="p">)</span>
    
    <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"Number of features in </span><span class="si">{</span><span class="n">category</span><span class="si">}</span><span class="s">: </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">cat_feats</span><span class="p">)</span><span class="si">}</span><span class="se">\n\n</span><span class="s">"</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">row</span> <span class="ow">in</span> <span class="n">cat_feats</span><span class="p">.</span><span class="n">iterrows</span><span class="p">():</span>
        <span class="n">feat</span> <span class="o">=</span> <span class="n">row</span><span class="p">[</span><span class="s">"Attribute"</span><span class="p">]</span>
        <span class="k">print</span><span class="p">(</span><span class="n">feat</span><span class="p">)</span>
        <span class="k">print</span><span class="p">(</span><span class="n">row</span><span class="p">[</span><span class="s">"Description"</span><span class="p">])</span>
        <span class="k">print</span><span class="p">(</span><span class="s">"Null percentage:"</span><span class="p">,</span> <span class="n">row</span><span class="p">[</span><span class="s">"null_percentage"</span><span class="p">])</span>
        <span class="k">print</span><span class="p">(</span><span class="n">azdias</span><span class="p">[</span><span class="n">feat</span><span class="p">].</span><span class="n">value_counts</span><span class="p">())</span>
        <span class="k">print</span><span class="p">()</span>
</code></pre></div></div>

<p>The output of each feature was similar to this</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">KBA13_ALTERHALTER_30</span>
<span class="n">share</span> <span class="n">of</span> <span class="n">car</span> <span class="n">owners</span> <span class="n">below</span> <span class="mi">31</span> <span class="n">within</span> <span class="n">the</span> <span class="n">PLZ8</span>
<span class="n">Null</span> <span class="n">percentage</span><span class="p">:</span> <span class="mf">0.11871354018812394</span>
<span class="mf">3.0</span>    <span class="mi">333405</span>
<span class="mf">2.0</span>    <span class="mi">160653</span>
<span class="mf">4.0</span>    <span class="mi">147128</span>
<span class="mf">1.0</span>     <span class="mi">72911</span>
<span class="mf">5.0</span>     <span class="mi">71324</span>
<span class="n">Name</span><span class="p">:</span> <span class="n">KBA13_ALTERHALTER_30</span><span class="p">,</span> <span class="n">dtype</span><span class="p">:</span> <span class="n">int64</span>
</code></pre></div></div>

<p>Which enabled us to skim the features in a given category easily and write down notes and thoughts about these features.</p>

<h3 id="plz8-features">PLZ8 Features</h3>

<p>According to <a href="https://datarade.ai/data-products/plz8-germany-and-plz8-germany-xxl">https://datarade.ai/data-products/plz8-germany-and-plz8-germany-xxl</a>, Germany has been divided into 84,000 PLZ8 boundaries, so this category contains socio-economic data related to each PLZ8 boundary which helps in optimizing distribution of promotional materials, and in our case the mail-order sales.</p>

<p>By skimming through the results I found that that:</p>

<ol>
  <li>All features are ordinal categorical except <strong>KBA13_ANZAHL_PKW</strong>.</li>
  <li>105 features have the same null percentage which is 11.87%, while the remaining 7 have 13.07%</li>
</ol>

<p><img src="/images/arvato/null_plz8.png" alt="null plz8" /></p>

<p>This rings the bell for data MCAR or MNAR, as there is a pattern that is most likely unrelated to observed data, but can or can’t be related to unobserved data, which is why some persons just don’t have PLZ8 data collected.</p>

<ol>
  <li><strong>KBA13_ANZAHL_PKW</strong> is supposed to encode the number of cars in the PLZ8, but it has high values of peculiar number which are 1400, 1500, 1300, etc.</li>
</ol>

<p><img src="/images/arvato/KBA13_ANZAHL_PKW.png" alt="KBA13_ANZAHL_PKW" /></p>

<p>We can see that the bins starts getting less granular is we exceed 1200. My guess is that this data was spread between 1300 and the max values, but the granularity of these section was decreased, which explains why the distribution is right skewed but then we start seeing bumps near the end.</p>

<p>We could leave it as is, as I don’t think it would make much difference. Or we can follow the lead of the last section and reduce the granularity of the whole feature.</p>

<h3 id="microcell-rr3_id-features">Microcell (RR3_ID) Features</h3>

<p>By skimming through the results I can see that:</p>

<ol>
  <li>The most prominent null percentage is 16.6%, while a few have less and only has 53.46% which is <strong>KBA05_BAUMAX</strong> (most common building-type within the cell).</li>
</ol>

<h3 id="why-is-this-data-missing">Why is this data missing?</h3>

<p>This is data about a collective of individuals, and it should mean that we have any indicator about this collective, like an identifier for the microcell of this person, or the plz8.</p>

<p>I looked for any feature that has the postcode or the PLZ8 area or anything related, but I didn’t find one. Therefore I thought that in handling missing data we should do the following:</p>

<ol>
  <li>First we should look for rows that have high percentage of missing values across all feature categories</li>
  <li>We should then drop these rows as we can’t use other feature categories to infer them (if we used a method like KNN imputation)</li>
  <li>We shall then impute the missing values only when the rows are missing from a feature category, but not from the other. That’s because the data is extremely related to each other <strong>(we have information about the person, their household, their community, their area, their microcell and their plz8, and all of these are related to each other, so we can use them to infer missing information about each other.)</strong></li>
</ol>

<h3 id="person-features">Person Features</h3>

<p>Notes about the features:</p>

<ul>
  <li><strong>GEBURTSJAHR</strong> (year of birth) has 44% missing values (encoded as 0), which need to be dropped, as we can’t infer year of birth, and I don’t think it would be that indicative of anything when we have much more deep information about each individual.</li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">GEBURTSJAHR</span>
<span class="n">year</span> <span class="n">of</span> <span class="n">birth</span>
<span class="n">Null</span> <span class="n">percentage</span><span class="p">:</span> <span class="mf">0.0</span>
<span class="mi">0</span>       <span class="mi">392318</span>
<span class="mi">1967</span>     <span class="mi">11183</span>
<span class="mi">1965</span>     <span class="mi">11090</span>
<span class="mi">1966</span>     <span class="mi">10933</span>
<span class="mi">1970</span>     <span class="mi">10883</span>
<span class="p">...</span>
</code></pre></div></div>

<ul>
  <li>Some features have 0.5% missing values which are mostly related to social status, and it’s peculiar why this data is missing about these individuals. If these individuals have more missing data in the other feature categories then we can safely drop them and not worry about their missing data type.</li>
  <li><strong>PRAEGENDE_JUGENDJAHRE</strong> (dominating movement in the person’s youth (avantgarde or mainstream)), <strong>NATIONALITAET_KZ</strong> (nationaltity), <strong>VERS_TYP</strong> (insurance typology), <strong>HEALTH_TYP</strong> (health typology) and <strong>SHOPPER_TYP</strong> (shopping typology) have around 12% missing values.</li>
  <li><strong>AGER_TYP</strong> (best-ager typology) has 76% missing values, in addition to around 1% of individuals that couldn’t be classified (encode as 0).</li>
  <li><strong>TITEL_KZ</strong> (flag whether this person holds an academic title) has 99% missing values, and it could be just that only 1% hold academic titles and hence the data is correct, and it could be that the data isn’t complete and hence we should drop it.</li>
</ul>

<p>At this point in the exploration, I was curious to find how are the missing values in current features related with the previous categories’ ones?</p>

<p>So I determined a threshold for the percentage of missing values in a feature to include this test, and that should be the highest repeating missing values percentage we have seen so far which is 16.6%.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># all features with missing percentage less than 17% and higher than 0%
</span><span class="n">features_missing</span> <span class="o">=</span> <span class="n">azdias</span><span class="p">.</span><span class="n">columns</span><span class="p">[((</span><span class="n">azdias_new</span><span class="p">.</span><span class="n">isnull</span><span class="p">().</span><span class="nb">sum</span><span class="p">()</span> <span class="o">/</span> <span class="n">azdias</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span> <span class="o">&lt;</span> <span class="mf">0.17</span><span class="p">)</span> <span class="o">&amp;</span>
                                  <span class="p">((</span><span class="n">azdias_new</span><span class="p">.</span><span class="n">isnull</span><span class="p">().</span><span class="nb">sum</span><span class="p">()</span> <span class="o">/</span> <span class="n">azdias</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]))</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">]</span>

<span class="c1"># narrow down to only features of the explored categories
</span><span class="n">plz8_feats</span> <span class="o">=</span> <span class="n">dias_atts</span><span class="p">[</span><span class="n">dias_atts</span><span class="p">[</span><span class="s">"Information level"</span><span class="p">]</span> <span class="o">==</span> <span class="s">"PLZ8"</span><span class="p">][</span><span class="s">"Attribute"</span><span class="p">].</span><span class="n">unique</span><span class="p">()</span>
<span class="n">rr3_feats</span> <span class="o">=</span> <span class="n">dias_atts</span><span class="p">[</span><span class="n">dias_atts</span><span class="p">[</span><span class="s">"Information level"</span><span class="p">]</span> <span class="o">==</span> <span class="s">"Microcell (RR3_ID)"</span><span class="p">][</span><span class="s">"Attribute"</span><span class="p">].</span><span class="n">unique</span><span class="p">()</span>
<span class="n">person_feats</span> <span class="o">=</span> <span class="n">dias_atts</span><span class="p">[</span><span class="n">dias_atts</span><span class="p">[</span><span class="s">"Information level"</span><span class="p">]</span> <span class="o">==</span> <span class="s">"Person"</span><span class="p">][</span><span class="s">"Attribute"</span><span class="p">].</span><span class="n">unique</span><span class="p">()</span>
<span class="n">features_missing</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">set</span><span class="p">(</span><span class="n">plz8_feats</span><span class="p">).</span><span class="n">union</span><span class="p">(</span><span class="n">rr3_feats</span><span class="p">).</span><span class="n">union</span><span class="p">(</span><span class="n">person_feats</span><span class="p">).</span><span class="n">intersection</span><span class="p">(</span><span class="n">features_missing</span><span class="p">))</span>
</code></pre></div></div>

<p>Then I looked into the percentage of rows with 100% missing values.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># azdias with only features that have missing values
</span><span class="n">azdias_missing</span> <span class="o">=</span> <span class="n">azdias</span><span class="p">[</span><span class="n">features_missing</span><span class="p">]</span>

<span class="c1"># flag rows with all missing values
</span><span class="n">rows_all_missing</span> <span class="o">=</span> <span class="n">azdias_missing</span><span class="p">.</span><span class="n">isna</span><span class="p">().</span><span class="nb">sum</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">==</span> <span class="n">azdias_missing</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>

<span class="k">print</span><span class="p">(</span><span class="s">"Number of rows with all missing values:"</span><span class="p">,</span> <span class="n">rows_all_missing</span><span class="p">.</span><span class="nb">sum</span><span class="p">())</span>
<span class="k">print</span><span class="p">(</span><span class="s">"Percentage of rows with all missing values:"</span><span class="p">,</span> <span class="n">rows_all_missing</span><span class="p">.</span><span class="nb">sum</span><span class="p">()</span><span class="o">/</span><span class="n">azdias_missing</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>

<span class="n">Number</span> <span class="n">of</span> <span class="n">rows</span> <span class="k">with</span> <span class="nb">all</span> <span class="n">missing</span> <span class="n">values</span><span class="p">:</span> <span class="mi">7</span>
<span class="n">Percentage</span> <span class="n">of</span> <span class="n">rows</span> <span class="k">with</span> <span class="nb">all</span> <span class="n">missing</span> <span class="n">values</span><span class="p">:</span> <span class="mf">7.854393018117841e-06</span>
</code></pre></div></div>

<p>We can see that there are 7 rows that have all the values for the features explored missing, and we already know that there are rows with high percentage of missing features. <strong>But how many are there exactly?</strong></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">rows_missing_p</span> <span class="o">=</span> <span class="n">azdias_missing</span><span class="p">.</span><span class="n">isnull</span><span class="p">().</span><span class="nb">sum</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">/</span><span class="n">azdias_missing</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">np</span><span class="p">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mf">0.8</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">):</span>
    <span class="k">print</span><span class="p">(</span><span class="s">"Percentage of rows with more than {:.2f}% values missing: {}"</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="n">i</span><span class="o">*</span><span class="mi">100</span><span class="p">,</span> <span class="p">(</span><span class="n">rows_missing_p</span><span class="o">&gt;</span><span class="n">i</span><span class="p">).</span><span class="nb">sum</span><span class="p">()</span><span class="o">/</span><span class="n">azdias</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]))</span>
</code></pre></div></div>
<div class="language-markdown highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Percentage of rows with more than 0.00% values missing: 0.22381092905126787
Percentage of rows with more than 10.00% values missing: 0.1727225906929931
Percentage of rows with more than 20.00% values missing: 0.17272146863684765
Percentage of rows with more than 30.00% values missing: 0.1315577168850375
Percentage of rows with more than 40.00% values missing: 0.11871690635656026
Percentage of rows with more than 50.00% values missing: 0.11871354018812394
Percentage of rows with more than 60.00% values missing: 0.11799766836732976
Percentage of rows with more than 70.00% values missing: 0.11217980725319533
</code></pre></div></div>

<p>We can see the there is almost 12% of rows that more than 50% missing values. In order to figure out if these rows are problematic here only or overall in the dataset, we can look into the same information using all features in the dataset.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">rows_missing_p</span> <span class="o">=</span> <span class="n">azdias_new</span><span class="p">.</span><span class="n">isnull</span><span class="p">().</span><span class="nb">sum</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">/</span><span class="n">azdias_new</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">np</span><span class="p">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mf">0.8</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">):</span>
    <span class="k">print</span><span class="p">(</span><span class="s">"Percentage of rows with more than {:.2f}% values missing: {}"</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="n">i</span><span class="o">*</span><span class="mi">100</span><span class="p">,</span> <span class="p">(</span><span class="n">rows_missing_p</span><span class="o">&gt;</span><span class="n">i</span><span class="p">).</span><span class="nb">sum</span><span class="p">()</span><span class="o">/</span><span class="n">azdias</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]))</span>
</code></pre></div></div>
<div class="language-markdown highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Percentage of rows with more than 0.00% values missing: 1.0
Percentage of rows with more than 10.00% values missing: 0.17350802999480489
Percentage of rows with more than 20.00% values missing: 0.14647545333873416
Percentage of rows with more than 30.00% values missing: 0.11872139458114205
Percentage of rows with more than 40.00% values missing: 0.11250408147922905
Percentage of rows with more than 50.00% values missing: 0.11216746463559543
Percentage of rows with more than 60.00% values missing: 0.10454645929573024
Percentage of rows with more than 70.00% values missing: 0.0824902016447099
</code></pre></div></div>

<p>We can see that in the whole the dataset 11.2% of rows have more than 50% missing values. This part of the data seemed problematic, and it suggested that these rows should be dropped. But first I wanted to explore them with relationship to all features that have missing values.</p>

<p>And what I did was this, I look for the percentage of each feature’s missing values in these rows compared the total number of missing values available in all rows.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># calculate features missing values
</span><span class="n">feat_missing_count</span> <span class="o">=</span> <span class="n">azdias_new</span><span class="p">.</span><span class="n">isna</span><span class="p">().</span><span class="nb">sum</span><span class="p">()</span>

<span class="c1"># filter out rows with more than 50% missing values
</span><span class="n">half_missing_rows</span> <span class="o">=</span> <span class="n">azdias_new</span><span class="p">[</span><span class="n">rows_missing_p</span> <span class="o">&gt;</span> <span class="mf">0.5</span><span class="p">]</span>

<span class="c1"># transpose the dataframe to make the features as index
</span><span class="n">half_missing_rows_t</span> <span class="o">=</span> <span class="n">half_missing_rows</span><span class="p">.</span><span class="n">T</span>

<span class="c1"># calculate the percentage of null values in these rows for each feature
</span><span class="n">half_missing_rows_t</span><span class="p">[</span><span class="s">"null_percentage"</span><span class="p">]</span> <span class="o">=</span> <span class="n">half_missing_rows_t</span><span class="p">.</span><span class="n">isna</span><span class="p">().</span><span class="nb">sum</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">/</span><span class="n">feat_missing_count</span>

<span class="c1"># sort values by null_percentage
</span><span class="n">half_missing_rows_t</span> <span class="o">=</span> <span class="n">half_missing_rows_t</span><span class="p">.</span><span class="n">sort_values</span><span class="p">(</span><span class="s">"null_percentage"</span><span class="p">,</span> <span class="n">ascending</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>

<span class="c1"># select features that have null values
</span><span class="n">half_missing_rows_t</span> <span class="o">=</span> <span class="n">half_missing_rows_t</span><span class="p">.</span><span class="n">query</span><span class="p">(</span><span class="s">"null_percentage &gt; 0"</span><span class="p">)</span>

<span class="c1"># print each feature, category and percent of missing values
</span><span class="n">category_missing</span> <span class="o">=</span> <span class="n">defaultdict</span><span class="p">(</span><span class="nb">list</span><span class="p">)</span>
<span class="n">category_missing_p</span> <span class="o">=</span> <span class="n">defaultdict</span><span class="p">(</span><span class="nb">list</span><span class="p">)</span>
<span class="n">category_missing_count</span> <span class="o">=</span> <span class="n">defaultdict</span><span class="p">(</span><span class="nb">int</span><span class="p">)</span>
<span class="k">for</span> <span class="n">feat</span><span class="p">,</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">half_missing_rows_t</span><span class="p">[</span><span class="s">"null_percentage"</span><span class="p">].</span><span class="n">iteritems</span><span class="p">():</span>
    <span class="k">if</span> <span class="n">feat</span> <span class="ow">in</span> <span class="nb">set</span><span class="p">(</span><span class="n">dias_atts</span><span class="p">.</span><span class="n">Attribute</span><span class="p">):</span>
        <span class="n">category</span> <span class="o">=</span> <span class="n">dias_atts</span><span class="p">.</span><span class="n">query</span><span class="p">(</span><span class="s">"Attribute == @feat"</span><span class="p">)[</span><span class="s">"Information level"</span><span class="p">].</span><span class="n">item</span><span class="p">()</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">category</span> <span class="o">=</span> <span class="s">"Unknown"</span> 
    <span class="n">category_missing_p</span><span class="p">[</span><span class="n">category</span><span class="p">].</span><span class="n">append</span><span class="p">(</span><span class="n">p</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">p</span> <span class="o">&gt;</span> <span class="mf">0.9</span><span class="p">:</span>
        <span class="n">category_missing</span><span class="p">[</span><span class="n">category</span><span class="p">].</span><span class="n">append</span><span class="p">(</span><span class="n">feat</span><span class="p">)</span>
        <span class="n">category_missing_count</span><span class="p">[</span><span class="n">category</span><span class="p">]</span> <span class="o">+=</span> <span class="mi">1</span>
    <span class="k">print</span><span class="p">(</span><span class="n">feat</span><span class="p">,</span> <span class="n">category</span><span class="p">,</span> <span class="n">p</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-markdown highlighter-rouge"><div class="highlight"><pre class="highlight"><code>WOHNLAGE Building 1.0
ANZ_TITEL Household 1.0
EINGEFUEGT_AM Unknown 1.0
DSL_FLAG Unknown 1.0
GEBAEUDETYP Building 1.0
AKT_DAT_KL Unknown 1.0
MIN_GEBAEUDEJAHR Building 1.0
MOBI_RASTER Unknown 1.0
OST_WEST_KZ Building 1.0
SOHO_KZ Unknown 1.0
HH_EINKOMMEN_SCORE Household 1.0
UNGLEICHENN_FLAG Unknown 1.0
KBA05_MODTEMP Building 1.0
EINGEZOGENAM_HH_JAHR Unknown 1.0
ANZ_HAUSHALTE_AKTIV Building 1.0
ALTER_HH Household 1.0
ANZ_STATISTISCHE_HAUSHALTE Unknown 1.0
WOHNDAUER_2008 Household 1.0
ANZ_PERSONEN Household 1.0
ANZ_KINDER Unknown 1.0
...
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">for</span> <span class="n">category</span><span class="p">,</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">category_missing_p</span><span class="p">.</span><span class="n">items</span><span class="p">():</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">hist</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="n">category</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">title</span><span class="p">(</span><span class="s">"Distribution of Null Values Ratio between Data with &gt;= 50% Missing Values and Full Data by Category"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s">"Null Ratio"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">legend</span><span class="p">()</span>
</code></pre></div></div>

<p><img src="/images/arvato/null_ratio_1.png" alt="null ratio 1" /></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">category_mean_p</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">Series</span><span class="p">({</span><span class="n">category</span><span class="p">:</span> <span class="n">np</span><span class="p">.</span><span class="n">mean</span><span class="p">(</span><span class="n">p</span><span class="p">)</span> <span class="k">for</span> <span class="n">category</span><span class="p">,</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">category_missing_p</span><span class="p">.</span><span class="n">items</span><span class="p">()}).</span><span class="n">sort_values</span><span class="p">(</span><span class="n">ascending</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
<span class="n">category_mean_p</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">kind</span><span class="o">=</span><span class="s">"bar"</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="s">"Mean Null Values Ratio between Data with &gt;= 50% Missing Values and Full Data"</span><span class="p">);</span>
<span class="n">plt</span><span class="p">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s">"Categories"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s">"Mean Null Ratio"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">savefig</span><span class="p">(</span><span class="s">"null_ratio_2.png"</span><span class="p">);</span>
</code></pre></div></div>

<p><img src="/images/arvato/null_ratio_2.png" alt="null ratio 2" /></p>

<p>Using these plots we can see that most of the features will benefit from dropping these rows to get rid of most of the missing values in the data, and the rest can be imputed using the method of our choice.</p>

<p>So I asked myself, if I dropped these rows completely, how many features would have more than 90% of their missing values removed?</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># first we need to add the original count of Unknown features
</span><span class="n">original_category_count</span> <span class="o">=</span> <span class="n">dias_atts</span><span class="p">[</span><span class="s">"Information level"</span><span class="p">].</span><span class="n">value_counts</span><span class="p">()</span>
<span class="n">original_category_count</span><span class="p">[</span><span class="s">"Unknown"</span><span class="p">]</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">azdias_feats</span><span class="p">.</span><span class="n">difference</span><span class="p">(</span><span class="n">dias_atts_feats</span><span class="p">))</span>
<span class="n">original_category_count</span> <span class="o">=</span> <span class="n">original_category_count</span><span class="p">.</span><span class="n">sort_values</span><span class="p">(</span><span class="n">ascending</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>

<span class="n">category_half_missing_count</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">Series</span><span class="p">(</span><span class="n">category_missing_count</span><span class="p">).</span><span class="n">sort_values</span><span class="p">(</span><span class="n">ascending</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
<span class="n">category_half_missing_count</span><span class="p">[</span><span class="n">original_category_count</span><span class="p">.</span><span class="n">index</span><span class="p">].</span><span class="n">plot</span><span class="p">(</span><span class="n">kind</span><span class="o">=</span><span class="s">"bar"</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s">'blue'</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">"Missing rows"</span><span class="p">)</span>
<span class="n">original_category_count</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">kind</span><span class="o">=</span><span class="s">"bar"</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s">'green'</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">"All"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">xticks</span><span class="p">(</span><span class="n">rotation</span><span class="o">=</span><span class="mi">90</span><span class="p">);</span>
<span class="n">plt</span><span class="p">.</span><span class="n">legend</span><span class="p">();</span>
<span class="n">plt</span><span class="p">.</span><span class="n">title</span><span class="p">(</span><span class="s">"Number of Features with More than 90% Missing Values in Rows with More than 50% Missing Values"</span><span class="p">)</span>
</code></pre></div></div>

<p><img src="/images/arvato/drop_missing_rows.png" alt="drop missing rows" /></p>

<p>This graph gives a much clearer picture of what is going on, so let me explain:</p>

<ul>
  <li>First this graph has the count of features that have more than 90% missing values only in the rows with more than 50% missing values, so it’s basically a win-win if we dropped these rows as it will automatically fix the problem of these features missing values, and the rest can be imputed.</li>
  <li>The majority of PLZ8, Building, Postcode and Community features are completely missing in rows with more than 50% missing values so if we drop these rows we won’t need to impute them.</li>
  <li>Around 25% of the features with no known category won’t need imputation if we dropped rows with more than 50% missing values.</li>
  <li>Some features from Household, RR4 and RR1 categories will be fixed.</li>
</ul>

<p><strong>So now that I was satisfied with the results of this detour, I continued exploring the feature in the remaining categories.</strong></p>

<h3 id="household-features">Household Features</h3>

<ol>
  <li><strong>ALTER_HH</strong> (main age within the household) has missing values encoded as 0.</li>
  <li><strong>WOHNDAUER_2008</strong> (length of residence) won’t need imputation as 100% of missing values are present in the rows that we are dropping.</li>
  <li><strong>D19_GESAMT_ONLINE_QUOTE_12</strong> and similar features encode whether a person has no transaction in the previous 12 months, but there are some null values of which only 30% are present in the rows we are going to drop.</li>
  <li><strong>W_KEIT_KIND_HH</strong> (likelihood of a child present in this household (can be specified in child age groups)) has 8% missing values of which 60% are in rows that we are dropping.</li>
</ol>

<h3 id="microcell-rr4_id-features">Microcell (RR4_ID) Features</h3>

<ol>
  <li><strong>CAMEO_DEU_2015</strong> will need one hot encoding and has 11% missing values which should be imputed.</li>
  <li><strong>CAMEO_DEUG_2015</strong> is a less detailed CAMEO_DEU_2015 and also has 11% missing values that should be imputed.</li>
  <li><strong>KBA05_ANTG1</strong> (number of 1-2 family houses in the cell), <strong>KBA05_ANTG2</strong>, <strong>KBA05_ANTG3</strong> and <strong>KBA05_ANTG4</strong> have 15% missing values of which 74% are present in rows we are going to drop.</li>
  <li><strong>KBA05_ANHANG</strong> (share of trailers in the microcell) has 16.5% of which 67% missing values in rows we are going to drop.</li>
  <li><strong>KBA05_ALTER1</strong>, <strong>KBA05_ALTER2</strong>, <strong>KBA05_ALTER3</strong> and <strong>KBA05_ALTER4</strong> (share of car owners between X and Y years old) has 16.6% of which 67.4% missing values are in rows we are going to drop.</li>
</ol>

<h3 id="building-features">Building Features</h3>

<ol>
  <li><strong>KONSUMNAEHE</strong> (distance from a building to PoS (Point of Sale)) has 8% missing values that can’t be imputed, and fortunately 99% of these missing values are in the rows we are dropping.</li>
  <li>The rest of the features have 100% of it’s missing values in the rows we are dropping, except <strong>KBA05_HERSTTEMP</strong> (Development of the most common car manufacturers in the neighbourhood) which has 85%.</li>
</ol>

<h3 id="rr1_id-features">RR1_ID Features</h3>

<ol>
  <li><strong>ONLINE_AFFINITAET</strong> (online affinity) has only 0.5% missing values.</li>
  <li><strong>GEBAEUDETYP_RASTER</strong> (industrial areas) has 10.4% missing values of which 99% exist in the rows we are dropping.</li>
  <li><strong>MOBI_REGIO</strong> (moving patterns) has 15% missing values of which 74% exist in the rows we are dropping.</li>
  <li><strong>KKK</strong> (purchasing power) and <strong>REGIOTYP</strong> (AZ neighbourhood typology) have 17.7% missing values of which 61% exist in the rows we are dropping.</li>
</ol>

<h3 id="postcode-features">Postcode Features</h3>

<ol>
  <li>All features have 10.5% missing values of which 99% exist in the rows that we are dropping.</li>
</ol>

<h3 id="community-features">Community Features</h3>

<ol>
  <li>All features have 10.9% missing values of which 95.8% are in rows that we are dropping.</li>
</ol>

<h3 id="unknown-features">Unknown Features</h3>

<p>Since we have no information about the feature description, I looked through them looking for what they mean and tried to find which features are worth keeping and which are not.</p>

<p>I replaced 0 and -1 values in features with null as I did with other features, however in other features knew that these values were actually missing from <code class="language-plaintext highlighter-rouge">DIAS Attributes - Values 2017.xlsx</code> , but in this case it was just an assumption based on our knowledge with the data so far.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># find features that don't have categories
</span><span class="n">no_cat_feats</span> <span class="o">=</span> <span class="n">azdias_new</span><span class="p">[</span><span class="nb">list</span><span class="p">(</span><span class="nb">set</span><span class="p">(</span><span class="n">azdias_feats</span><span class="p">).</span><span class="n">difference</span><span class="p">(</span><span class="n">dias_atts_feats</span><span class="p">))].</span><span class="n">copy</span><span class="p">()</span>

<span class="c1"># change 0 and -1 to null in non-binary features
</span><span class="k">for</span> <span class="n">feat</span> <span class="ow">in</span> <span class="n">no_cat_feats</span><span class="p">.</span><span class="n">columns</span><span class="p">:</span>
    <span class="k">if</span> <span class="n">no_cat_feats</span><span class="p">[</span><span class="n">feat</span><span class="p">].</span><span class="n">nunique</span><span class="p">()</span> <span class="o">&gt;</span> <span class="mi">2</span><span class="p">:</span>
        <span class="n">no_cat_feats</span><span class="p">[</span><span class="n">feat</span><span class="p">].</span><span class="n">replace</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">np</span><span class="p">.</span><span class="n">nan</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
        <span class="n">no_cat_feats</span><span class="p">[</span><span class="n">feat</span><span class="p">].</span><span class="n">replace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">np</span><span class="p">.</span><span class="n">nan</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
        
<span class="c1"># transpose df so that features are index
</span><span class="n">no_cat_feats</span> <span class="o">=</span> <span class="n">no_cat_feats</span><span class="p">.</span><span class="n">T</span>

<span class="c1"># calculate null_percentage of features
</span><span class="n">no_cat_feats</span><span class="p">[</span><span class="s">"null_percentage"</span><span class="p">]</span> <span class="o">=</span> <span class="n">no_cat_feats</span><span class="p">.</span><span class="n">isna</span><span class="p">().</span><span class="nb">sum</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">/</span><span class="n">no_cat_feats</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>

<span class="c1"># sort by null percentage
</span><span class="n">no_cat_feats</span><span class="p">.</span><span class="n">sort_values</span><span class="p">(</span><span class="s">"null_percentage"</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
</code></pre></div></div>

<ol>
  <li><strong>KOMBIALTER</strong> translates to combial, which I have no idea what it means, but it has no missing values. However, it’s values are 1-2-3-4 and then 9, which could mean that 9 is the missing value.</li>
  <li><strong>D19_KONSUMTYP_MAX</strong> (D19 CONSUMPTION TYPE MAX) and it also has no missing values.</li>
  <li><strong>LNR</strong> has no missing values, and it looks like an individual ID so it should be dropped.</li>
  <li><strong>CJT_TYP_(1-5)</strong>, <strong>CJT_KATALOGNUTZER</strong>, <strong>RT_SCHNAEPPCHEN</strong> (RT Bargain) and <strong>RT_KEIN_ANREIZ</strong> (RT No Incentive) have 0.5% missing values which is trivial and I think it is going to be removed with dropped rows.</li>
  <li><strong>UNGLEICHENN_FLAG</strong> (Inequality flag) has 8% missing values, and I’m not sure if this feature has missing values encoded as 0 or not, but either way all of it’s missing values will be dropped with the rows.</li>
  <li><strong>EINGEZOGENAM_HH_JAHR</strong> (RECOVERED HH YEAR) has 8% missing values which will all be dropped with the rows we are dropping, and it encodes information related to year.</li>
  <li><strong>SOHO_KZ</strong> has 8% missing values, in addition to a more than 90% 0, and I don’t understand what it means.</li>
  <li><strong>AKT_DAT_KL</strong>, <strong>VK_DHT4A</strong>, <strong>VK_ZG11</strong> and <strong>VK_DISTANZ</strong>, have 8% missing values, and I couldn’t find anything related to it’s translation.</li>
  <li><strong>RT_UEBERGROESSE</strong> (RT OVER-SIZE) has 8% missing values.</li>
  <li><strong>EINGEFUEGT_AM</strong> (INSERTED_AM) is the timestamp of insertion, but which data exactly? It has 10.5% missing values.</li>
  <li><strong>DSL_FLAG</strong> and <strong>MOBI_RASTER</strong> have 10.5% missing values with no translation available.</li>
  <li><strong>KONSUMZELLE</strong> (Consumer Cell) has 10.5% missing values and only 0-1 values, so I don’t understand what does this feature mean exactly.</li>
  <li><strong>FIRMENDICHTE</strong> (COMPANY DENSITY) has 10.5% missing values, and I think that all of the featuers so far that have 10.5% missing values come from the same source, which I don’t know exactly what it is so far.</li>
  <li><strong>ANZ_STATISTISCHE_HAUSHALTE</strong> (ANZ STATISTICAL BUDGETS) has 10.5% missing values. It’s distribution seems to be right skewed.</li>
  <li><strong>STRUKTURTYP</strong> (STRUCTURE TYPE) has 10.9% missing values.</li>
  <li><strong>GEMEINDETYP</strong> (COMMUNITY TYPE) has 10.9% missing values, and it has weird values that don’t make sense (not ordinal or nominal). I could further investigate if it has any relation with other community level features.</li>
  <li><strong>UMFELD_JUNG</strong> (ENVIRONMENT YOUNG) and <strong>UMFELD_ALT</strong> (ENVIRONMENT OLD) has 10.9% missing values.</li>
  <li><strong>CAMEO_INTL_2015</strong> CAMEO is a consumer segmentation system linking address information to demographic, lifestyle and socio-economic insight. We could use KNN imputation to predict the missing values in CAMEO features since they are related to demographic data.</li>
  <li>All <strong>KB18</strong> features are PLZ8 features.</li>
  <li><strong>D19_LETZTER_KAUF_BRANCHE</strong> (D19 LAST PURCHASE SECTOR) has 28% missing values, and I think that it has valuable information, so it shouldn’t be dropped with other D19 features.</li>
  <li><strong>ALTERSKATEGORIE FEIN</strong> (AGE CATEGORY FINE) has 34% missing values (which are 0), and we know that they are missing because category 1 only has one data point, and 0 has 41188.</li>
  <li>The rest of the features have more than 50% missing values, so I’m going to drop them except <strong>ANZ_KINDER</strong> (probably the number of children), <strong>ALTER_KINDX</strong> (age of Xth child), as I understand their meaning and know that there missing values are not missing at random.</li>
</ol>

<h1 id="data-preprocessing">Data Preprocessing</h1>

<p>After the previous exploration, I was able to make some final decisions about how to clean the data for the tasks at hand.</p>

<h3 id="steps">Steps</h3>

<ol>
  <li>Clean columns with mixed types</li>
  <li>Drop columns with more than 50% missing values</li>
  <li>Replace encoded unknown values with null from Values sheet and
    <ol>
      <li>Replace <strong>ALTER_HH</strong> 0 to null</li>
      <li>Replace <strong>KOMBIALTER</strong> 9 to null</li>
    </ol>
  </li>
  <li>Features to drop:
    <ol>
      <li><strong>GEBURTSJAHR</strong> (year of birth) has 44% missing values</li>
      <li><strong>AGER_TYP</strong> (best-ager typology) has 76% missing values</li>
      <li><strong>CAMEO_DEU_2015</strong> as it is categorical and needs one-hot-encoding while <strong>CAMEO_DEUG_2015</strong> is ordinal and can be better used with PCA.</li>
      <li><strong>LNR</strong> as it is an individual identifier</li>
      <li>All features with more than 50% missing values</li>
    </ol>
  </li>
  <li>Feature engineering:
    <ol>
      <li><strong>D19_LETZTER_KAUF_BRANCHE</strong> need one hot encoding</li>
      <li><strong>MIN_GEBAEUDEJAHR</strong> should be changed to number of years between 2017 and date</li>
      <li><strong>EINGEFUEGT_AM</strong> should be changed to time between 2017 and timestamp</li>
      <li>Change <strong>ALTER_KINDX</strong> and <strong>ANZ_KINDER</strong> null values to 0</li>
      <li>Convert <strong>OST_WEST_KZ</strong> to binary labels</li>
      <li>Convert <strong>CAMEO_INTL_2015</strong> and <strong>CAMEO_DEUG_2015</strong> to int</li>
    </ol>
  </li>
  <li>Impute missing values</li>
</ol>

<h2 id="uncertainties">Uncertainties</h2>

<ol>
  <li><strong>KBA13_ANZAHL_PKW</strong> is supposed to encode the number
of cars in the PLZ8, but it has high values of peculiar number which are 1400, 1500, 1300, etc. but I’ll keep it.</li>
  <li><strong>KONSUMZELLE</strong> (Consumer Cell) has 10.5% missing
values and only 0-1 values, where 99% of these missing values will be
dropped with rows we are dropping first. I don’t understand what does
this feature mean exactly. but I’ll keep it.</li>
  <li><strong>GEMEINDETYP</strong> (COMMUNITY TYPE) has 10.9% missing
values, and it has weird values that don’t make sense (not ordinal or
nominal), so I’ll drop it.</li>
  <li><strong>RT_UEBERGROESSE</strong> (RT OVER-SIZE) has small percentage of values encoded as 0, which I don’t know if they are missing or not. 89% of it’s missing values will be dropped with rows, so I’ll keep it.</li>
</ol>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">clean_dataset</span><span class="p">(</span><span class="n">df</span><span class="p">,</span> <span class="n">p_row</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">p_col</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">drop_uncertain</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">keep_features</span><span class="o">=</span><span class="p">[]):</span>
    <span class="s">"""
    Clean dataset using insights gained during EDA.
    
    inputs
    
    1. df (pandas dataframe)
    2. p_thresh (float)  -  maximum threshold of null values in columns
    3. drop_uncertain (bool)  -  drop features we are uncertain from
    """</span>
    <span class="c1"># Make a new copy of the dataframe
</span>    <span class="n">clean_df</span> <span class="o">=</span> <span class="n">df</span><span class="p">.</span><span class="n">copy</span><span class="p">()</span>
    
    <span class="c1"># Clean columns with mixed dtypes
</span>    <span class="n">clean_df</span><span class="p">[</span><span class="s">"CAMEO_DEUG_2015"</span><span class="p">].</span><span class="n">replace</span><span class="p">(</span><span class="s">'X'</span><span class="p">,</span> <span class="n">np</span><span class="p">.</span><span class="n">nan</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
    <span class="n">clean_df</span><span class="p">[</span><span class="s">"CAMEO_INTL_2015"</span><span class="p">].</span><span class="n">replace</span><span class="p">(</span><span class="s">'XX'</span><span class="p">,</span> <span class="n">np</span><span class="p">.</span><span class="n">nan</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
    
    <span class="c1"># Replace unknown values with missing
</span>    <span class="n">clean_df</span> <span class="o">=</span> <span class="n">replace_unknown_with_null</span><span class="p">(</span><span class="n">clean_df</span><span class="p">)</span>   

    <span class="c1"># Drop rows with more than 50% missing values
</span>    <span class="n">min_count</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(((</span><span class="mi">1</span> <span class="o">-</span> <span class="n">p_row</span><span class="p">))</span><span class="o">*</span><span class="n">clean_df</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">clean_df</span><span class="p">.</span><span class="n">dropna</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">thresh</span><span class="o">=</span><span class="n">min_count</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
    
    <span class="c1"># Drop duplicated rows
</span>    <span class="n">clean_df</span><span class="p">.</span><span class="n">drop_duplicates</span><span class="p">(</span><span class="n">inplace</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
       
    <span class="c1"># Drop GEBURTSJAHR (year of birth) that has 44% missing values 
</span>    <span class="n">clean_df</span><span class="p">.</span><span class="n">drop</span><span class="p">(</span><span class="s">'GEBURTSJAHR'</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
      
    <span class="c1"># Drop LNR which is a unique indentifier
</span>    <span class="n">clean_df</span><span class="p">.</span><span class="n">drop</span><span class="p">(</span><span class="s">'LNR'</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
    
    <span class="c1"># Drop CAMEO_DEU_2015 as it's not suitable for PCA
</span>    <span class="n">clean_df</span><span class="p">.</span><span class="n">drop</span><span class="p">(</span><span class="s">'CAMEO_DEU_2015'</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
    
    <span class="c1"># Drop features with more than p_thresh missing values
</span>    <span class="n">features_missing_p</span> <span class="o">=</span> <span class="n">clean_df</span><span class="p">.</span><span class="n">isna</span><span class="p">().</span><span class="nb">sum</span><span class="p">()</span> <span class="o">/</span> <span class="n">clean_df</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">features_above_p_thresh</span> <span class="o">=</span> <span class="n">clean_df</span><span class="p">.</span><span class="n">columns</span><span class="p">[</span><span class="n">features_missing_p</span> <span class="o">&gt;</span> <span class="n">p_col</span><span class="p">]</span>
    
    <span class="n">features_to_keep</span> <span class="o">=</span> <span class="p">[</span><span class="s">"ALTER_KIND1"</span><span class="p">,</span> <span class="s">"ALTER_KIND2"</span><span class="p">,</span> <span class="s">"ALTER_KIND3"</span><span class="p">,</span> <span class="s">"ALTER_KIND4"</span><span class="p">,</span> <span class="s">"ANZ_KINDER"</span><span class="p">]</span> <span class="o">+</span> <span class="n">keep_features</span>
    <span class="n">features_to_remove</span> <span class="o">=</span> <span class="p">[</span><span class="n">feat</span> <span class="k">for</span> <span class="n">feat</span> <span class="ow">in</span> <span class="n">features_above_p_thresh</span> <span class="k">if</span> <span class="n">feat</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">features_to_keep</span><span class="p">]</span>
    
    <span class="n">clean_df</span><span class="p">.</span><span class="n">drop</span><span class="p">(</span><span class="n">features_to_remove</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
    
    <span class="c1"># Drop uncertain features
</span>    <span class="k">if</span> <span class="n">drop_uncertain</span><span class="p">:</span>
        <span class="n">uncertain_features</span> <span class="o">=</span> <span class="p">[</span><span class="s">"GEMEINDETYP"</span><span class="p">]</span>
        <span class="n">clean_df</span><span class="p">.</span><span class="n">drop</span><span class="p">(</span><span class="n">uncertain_features</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
        
    <span class="c1"># Feature Engineering
</span>    <span class="c1"># One Hot Encoding D19_LETZTER_KAUF_BRANCHE
</span>    <span class="n">dummies</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">get_dummies</span><span class="p">(</span><span class="n">clean_df</span><span class="p">[</span><span class="s">"D19_LETZTER_KAUF_BRANCHE"</span><span class="p">],</span> <span class="n">prefix</span><span class="o">=</span><span class="s">"D19_LETZTER_KAUF_BRANCHE"</span><span class="p">)</span>
    <span class="n">clean_df</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">concat</span><span class="p">([</span><span class="n">clean_df</span><span class="p">,</span> <span class="n">dummies</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">clean_df</span><span class="p">.</span><span class="n">drop</span><span class="p">(</span><span class="s">"D19_LETZTER_KAUF_BRANCHE"</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
    
    <span class="c1"># Calculate year difference in MIN_GEBAEUDEJAHR
</span>    <span class="n">clean_df</span><span class="p">[</span><span class="s">"MIN_GEBAEUDEJAHR_ENG"</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="mi">2017</span> <span class="o">-</span> <span class="n">clean_df</span><span class="p">[</span><span class="s">"MIN_GEBAEUDEJAHR"</span><span class="p">])</span>
    <span class="n">clean_df</span><span class="p">.</span><span class="n">drop</span><span class="p">(</span><span class="s">"MIN_GEBAEUDEJAHR"</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
    
    <span class="c1"># Calculate days difference in EINGEFUEGT_AM
</span>    <span class="n">current</span> <span class="o">=</span> <span class="n">datetime</span><span class="p">.</span><span class="n">strptime</span><span class="p">(</span><span class="s">"2017-01-01"</span><span class="p">,</span> <span class="s">"%Y-%m-%d"</span><span class="p">)</span>
    <span class="n">clean_df</span><span class="p">[</span><span class="s">"EINGEFUEGT_AM_DAY"</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="n">current</span> <span class="o">-</span> <span class="n">pd</span><span class="p">.</span><span class="n">to_datetime</span><span class="p">(</span><span class="n">clean_df</span><span class="p">[</span><span class="s">"EINGEFUEGT_AM"</span><span class="p">])).</span><span class="n">dt</span><span class="p">.</span><span class="n">days</span>
    <span class="n">clean_df</span><span class="p">.</span><span class="n">drop</span><span class="p">(</span><span class="s">"EINGEFUEGT_AM"</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
    
    <span class="c1"># Replace null values in ALTER_KIND and ANZ_KINDER with 0 to avoid imputation
</span>    <span class="k">for</span> <span class="n">feat</span> <span class="ow">in</span> <span class="n">clean_df</span><span class="p">.</span><span class="n">columns</span><span class="p">[</span><span class="n">clean_df</span><span class="p">.</span><span class="n">columns</span><span class="p">.</span><span class="nb">str</span><span class="p">.</span><span class="n">startswith</span><span class="p">(</span><span class="s">"ALTER_KIND"</span><span class="p">)]:</span>
        <span class="n">clean_df</span><span class="p">[</span><span class="n">feat</span><span class="p">].</span><span class="n">replace</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">nan</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
    <span class="n">clean_df</span><span class="p">[</span><span class="s">"ANZ_KINDER"</span><span class="p">].</span><span class="n">replace</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">nan</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
    
    <span class="c1"># Convert OST_WEST_KZ to binary labels
</span>    <span class="n">clean_df</span><span class="p">[</span><span class="s">"OST_WEST_KZ"</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="n">clean_df</span><span class="p">[</span><span class="s">"OST_WEST_KZ"</span><span class="p">]</span> <span class="o">==</span> <span class="s">"W"</span><span class="p">).</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">uint8</span><span class="p">)</span>
    
    <span class="c1"># Convert CAMEO_INTL_2015 and CAMEO_DEUG_2015 to float32
</span>    <span class="n">CAMEO_feats</span> <span class="o">=</span> <span class="p">[</span><span class="s">"CAMEO_INTL_2015"</span><span class="p">,</span> <span class="s">"CAMEO_DEUG_2015"</span><span class="p">]</span>
    <span class="n">clean_df</span><span class="p">[</span><span class="n">CAMEO_feats</span><span class="p">]</span> <span class="o">=</span> <span class="n">clean_df</span><span class="p">[</span><span class="n">CAMEO_feats</span><span class="p">].</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">float32</span><span class="p">)</span>

    <span class="c1"># Convert float16 features to float32 to enable arithmetic operations
</span>    <span class="n">float_feats</span> <span class="o">=</span> <span class="n">clean_df</span><span class="p">.</span><span class="n">select_dtypes</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">float16</span><span class="p">).</span><span class="n">columns</span>
    <span class="n">clean_df</span><span class="p">[</span><span class="n">float_feats</span><span class="p">]</span> <span class="o">=</span> <span class="n">clean_df</span><span class="p">[</span><span class="n">float_feats</span><span class="p">].</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">float32</span><span class="p">)</span>
    
    <span class="k">return</span> <span class="n">clean_df</span>
</code></pre></div></div>

<p>The last step remaining was imputing the data after this cleaning pipeline, so I checked the percentage of missing values in features after it.</p>

<p><img src="/images/arvato/missing_after_cleaning.png" alt="missing after cleaning" /></p>

<p>We can see now that imputations won’t be a big deal, and that there is very small number of features that exceed 10% missing values.</p>

<h3 id="but-how-shall-we-impute-that-data-there-are-multiple-ways-we-could-that-including">But how shall we impute that data? There are multiple ways we could that including:</h3>

<ol>
  <li>Imputing missing values with 0 or -1 which was used originally in the dataset</li>
  <li>Imputing missing values with mean, median or mode and adding and indicator (or not) for missing data</li>
  <li>KNN imputation</li>
  <li>Iterative imputation using linear regression</li>
</ol>

<p>My gut feeling says that I should go with KNN imputation, as I think that features of the same category could be of huge aid in determining which value to impute with, instead of imputing with 0 or mode (since we are dealing with ordinal data).</p>

<p>But KNN imputation will be time and memory intensive, so we can’t do on all features. So we probably should stick with an easy strategy like mode imputation for features below a threshold and KNN for features above.</p>

<h3 id="but-what-that-shall-that-threshold-be">But what that shall that threshold be?</h3>

<p>Since KNN is memory and time intensive, we need the majority of features to be imputed using mode values, and only small number of features shall be imputed using KNN.</p>

<p><img src="/images/arvato/missing_after_cleaning_box.png" alt="missing after cleaning box" /></p>

<p>Using this box plot we can see that we can only impute features above 10% using KNN. So in order to do that we need to first impute features lower than 10% using mode imputation, then impute the rest of the features using KNN.</p>

<h3 id="after-several-trials-it-seemed-that-knn-takes-a-really-long-time">After several trials, it seemed that KNN takes a really long time.</h3>

<p>So instead, I imputed the remaining 10 features using mean imputation to avoid introducing any spike in the data since we are dealing with features that might have more than 20% missing values.</p>

<p>I have used sklearn’s ColumnTransformer to impute each set of features using their specified method.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Features below 10% null percentage
</span><span class="n">features_below_10</span> <span class="o">=</span> <span class="n">na_features</span><span class="p">[</span><span class="n">na_features</span> <span class="o">&lt;</span> <span class="mf">0.1</span><span class="p">].</span><span class="n">index</span>

<span class="c1"># Features above 10% null percentage
</span><span class="n">features_above_10</span> <span class="o">=</span> <span class="n">na_features</span><span class="p">[</span><span class="n">na_features</span> <span class="o">&gt;</span> <span class="mf">0.1</span><span class="p">].</span><span class="n">index</span>

<span class="n">imputer</span> <span class="o">=</span> <span class="n">ColumnTransformer</span><span class="p">([</span>
    <span class="p">(</span><span class="s">'mode'</span><span class="p">,</span> <span class="n">SimpleImputer</span><span class="p">(</span><span class="n">strategy</span><span class="o">=</span><span class="s">"most_frequent"</span><span class="p">),</span> <span class="n">features_below_10</span><span class="p">),</span>
    <span class="p">(</span><span class="s">'mean'</span><span class="p">,</span> <span class="n">SimpleImputer</span><span class="p">(</span><span class="n">strategy</span><span class="o">=</span><span class="s">"mean"</span><span class="p">),</span> <span class="n">features_above_10</span><span class="p">)</span>
<span class="p">],</span> <span class="n">remainder</span><span class="o">=</span><span class="s">"passthrough"</span><span class="p">)</span>

<span class="n">imputed_azdias</span> <span class="o">=</span> <span class="n">imputer</span><span class="p">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">clean_azdias</span><span class="p">).</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">float32</span><span class="p">)</span>

</code></pre></div></div>

<p>After finally imputing <code class="language-plaintext highlighter-rouge">AZDIAS</code> we were ready to go through with the first task which customer segmentation.</p>

<h1 id="customer-segmentation-1">Customer Segmentation</h1>

<p>As I mentioned in the beginning, in this part we are supposed to find the part of the general population that represents the core customer base of the business.</p>

<p>In order to do that we need to the following:</p>

<ol>
  <li>Cluster the general population dataset (<code class="language-plaintext highlighter-rouge">AZDIAS</code>)</li>
  <li>Predict the clusters of our customers dataset (<code class="language-plaintext highlighter-rouge">CUSTOMERS</code>)</li>
  <li>Find the clusters that are over represented in <code class="language-plaintext highlighter-rouge">CUSTOMERS</code></li>
  <li>Analyze these cluster in <code class="language-plaintext highlighter-rouge">AZDIAS</code></li>
</ol>

<p>And in order to cluster either of the datasets we need to reduce the dimensions of the datasets using PCA, as it could improve the results of K-Means clustering algorithm.</p>

<p>This was done by first training the PCA on <code class="language-plaintext highlighter-rouge">AZDIAS</code>, and then reduce <code class="language-plaintext highlighter-rouge">CUSTOMERS</code> using the trained algorithm.</p>

<p>The data needs to be scaled for PCA to obtain good results, so I decided to use sklearn’s StandardScaler. However, since the data was huge, I decided to incrementally fit both the StandardScaler and PCA, as fitting them normally took a long time.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">batch_fit_scaler</span><span class="p">(</span><span class="n">scaler</span><span class="p">,</span> <span class="n">data</span><span class="p">,</span> <span class="n">n_batches</span><span class="o">=</span><span class="mi">100</span><span class="p">):</span>
    <span class="s">"""
    Fit a scaler to data through n batches.
    
    Input:
    transfromer (scikit-learn Scaler object)
    data (numpy array or pandas dataframe)
    n_batches (int)  -  number of batches for fitting the scaler
    
    Output:
    Fitted Scaler
    """</span>
    <span class="k">for</span> <span class="n">X_batch</span> <span class="ow">in</span> <span class="n">tqdm</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">array_split</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">n_batches</span><span class="p">),</span> <span class="n">total</span><span class="o">=</span><span class="n">n_batches</span><span class="p">):</span>
        <span class="n">scaler</span><span class="p">.</span><span class="n">partial_fit</span><span class="p">(</span><span class="n">X_batch</span><span class="p">)</span>
        
    <span class="k">return</span> <span class="n">scaler</span>

<span class="k">def</span> <span class="nf">batch_fit_pca</span><span class="p">(</span><span class="n">pca</span><span class="p">,</span> <span class="n">scaler</span><span class="p">,</span> <span class="n">data</span><span class="p">,</span> <span class="n">n_batches</span><span class="o">=</span><span class="mi">100</span><span class="p">):</span>
    <span class="s">"""
    Fit an Incremental PCA to scaled data through n batches.
    
    Input:
    pca (scikit-learn IncrementalPCA object)
    scaler (scikit-learn Scaler object)
    data (numpy array or pandas dataframe)
    n_batches (int)  -  number of batches for fitting the transformer
    
    Output:
    Fitted IncrementalPCA
    """</span>
    <span class="k">for</span> <span class="n">X_batch</span> <span class="ow">in</span> <span class="n">tqdm</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">array_split</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">n_batches</span><span class="p">),</span> <span class="n">total</span><span class="o">=</span><span class="n">n_batches</span><span class="p">):</span>
        <span class="n">scaled_X_batch</span> <span class="o">=</span> <span class="n">scaler</span><span class="p">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X_batch</span><span class="p">)</span>
        <span class="n">pca</span><span class="p">.</span><span class="n">partial_fit</span><span class="p">(</span><span class="n">scaled_X_batch</span><span class="p">)</span>
    
    <span class="k">return</span> <span class="n">pca</span>
        

<span class="k">def</span> <span class="nf">batch_transform_pca</span><span class="p">(</span><span class="n">pca</span><span class="p">,</span> <span class="n">scaler</span><span class="p">,</span> <span class="n">data</span><span class="p">,</span> <span class="n">n_batches</span><span class="o">=</span><span class="mi">100</span><span class="p">):</span>
    <span class="s">"""
    Transform large data using fitted pca.
    
    Input:
    pca (Fitted IncrementalPCA)
    scaler (Fitted Scaler)
    data (numpy array or pandas dataframe)
    n_batches (int)  -  number of batches for fitting the transformer
    
    Output:
    Transformed data
    """</span>
    <span class="n">pca_data</span> <span class="o">=</span> <span class="bp">None</span>
    <span class="k">for</span> <span class="n">X_batch</span> <span class="ow">in</span> <span class="n">tqdm</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">array_split</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">n_batches</span><span class="p">),</span> <span class="n">total</span><span class="o">=</span><span class="n">n_batches</span><span class="p">):</span>
        <span class="n">scaled_X_batch</span> <span class="o">=</span> <span class="n">scaler</span><span class="p">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X_batch</span><span class="p">)</span>
        <span class="n">pca_X_batch</span> <span class="o">=</span> <span class="n">pca</span><span class="p">.</span><span class="n">transform</span><span class="p">(</span><span class="n">scaled_X_batch</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">pca_data</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>
            <span class="n">pca_data</span> <span class="o">=</span> <span class="n">pca_X_batch</span>
        <span class="k">else</span><span class="p">:</span> 
            <span class="n">pca_data</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">vstack</span><span class="p">([</span><span class="n">pca_data</span><span class="p">,</span> <span class="n">pca_X_batch</span><span class="p">])</span>
    <span class="k">return</span> <span class="n">pca_data</span>

<span class="n">scaler</span> <span class="o">=</span> <span class="n">batch_fit_scaler</span><span class="p">(</span><span class="n">scaler</span><span class="p">,</span> <span class="n">imputed_azdias</span><span class="p">)</span>
<span class="n">pca</span> <span class="o">=</span> <span class="n">batch_fit_pca</span><span class="p">(</span><span class="n">pca</span><span class="p">,</span> <span class="n">scaler</span><span class="p">,</span> <span class="n">imputed_azdias</span><span class="p">)</span>

<span class="n">cumsum</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">cumsum</span><span class="p">(</span><span class="n">pca</span><span class="p">.</span><span class="n">explained_variance_ratio_</span><span class="p">)</span>
<span class="n">d</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">cumsum</span> <span class="o">&gt;=</span> <span class="mf">0.95</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span>
<span class="k">print</span><span class="p">(</span><span class="s">"Minimum number of dimensions that have 95% of original data variance:"</span><span class="p">,</span> <span class="n">d</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-markdown highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Minimum number of dimensions that have 95% of original data variance: 224
</code></pre></div></div>

<p>After fitting them, I used the cumulative explained variance of each principal component to find the number of them which explains 95% of variability in the dataset, and I found that to be 224.</p>

<p>Then I proceeded with clustering using sklearn’s MiniBatchKMeans, and I tested several values for the number of clusters to use and compared the results of their <strong>Inertia</strong> (Average distance between each point and it’s cluster center).</p>

<p><img src="/images/arvato/kmeans_intertia.png" alt="kmeans inertia" /></p>

<p>As we can see, the more clusters we set, the better the score we get. But this keeps going forever, so we need to find the elbow point, and I think that is 7.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">k</span> <span class="o">=</span> <span class="mi">7</span>
<span class="n">init</span> <span class="o">=</span> <span class="s">"k-means++"</span>
<span class="n">n_init</span> <span class="o">=</span> <span class="mi">10</span>
<span class="n">n_batches</span> <span class="o">=</span> <span class="mi">100</span>
<span class="n">batch_size</span> <span class="o">=</span> <span class="n">pca_azdias</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">//</span><span class="n">n_batches</span>
<span class="n">seed</span> <span class="o">=</span> <span class="mi">42</span>

<span class="n">kmeans</span> <span class="o">=</span> <span class="n">MiniBatchKMeans</span><span class="p">(</span><span class="n">n_clusters</span><span class="o">=</span><span class="n">k</span><span class="p">,</span> <span class="n">init</span><span class="o">=</span><span class="n">init</span><span class="p">,</span> <span class="n">n_init</span><span class="o">=</span><span class="n">n_init</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="n">seed</span><span class="p">)</span>
<span class="n">kmeans</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">pca_azdias</span><span class="p">)</span>

<span class="c1"># Calculate percentage of each cluster in azdias
</span><span class="n">azdias_clusters_p</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">Series</span><span class="p">(</span><span class="n">kmeans</span><span class="p">.</span><span class="n">labels_</span><span class="p">).</span><span class="n">value_counts</span><span class="p">()</span><span class="o">/</span><span class="nb">len</span><span class="p">(</span><span class="n">kmeans</span><span class="p">.</span><span class="n">labels_</span><span class="p">)</span>
</code></pre></div></div>

<p>After fitting, it was time to do the same, but for <code class="language-plaintext highlighter-rouge">CUSTOMERS</code> . After that, we shall compare each cluster representation in both populations to finally find out the core customer base of the business.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Clean the customers dataset using the pipeline we made earlier
</span><span class="n">clean_customers</span> <span class="o">=</span> <span class="n">clean_dataset</span><span class="p">(</span><span class="n">customers</span><span class="p">,</span> <span class="n">keep_features</span><span class="o">=</span><span class="p">[</span><span class="s">"KBA13_ANTG4"</span><span class="p">])</span>

<span class="c1"># Re-arrange columns to be just as clean_azdias to avoid any problems with imputer
</span><span class="n">clean_customers</span> <span class="o">=</span> <span class="n">clean_customers</span><span class="p">[</span><span class="n">clean_azdias</span><span class="p">.</span><span class="n">columns</span><span class="p">]</span>

<span class="c1"># Impute missing values using imputer fitted on clean_azdias
</span><span class="n">imputed_customers</span> <span class="o">=</span> <span class="n">imputer</span><span class="p">.</span><span class="n">transform</span><span class="p">(</span><span class="n">clean_customers</span><span class="p">)</span>

<span class="c1"># transform using scaler
</span><span class="n">scaled_customers</span> <span class="o">=</span> <span class="n">scaler</span><span class="p">.</span><span class="n">transform</span><span class="p">(</span><span class="n">imputed_customers</span><span class="p">)</span>

<span class="c1"># transform using pca
</span><span class="n">pca_customers</span> <span class="o">=</span> <span class="n">pca</span><span class="p">.</span><span class="n">transform</span><span class="p">(</span><span class="n">scaled_customers</span><span class="p">)[:,</span> <span class="p">:</span><span class="n">d</span><span class="p">]</span>

<span class="c1"># Predict KMeans labels
</span><span class="n">customers_clusters</span> <span class="o">=</span> <span class="n">kmeans</span><span class="p">.</span><span class="n">predict</span><span class="p">(</span><span class="n">pca_customers</span><span class="p">)</span>

<span class="c1"># Calculate percentage of each cluster in customers
</span><span class="n">customers_clusters_p</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">Series</span><span class="p">(</span><span class="n">customers_clusters</span><span class="p">).</span><span class="n">value_counts</span><span class="p">()</span><span class="o">/</span><span class="nb">len</span><span class="p">(</span><span class="n">customers_clusters</span><span class="p">)</span>
</code></pre></div></div>

<p><img src="/image/arvato/clusters.png" alt="clusters" /></p>

<ol>
  <li>We can see an over representation of <strong>cluster 0</strong> in <code class="language-plaintext highlighter-rouge">CUSTOMERS</code> when compared to <code class="language-plaintext highlighter-rouge">AZDIAS</code>, with over <strong>40%</strong> of <code class="language-plaintext highlighter-rouge">CUSTOMERS</code> being in this cluster.</li>
  <li><strong>Clusters 4</strong> and <strong>6</strong> percentages in <code class="language-plaintext highlighter-rouge">CUSTOMERS</code> also exceed their counterparts in <code class="language-plaintext highlighter-rouge">AZDIAS</code>.</li>
  <li>The most rare cluster in <code class="language-plaintext highlighter-rouge">CUSTOMERS</code> is <strong>cluster 5</strong>, followed by <strong>3, 2</strong> and <strong>1</strong>.</li>
  <li>Therefore, the clusters that are <strong><em>more generally inclined</em></strong> to be customers are <strong>0, 4</strong> and <strong>6</strong>.</li>
  <li>While the clusters that are <strong><em>less inclined</em></strong> to be customers are <strong>1, 2, 3,</strong> and <strong>5</strong>.</li>
</ol>

<p>We have a lot features, so instead of looking into the difference between customers and non-customers in all of them, I skimmed the difference between cluster 0 (the most frequent customer cluster) and cluster 5 (the most frequent non-customer cluster) in all of the features to find the ones which we can use to understand the difference between the whole customer base and the rest of the population.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">compare_features</span><span class="p">(</span><span class="n">dfs</span><span class="o">=</span><span class="p">[],</span> <span class="n">labels</span><span class="o">=</span><span class="p">[]):</span>
    <span class="s">"""Plots all features of passes dataframes for comparison"""</span>
    <span class="c1"># parameters of subplot
</span>    <span class="n">nrows</span> <span class="o">=</span> <span class="mi">1</span>
    <span class="n">ncols</span> <span class="o">=</span> <span class="mi">2</span>

    <span class="c1"># number of features
</span>    <span class="n">nfeats</span> <span class="o">=</span> <span class="n">feat_cat_df</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    
    <span class="c1"># colors 
</span>    <span class="n">colors</span> <span class="o">=</span> <span class="s">"bgrcmy"</span>
    

    <span class="c1"># loop over 4 features at a time to plot them in a row
</span>    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">nfeats</span><span class="p">,</span> <span class="n">ncols</span><span class="p">):</span>

        <span class="c1"># make subplots
</span>        <span class="n">fig</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">nrows</span><span class="o">=</span><span class="n">nrows</span><span class="p">,</span> <span class="n">ncols</span><span class="o">=</span><span class="n">ncols</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">3</span><span class="p">));</span>

        <span class="c1"># loop over each of 4 features
</span>        <span class="k">for</span> <span class="n">j</span><span class="p">,</span> <span class="n">feat_idx</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">i</span><span class="o">+</span><span class="n">ncols</span><span class="p">)):</span>
            <span class="c1"># feature name and category
</span>            <span class="n">feat</span> <span class="o">=</span> <span class="n">feat_cat_df</span><span class="p">.</span><span class="n">iloc</span><span class="p">[</span><span class="n">feat_idx</span><span class="p">].</span><span class="n">feature</span>
            <span class="n">cat</span> <span class="o">=</span> <span class="n">feat_cat_df</span><span class="p">.</span><span class="n">iloc</span><span class="p">[</span><span class="n">feat_idx</span><span class="p">].</span><span class="n">category</span>
            
            <span class="c1"># ldict with label as key and df feature as value
</span>            <span class="n">dfs_feat</span> <span class="o">=</span> <span class="p">{}</span>
            <span class="k">for</span> <span class="n">df</span><span class="p">,</span> <span class="n">label</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">dfs</span><span class="p">,</span> <span class="n">labels</span><span class="p">):</span>
                <span class="n">dfs_feat</span><span class="p">[</span><span class="n">label</span><span class="p">]</span> <span class="o">=</span> <span class="n">df</span><span class="p">.</span><span class="n">loc</span><span class="p">[:,</span> <span class="n">feat</span><span class="p">]</span>

            <span class="c1"># plot using histogram if unique values exceeds 11
</span>            <span class="k">if</span> <span class="n">dfs_feat</span><span class="p">[</span><span class="n">label</span><span class="p">].</span><span class="n">nunique</span><span class="p">()</span> <span class="o">&gt;</span> <span class="mi">16</span><span class="p">:</span>
                <span class="k">for</span> <span class="p">(</span><span class="n">label</span><span class="p">,</span> <span class="n">df_feat</span><span class="p">),</span> <span class="n">color</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">dfs_feat</span><span class="p">.</span><span class="n">items</span><span class="p">(),</span> <span class="n">colors</span><span class="p">):</span>
                    <span class="n">df_feat</span><span class="p">.</span><span class="n">hist</span><span class="p">(</span><span class="n">bins</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">color</span><span class="p">,</span> <span class="n">density</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.4</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axes</span><span class="p">[</span><span class="n">j</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="n">label</span><span class="p">)</span>
                <span class="n">axes</span><span class="p">[</span><span class="n">j</span><span class="p">].</span><span class="n">legend</span><span class="p">()</span>

            <span class="c1"># plot using bar chart
</span>            <span class="k">else</span><span class="p">:</span>
                <span class="n">columns</span> <span class="o">=</span> <span class="p">[]</span>
                <span class="n">feats_p</span> <span class="o">=</span> <span class="bp">None</span>
                <span class="c1"># concatenate all df features values counts in one dataframe and plot bar plot
</span>                <span class="k">for</span> <span class="n">label</span><span class="p">,</span> <span class="n">df_feat</span> <span class="ow">in</span> <span class="n">dfs_feat</span><span class="p">.</span><span class="n">items</span><span class="p">():</span>
                    <span class="n">columns</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">label</span><span class="p">)</span>
                    <span class="n">feat_p</span> <span class="o">=</span> <span class="n">df_feat</span><span class="p">.</span><span class="n">value_counts</span><span class="p">()</span><span class="o">/</span><span class="n">df_feat</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
                    <span class="n">feats_p</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">concat</span><span class="p">([</span><span class="n">feats_p</span><span class="p">,</span> <span class="n">feat_p</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
                <span class="n">feats_p</span><span class="p">.</span><span class="n">columns</span> <span class="o">=</span> <span class="n">columns</span>
                <span class="n">feats_p</span><span class="p">.</span><span class="n">sort_index</span><span class="p">().</span><span class="n">plot</span><span class="p">(</span><span class="n">kind</span><span class="o">=</span><span class="s">"bar"</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axes</span><span class="p">[</span><span class="n">j</span><span class="p">])</span>

            <span class="c1"># assign unknown feature description 
</span>            <span class="n">feat_desc</span> <span class="o">=</span> <span class="s">'Unknown'</span>

            <span class="c1"># if feature description exists include instead of unknown
</span>            <span class="k">if</span> <span class="n">feat</span> <span class="ow">in</span> <span class="n">known_feats</span><span class="p">:</span>
                <span class="n">feat_desc</span> <span class="o">=</span> <span class="n">dias_atts</span><span class="p">[</span><span class="n">dias_atts</span><span class="p">[</span><span class="s">"Attribute"</span><span class="p">]</span> <span class="o">==</span> <span class="n">feat</span><span class="p">][</span><span class="s">"Description"</span><span class="p">].</span><span class="n">item</span><span class="p">()</span>

            <span class="c1"># set title as feature, category and description
</span>            <span class="n">axes</span><span class="p">[</span><span class="n">j</span><span class="p">].</span><span class="n">set_title</span><span class="p">(</span><span class="sa">f</span><span class="s">"</span><span class="si">{</span><span class="n">feat</span><span class="si">}</span><span class="se">\n</span><span class="si">{</span><span class="n">cat</span><span class="si">}</span><span class="se">\n</span><span class="si">{</span><span class="n">feat_desc</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>

<span class="c1"># Filtering individuals from cluster 0 in AZDIAS
</span><span class="n">cluster_0</span> <span class="o">=</span> <span class="n">clean_azdias</span><span class="p">[</span><span class="n">kmeans</span><span class="p">.</span><span class="n">labels_</span> <span class="o">==</span> <span class="mi">0</span><span class="p">]</span>

<span class="c1"># Filtering individuals from cluster 5 in AZDIAS
</span><span class="n">cluster_5</span> <span class="o">=</span> <span class="n">clean_azdias</span><span class="p">[</span><span class="n">kmeans</span><span class="p">.</span><span class="n">labels_</span> <span class="o">==</span> <span class="mi">5</span><span class="p">]</span>

<span class="n">compare_features</span><span class="p">([</span><span class="n">cluster_0</span><span class="p">,</span> <span class="n">cluster_5</span><span class="p">],</span> <span class="p">[</span><span class="s">"Cluster 0"</span><span class="p">,</span> <span class="s">"Cluster 5"</span><span class="p">])</span>
</code></pre></div></div>

<p>Through skimming over the plots of all of the features in these two clusters, I selected the following features to explore the difference between the full customer base and the rest of the population.</p>

<ol>
  <li><strong>ALTERSKATEGORIE_GROB</strong> (Age through prename analysis)</li>
  <li><strong>ANREDE_KZ</strong> (Gender)</li>
  <li><strong>CJT_GESMATTYP</strong> (Preferred information and buying channels)</li>
  <li><strong>FINANZTYP</strong> (Financial type)</li>
  <li><strong>LP_LEBENSPHASE_FEIN</strong> (Lifestage)</li>
  <li><strong>RETOURTYP_BK_S</strong> (Return type)</li>
  <li><strong>ALTER_HH</strong> (Main age within household)</li>
  <li><strong>HH_EINKOMMEN_SCORE</strong> (Estimated household net income)</li>
  <li><strong>WOHNLAGE</strong> (Neighbourhood area)</li>
  <li><strong>MOBI_REGIO</strong> (Moving patterns)</li>
</ol>

<p><strong>There are more features that also emphasize differences between customers and non-customers, however I found that they offer redundant information.</strong></p>

<p>There are also some features that shows no difference between the two groups, specifically features that are related to motor vehicles information in PLZ8 areas or microcells. Which indicated that we might want to use some sort of feature selection in the final machine learning pipeline.</p>

<p>My intuition is that we should visualize the differences between cluster 0-4-6 and clusters 2-3-5 leaving out cluster 1, as individuals in this cluster have really similar tendencies of being customers or non-customers. So that would decrease the sharpness of differences between the two groups.</p>

<p>I shall also visualize the difference between the clusters of the customer base in order to further understand them.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">customers</span> <span class="o">=</span> <span class="n">clean_azdias</span><span class="p">[</span><span class="n">np</span><span class="p">.</span><span class="n">in1d</span><span class="p">(</span><span class="n">kmeans</span><span class="p">.</span><span class="n">labels_</span><span class="p">,</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">6</span><span class="p">])]</span>
<span class="n">cluster_0</span> <span class="o">=</span> <span class="n">clean_azdias</span><span class="p">[</span><span class="n">kmeans</span><span class="p">.</span><span class="n">labels_</span> <span class="o">==</span> <span class="mi">0</span><span class="p">]</span>
<span class="n">cluster_4</span> <span class="o">=</span> <span class="n">clean_azdias</span><span class="p">[</span><span class="n">kmeans</span><span class="p">.</span><span class="n">labels_</span> <span class="o">==</span> <span class="mi">4</span><span class="p">]</span>
<span class="n">cluster_6</span> <span class="o">=</span> <span class="n">clean_azdias</span><span class="p">[</span><span class="n">kmeans</span><span class="p">.</span><span class="n">labels_</span> <span class="o">==</span> <span class="mi">6</span><span class="p">]</span>

<span class="n">non_customers</span> <span class="o">=</span> <span class="n">clean_azdias</span><span class="p">[</span><span class="n">np</span><span class="p">.</span><span class="n">in1d</span><span class="p">(</span><span class="n">kmeans</span><span class="p">.</span><span class="n">labels_</span><span class="p">,</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">])]</span>
<span class="n">cluster_2</span> <span class="o">=</span> <span class="n">clean_azdias</span><span class="p">[</span><span class="n">kmeans</span><span class="p">.</span><span class="n">labels_</span> <span class="o">==</span> <span class="mi">2</span><span class="p">]</span>
<span class="n">cluster_3</span> <span class="o">=</span> <span class="n">clean_azdias</span><span class="p">[</span><span class="n">kmeans</span><span class="p">.</span><span class="n">labels_</span> <span class="o">==</span> <span class="mi">3</span><span class="p">]</span>
<span class="n">cluster_5</span> <span class="o">=</span> <span class="n">clean_azdias</span><span class="p">[</span><span class="n">kmeans</span><span class="p">.</span><span class="n">labels_</span> <span class="o">==</span> <span class="mi">5</span><span class="p">]</span>
</code></pre></div></div>

<h2 id="age">Age</h2>

<p><img src="/images/arvato/Age.png" alt="Age Analysis" /></p>

<h4 id="customers-and-non-customers">Customers and Non-Customers</h4>

<p>We can see that customers have greater probability of being older, with almost 80% being above 45 years old. On the other hand non-customers tend have more than 50% of less than 46 years old. The age groups that is mostly shared between the two groups is 46-460 years group.</p>

<h4 id="customer-clusters">Customer Clusters</h4>

<p>We can see that cluster 4 stand out with higher percentage of indiviudals less than 46 years old, while cluster 0 has more than 90% of it’s population above 46 years old.</p>

<p>So cluster 0 includes is mostly elders with the majority being above 60 years old, while cluster 4 has the majority above 45 but also has higher than average percentage of younger indivduals, and cluster 6 is similar to cluster 0 except that the percentage of 46-60 years indivduals is larger than cluster 0.</p>

<h2 id="gender">Gender</h2>

<p><img src="/images/arvato/Gender.png" alt="Gender Analysis" /></p>

<h4 id="customers-and-non-customers-1">Customers and Non-Customers</h4>

<p>The percentage of males in customers is higher than that in non-customers, while the percentage of females in both is higher than males.</p>

<h4 id="customer-clusters-1">Customer Clusters</h4>

<p>Cluster 0 has an over representation of males, where males is higher than all clusters and higher than female percentage in the same cluster. While cluster 4 and 6 have higher female percentages than cluster 0.</p>

<h2 id="preferred-information-and-buying-channels">Preferred Information and Buying Channels</h2>

<p><img src="/images/arvato/Channels.png" alt="Channels Analysis" /></p>

<h4 id="customer-and-non-customers">Customer and Non-Customers</h4>

<p>We can see than customers exceed non-customers in percentages of advertising and consumption minamilists and traditionalists, while non-customers tend to be more open in that spectrum.</p>

<h4 id="customer-clusters-2">Customer Clusters</h4>

<p>Since cluster 0 mostly represents elderly individuals, it’s expected that they will be over represented in the minimalists and traditionlists. And also since cluster 4 represents the younger customers, we don’t see alot of them as minimalist and traditionalists. And finally we can see that cluster 6 has the most uniform distribtution across the spectrum.</p>

<h2 id="financial-type">Financial Type</h2>

<p><img src="/images/arvato/Financial.png" alt="Financial Analysis" /></p>

<h4 id="customers-and-non-customers-2">Customers and Non-Customers</h4>

<p>20% of customers are money savers, while another 20% are inverstors, and around 35% are unremarkable which I guess means that they have no specific financial type. On the other hand, non-customers tend to have low financial interest.</p>

<h4 id="customer-clusters-3">Customer Clusters</h4>

<p>We can that the majority of cluster 0 with distinguished financial type are money savers, while in cluster 6 they are investors. Cluster 4 doesn’t show a specific type.</p>

<h2 id="life-stage">Life Stage</h2>

<p><img src="/images/arvato/Life_Stage.png" alt="Life Stage Analysis" /></p>

<h4 id="customers-and-non-customers-3">Customers and Non-Customers</h4>

<p>The most frequent non-customer type is single low-income and average earners of younger age, while customers’ most frequent type is singe high-income earner. However, there is no great difference between the most frequent value of customers and two next most frequent values, indicating the difference between clusters.</p>

<h4 id="customer-clusters-4">Customer Clusters</h4>

<p>Around 70% of cluster 6 are single, with the majority of them being single low-income average earners of higher age., while the most frequent type in cluster 0 is single high-income earners, while cluster 4’s most frequent type is high income earner of higher age from multiperson households. However, the remaining majority of cluster 4 types falls in younger aged families with different types of income.</p>

<h2 id="return-type">Return Type</h2>

<p><img src="/images/arvato/Return.png" alt="Return Type Analysis" /></p>

<h4 id="customers-and-non-customers-4">Customers and Non-Customers</h4>

<p>The most frequent type in customers is determined minimal returner, which I think means that these individuals aren’t the shopping type. They only buy what they need when they need it. The second frequent type in incentive receptive normal returner. While in non-customers, we can see that the most frequent type is influencable crazy shopper, and these wouldn’t definetly be interested in mail-order cataloges.</p>

<h4 id="customers-clusters">Customers Clusters</h4>

<p>First off we can see the cluster 0 and 6 are the only populating most of the customers belonging to the determined minimal returner category, and that makes sense since they are older individuals, and we have found that they are consumption minimalists and traditionalists. On the other hand, cluster 4 populates every other category with frequency higher than the determined minmal returner one, with them most frequent being demanding heavy returner.</p>

<h2 id="main-age-within-household">Main Age within Household</h2>

<p><img src="/images/arvato/Main_Age.png" alt="Main Age Analysis" /></p>

<h4 id="customers-and-non-customers-5">Customers and Non-Customers</h4>

<p>We have already investigated the age difference between customers and non-customers, and we can see that the main age within the household is also different between the two groups, where customers households tend be also older in age, while non-customers households tend to be younger.</p>

<h4 id="customer-clusters-5">Customer Clusters</h4>

<p>We can see that cluster 4 is the main cluster populating younger ages in customers clusters, while cluster 0 and 6 have nearly identical distributions representing the elderly segments of the customers.</p>

<h2 id="estimated-net-household-income">Estimated Net Household Income</h2>

<p><img src="/images/arvato/Net_Household.png" alt="Net Household Income Analysis" /></p>

<h4 id="customers-and-non-customers-6">Customers and Non-Customers</h4>

<p>We can see a huge difference between the distribution of customers and non-customers among estimated net household income, where more than 50% of non-customers come from very low income households, and only around 15% of customers do. The most frequent in customers is average income, and the second most is lower income. However, the total percentage of customers whose income is average or above exceeds 50%.</p>

<h4 id="customers-clusters-1">Customers Clusters</h4>

<p>Now we can see a difference between the two older segments, which are cluster 0 and 6. We can see that over 60% of cluster 6 households have either lower or very low income, while more than 70% of cluster 0 has average or higher income. Similarily cluster 4 also has around 70% of it’s households having average or higher income.</p>

<h4 id="does-this-mean-that-cluster-6-is-poorer-than-cluster-0"><strong>Does this mean that cluster 6 is poorer than cluster 0?</strong></h4>

<p>Will that would be the case if this feature indicated the income of the individual, however since this feature indicates the net household income, this doesn’t say anything about the specific individuals in cluster 0 and 6. Since cluster 6 had more tendency to be single, it makes sense that cluster 0 household income would be higher, because if cluster 0 is financially above average, it’s safe to say that probably the rest of their family is the same, and that would make their net income larger than the same individual if he was single, and that’s the situation for cluster 6.</p>

<h2 id="neighborhood-area">Neighborhood Area</h2>

<p><img src="/images/arvato/Neighborhood.png" alt="Neighborhood Analysis" /></p>

<h4 id="customers-and-non-customers-7">Customers and Non-Customers</h4>

<p>The most frequent neighborhood area in both customers and non-customers is average neighborhoods, however the next most frequent for customers is rural neighborhoods, while that of non-customers is poor neighboorhoods. We can also see that the percentage of customers occupying above average neighborhood areas is larger than non-customers’.</p>

<h4 id="customers-clusters-2">Customers Clusters</h4>

<p>We can see that our remark about the household income difference between cluster 0 and 6 has been useful, because cluster 6 to have the highest percentage occupying average and above neighborhood areas, while the most frequent neighborhood area for cluster 0 is rural areas, since they are mostly families. Cluster 4 is extremely similar to cluster 0 in this attribute.</p>

<h2 id="moving-patterns">Moving Patterns</h2>

<p><img src="/images/arvato/Moving_Patterns.png" alt="Moving Patterns Analysis" /></p>

<h4 id="customers-and-non-customers-8">Customers and Non-Customers</h4>

<p>50% of customers are classified as having low or very mobility, while more than 60% of non-customers are the extreme opposite, with classification of either high or very high mobility.</p>

<h4 id="customers-clusters-3">Customers Clusters</h4>

<p>Once again we can see some of the differing factors between cluster 0 and 6, since cluster 6 are mostly single individuals, more than 60% of their moving pattern is high or very high and 25% have middle mobility. On the other hand, since cluster 0 and 4 tend to in families, their mobility is much lower than cluster 6, with almost 75% of cluster 0 having low or very low mobility, and 65% of cluster 4 having the same.</p>

<p>Now that I solidly understood the customer base through the previous analysis, I proceed to the next part.</p>

<h1 id="mailout-campaign-1">Mailout Campaign</h1>

<p>As I previously said, in this part we are supposed to make a supervised learning model based on campaign data provided in <code class="language-plaintext highlighter-rouge">Udacity_MAILOUT_052018_TRAIN.csv</code> which hold demographic data for individuals targeted in marketing campaigns paired with whether they have became customers or not.</p>

<p>Since I already had a cleaning pipeline for the previous datasets, all I needed to do was to check whether this dataset conforms to what we already know in terms of null percentages in features.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Read mailout training data
</span><span class="n">mailout_train</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s">'../../data/Term2/capstone/arvato_data/Udacity_MAILOUT_052018_TRAIN.csv'</span><span class="p">,</span> <span class="n">sep</span><span class="o">=</span><span class="s">';'</span><span class="p">)</span>

<span class="c1"># Replace unknown values with null
</span><span class="n">mailout_train_new</span> <span class="o">=</span> <span class="n">replace_unknown_with_null</span><span class="p">(</span><span class="n">mailout_train</span><span class="p">)</span>

<span class="c1"># Check null percentages in columns and rows
</span><span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">14</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>

<span class="n">plt</span><span class="p">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">title</span><span class="p">(</span><span class="s">"Null Percentages in Columns"</span><span class="p">)</span>
<span class="n">mailout_null_columns</span> <span class="o">=</span> <span class="n">get_null_prop</span><span class="p">(</span><span class="n">mailout_train_new</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">plot</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

<span class="n">plt</span><span class="p">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">title</span><span class="p">(</span><span class="s">"Null Percentages in Rows"</span><span class="p">)</span>
<span class="n">mailout_null_rows</span> <span class="o">=</span> <span class="n">get_null_prop</span><span class="p">(</span><span class="n">mailout_train_new</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">plot</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
</code></pre></div></div>

<p><img src="/images/arvato/mailout_null_before.png" alt="Mailout Null Before" /></p>

<p>The null percentages somehow conform to the what we saw in <code class="language-plaintext highlighter-rouge">AZDIAS</code> and <code class="language-plaintext highlighter-rouge">CUSTOMERS</code> , so I proceeded with cleaning them using the cleaning function, then checking whether they have features that weren’t dropped compared to <code class="language-plaintext highlighter-rouge">clean_azdias</code> .</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Test cleaning the dataset using cleaning function
</span><span class="n">mailout_train_clean</span> <span class="o">=</span> <span class="n">clean_dataset</span><span class="p">(</span><span class="n">mailout_train_new</span><span class="p">)</span>

<span class="k">print</span><span class="p">(</span><span class="s">"Shape before cleaning:"</span><span class="p">,</span> <span class="n">mailout_train</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">"Shape after cleaning:"</span><span class="p">,</span> <span class="n">mailout_train_clean</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>

<span class="c1"># Check if feature set is the same as clean AZDIAS
</span><span class="nb">set</span><span class="p">(</span><span class="n">mailout_train_clean</span><span class="p">.</span><span class="n">columns</span><span class="p">)</span> <span class="o">==</span> <span class="nb">set</span><span class="p">(</span><span class="n">clean_azdias</span><span class="p">.</span><span class="n">columns</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-markdown highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Shape before cleaning: (42962, 367)
Shape after cleaning: (35093, 361)

False
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Check for difference in features in both datasets
</span><span class="k">print</span><span class="p">(</span><span class="s">"Features in clean MAILOUT not in clean AZDIAS:"</span><span class="p">,</span> <span class="nb">set</span><span class="p">(</span><span class="n">mailout_train_clean</span><span class="p">.</span><span class="n">columns</span><span class="p">).</span><span class="n">difference</span><span class="p">(</span><span class="n">clean_azdias</span><span class="p">.</span><span class="n">columns</span><span class="p">))</span>
<span class="k">print</span><span class="p">(</span><span class="s">"Features in clean AZDIAS not in clean MAILOUT:"</span><span class="p">,</span> <span class="nb">set</span><span class="p">(</span><span class="n">clean_azdias</span><span class="p">.</span><span class="n">columns</span><span class="p">).</span><span class="n">difference</span><span class="p">(</span><span class="n">mailout_train_clean</span><span class="p">.</span><span class="n">columns</span><span class="p">))</span>
</code></pre></div></div>

<div class="language-markdown highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Features in clean MAILOUT not in clean AZDIAS: {'AGER_TYP', 'D19_SONSTIGE', 'D19_VOLLSORTIMENT', 'EXTSEL992', 'VHA', 'RESPONSE', 'D19_BUCH_CD', 'D19_SOZIALES'}
Features in clean AZDIAS not in clean MAILOUT: {'KBA13_ANTG4'}
</code></pre></div></div>

<p>So there are some features that we have in <code class="language-plaintext highlighter-rouge">MAILOUT</code> that weren’t dropped, so if we opted to use these there are some points that we need to take care of:</p>

<ol>
  <li>They aren’t included in imputation pipeline</li>
  <li>They aren’t included in scaling pipeline</li>
  <li>They aren’t included in PCA transformer</li>
  <li>They aren’t included in clustering algorithm</li>
</ol>

<p>So they could be concatenated with the results of the original pipeline and cleaned separately if I wanted to use them, or we could just drop them.</p>

<p>Since at that moment, my main goal was to make a baseline to build upon, I didn’t to use the previous customer segmentation pipeline at all, and all I need was clean data to pass to a supervised machine learning algorithm. So I left them as they are.</p>

<p>I added the feature <code class="language-plaintext highlighter-rouge">KBA13_ANTG4</code> that was dropped during cleaning so that we are able to use the customer segmentation pipeline if we needed.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">mailout_train_clean</span> <span class="o">=</span> <span class="n">clean_dataset</span><span class="p">(</span><span class="n">mailout_train_new</span><span class="p">,</span> <span class="n">keep_features</span><span class="o">=</span><span class="p">[</span><span class="s">"KBA13_ANTG4"</span><span class="p">])</span>

</code></pre></div></div>

<p>After that I need to see these features that we didn’t include in the customer segmentation, and I also needed to look at the <code class="language-plaintext highlighter-rouge">RESPONSE</code> target feature to see how it is distributed.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">new_feats</span> <span class="o">=</span> <span class="p">[</span><span class="s">'D19_BUCH_CD'</span><span class="p">,</span> <span class="s">'VHA'</span><span class="p">,</span> <span class="s">'EXTSEL992'</span><span class="p">,</span> <span class="s">'D19_SOZIALES'</span><span class="p">,</span>
             <span class="s">'D19_VOLLSORTIMENT'</span><span class="p">,</span> <span class="s">'RESPONSE'</span><span class="p">,</span> <span class="s">'AGER_TYP'</span><span class="p">,</span> <span class="s">'D19_SONSTIGE'</span><span class="p">]</span>

<span class="n">mailout_train_clean</span><span class="p">[</span><span class="n">new_feats</span><span class="p">].</span><span class="n">hist</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">14</span><span class="p">,</span> <span class="mi">14</span><span class="p">));</span>
</code></pre></div></div>

<p><img src="/images/arvato/mailout_new_feats.png" alt="Mailout New Features" /></p>

<p>All of the features shall be imputed using mean values since all of their missing percentages exceed 10%.</p>

<div class="language-markdown highlighter-rouge"><div class="highlight"><pre class="highlight"><code>D19_BUCH_CD          0.427863
VHA                  0.447326
EXTSEL992            0.247884
D19_SOZIALES         0.282592
D19_VOLLSORTIMENT    0.378623
RESPONSE             0.000000
AGER_TYP             0.305047
D19_SONSTIGE         0.228963
dtype: float64
</code></pre></div></div>

<p>However, I have noticed that <code class="language-plaintext highlighter-rouge">RESPONSE</code> is extremely imbalanced, which means that results of the regular baseline will probably be really bad, and we’ll have to use certain techniques to handle that situation.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Impute clean AZDIAS features using old imputer
</span><span class="n">mailout_train_clean</span><span class="p">.</span><span class="n">loc</span><span class="p">[:,</span> <span class="n">clean_azdias</span><span class="p">.</span><span class="n">columns</span><span class="p">]</span> <span class="o">=</span> <span class="n">imputer</span><span class="p">.</span><span class="n">transform</span><span class="p">(</span><span class="n">mailout_train_clean</span><span class="p">[</span><span class="n">clean_azdias</span><span class="p">.</span><span class="n">columns</span><span class="p">])</span>

<span class="c1"># Ad-hoc impute EXTSEL992 
</span><span class="n">mailout_train_clean</span><span class="p">[</span><span class="s">"EXTSEL992"</span><span class="p">].</span><span class="n">fillna</span><span class="p">(</span><span class="n">value</span><span class="o">=</span><span class="n">mailout_train_clean</span><span class="p">[</span><span class="s">"EXTSEL992"</span><span class="p">].</span><span class="n">mean</span><span class="p">(),</span> <span class="n">inplace</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

<span class="c1"># Make new imputer for remaining features
</span><span class="n">mailout_imputer</span> <span class="o">=</span> <span class="n">SimpleImputer</span><span class="p">(</span><span class="n">strategy</span><span class="o">=</span><span class="s">"most_frequent"</span><span class="p">)</span>

<span class="c1"># Fit imputer and impute the remaining columns in place
</span><span class="n">mailout_train_clean</span><span class="p">.</span><span class="n">loc</span><span class="p">[:,</span> <span class="p">:]</span> <span class="o">=</span> <span class="n">mailout_imputer</span><span class="p">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">mailout_train_clean</span><span class="p">)</span>

<span class="c1"># Split training data into X and y
</span><span class="n">X</span> <span class="o">=</span> <span class="n">mailout_train_clean</span><span class="p">.</span><span class="n">loc</span><span class="p">[:,</span> <span class="n">mailout_train_clean</span><span class="p">.</span><span class="n">columns</span> <span class="o">!=</span> <span class="s">"RESPONSE"</span><span class="p">]</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">mailout_train_clean</span><span class="p">.</span><span class="n">loc</span><span class="p">[:,</span> <span class="s">"RESPONSE"</span><span class="p">]</span>
</code></pre></div></div>

<h3 id="now-that-we-have-a-clean-dataset-its-time-to-train-a-model">Now that we have a clean dataset it’s time to train a model</h3>

<p>The model that I have in mind is RandomForestClassifier.</p>

<p>In order to evaluate this baseline, we need to have some sort of validation set in order to score our results.</p>

<p>For validation I’ll use Stratified KFold cross validation (in order to account for RESPONSE imbalance), and I’ll use sklearn’s classfication report which shows recall, precision and f1-score for each label.</p>

<p>I’ll also use ROC AUC score since it’s the score of the final competition.</p>

<h3 id="the-advantage-of-classification-report-is-that-it-shows-us-the-whole-picutre-so-we-dont-get-decieved-if-the-model-is-performing-poorly-on-the-under-represented-class-and-performing-better-on-the-over-represented-one">The advantage of classification report is that it shows us the whole picutre, so we don’t get decieved if the model is performing poorly on the under-represented class, and performing better on the over-represented one.</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">model_validation</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">fold_results</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">final_results</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">metric</span><span class="o">=</span><span class="bp">False</span><span class="p">):</span>
    <span class="s">"""Evaluate model using sklearn's classification report."""</span>
    <span class="c1"># Instantiate StratifiedKFold 
</span>    <span class="n">skf</span> <span class="o">=</span> <span class="n">StratifiedKFold</span><span class="p">(</span><span class="n">n_splits</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="n">seed</span><span class="p">)</span>

    <span class="c1"># Make empty y_pred to fill predictions of each fold
</span>    <span class="n">y_pred</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">y</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>
    
    <span class="c1"># Initialize empty list for scores
</span>    <span class="n">scores</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="p">(</span><span class="n">train_idx</span><span class="p">,</span> <span class="n">test_idx</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">skf</span><span class="p">.</span><span class="n">split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)):</span>
        <span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span> <span class="o">=</span> <span class="n">X</span><span class="p">.</span><span class="n">iloc</span><span class="p">[</span><span class="n">train_idx</span><span class="p">],</span> <span class="n">X</span><span class="p">.</span><span class="n">iloc</span><span class="p">[</span><span class="n">test_idx</span><span class="p">]</span>
        <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">y</span><span class="p">.</span><span class="n">iloc</span><span class="p">[</span><span class="n">train_idx</span><span class="p">],</span> <span class="n">y</span><span class="p">.</span><span class="n">iloc</span><span class="p">[</span><span class="n">test_idx</span><span class="p">]</span>

        <span class="c1"># Fit model
</span>        <span class="n">model</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

        <span class="c1"># Predict fold y_test and add in y_pred
</span>        <span class="n">fold_pred</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
        <span class="n">y_pred</span><span class="p">[</span><span class="n">test_idx</span><span class="p">]</span> <span class="o">+=</span> <span class="n">fold_pred</span>

        <span class="c1"># Print classification report
</span>        <span class="k">if</span> <span class="n">fold_results</span><span class="p">:</span>
            <span class="k">print</span><span class="p">(</span><span class="s">"Fold:"</span><span class="p">,</span> <span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span>
            <span class="k">print</span><span class="p">(</span><span class="n">classification_report</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">fold_pred</span><span class="p">))</span>
        
        <span class="c1"># Calculate fold macro f1 scores
</span>        <span class="n">scores</span> <span class="o">=</span> <span class="n">metric</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">fold_pred</span><span class="p">)</span>

    <span class="c1"># Print final classification report
</span>    <span class="k">if</span> <span class="n">final_results</span><span class="p">:</span>
        <span class="k">print</span><span class="p">(</span><span class="s">"Final Report:"</span><span class="p">)</span>
        <span class="k">print</span><span class="p">(</span><span class="n">classification_report</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">))</span>
    
    <span class="c1"># Return metric scores if passed to the function
</span>    <span class="k">if</span> <span class="n">metric</span> <span class="o">!=</span> <span class="bp">None</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">scores</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">imblearn.ensemble</span> <span class="kn">import</span> <span class="n">BalancedRandomForestClassifier</span>

<span class="n">model_validation</span><span class="p">(</span><span class="n">BalancedRandomForestClassifier</span><span class="p">(</span><span class="n">n_estimators</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="n">seed</span><span class="p">),</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-markdown highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Final Report:
              precision    recall  f1-score   support<span class="sb">

         0.0       1.00      0.69      0.81     34658
         1.0       0.03      0.73      0.06       436

    accuracy                           0.69     35094
</span>   macro avg       0.51      0.71      0.44     35094
weighted avg       0.98      0.69      0.81     35094

Metric Score: 0.7085381814866175
</code></pre></div></div>

<p>We can see that this model still doesn’t have amazing performance, but it’s significantly better than the regular RandomForestClassifier.</p>

<div class="language-markdown highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
Final Report:
              precision    recall  f1-score   support<span class="sb">

         0.0       0.99      1.00      0.99     34658
         1.0       0.00      0.00      0.00       436

    accuracy                           0.99     35094
</span>   macro avg       0.49      0.50      0.50     35094
weighted avg       0.98      0.99      0.98     35094

Metric Score: 0.5
</code></pre></div></div>

<p>We can see that if we use the weighted average of any of the 3 metric used (precision, recall or f1-score) that the RandomForestClassifier wins, however, if we look into the metrics for the responsive class, we can see that the classifier completely missed them, and due to their lower percentage the classifier decided that the best strategy is to predict that all of the data points were non-responsive.</p>

<p>So as we can see, the best thing about classification_report is that it gives us a variety of metrics that we can use to judge our model.</p>

<h2 id="baseline-results">Baseline Results</h2>

<p>We can see that this model still has bad performance, but it’s siginifcantly better than the regular RandomForestClassifier.</p>

<p>The best thing about classification_report is that it gives us a variety of metrics that we can use to judge our model.</p>

<h3 id="therefore-we-can-see-the-results-are">Therefore, we can see the results are:</h3>

<ol>
  <li><em>Recall</em> (Macro): 0.71</li>
  <li><em>Precision</em> (Macro): 0.51</li>
  <li><em>F1-Score</em> (Macro): 0.44</li>
</ol>

<h3 id="the-first-imporvement-that-we-can-make-is-to-utilize-the-dimensionality-reduction-that-we-have-made-earlier">The first imporvement that we can make is to utilize the dimensionality reduction that we have made earlier.</h3>

<ol>
  <li>Select only features in clean_azdias</li>
  <li>Scale the features and reduce dimensions using PCA</li>
  <li>Predict using PCA</li>
</ol>

<h2 id="predicting-using-pca-reduced-dataset">Predicting using PCA reduced dataset</h2>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Filter for features used in clean_azdias
</span><span class="n">mailout_train_pre_pca</span> <span class="o">=</span> <span class="n">mailout_train_clean</span><span class="p">[</span><span class="n">clean_azdias</span><span class="p">.</span><span class="n">columns</span><span class="p">]</span>

<span class="c1"># Scale the features
</span><span class="n">mailout_train_pre_pca</span> <span class="o">=</span> <span class="n">scaler</span><span class="p">.</span><span class="n">transform</span><span class="p">(</span><span class="n">mailout_train_pre_pca</span><span class="p">)</span>

<span class="c1"># Reduce dimensions using pca to d dimensions
</span><span class="n">X</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">pca</span><span class="p">.</span><span class="n">transform</span><span class="p">(</span><span class="n">mailout_train_pre_pca</span><span class="p">)[:,</span> <span class="p">:</span><span class="n">d</span><span class="p">])</span>

<span class="n">model_validation</span><span class="p">(</span><span class="n">BalancedRandomForestClassifier</span><span class="p">(</span><span class="n">random_state</span><span class="o">=</span><span class="n">seed</span><span class="p">))</span>
</code></pre></div></div>

<div class="language-markdown highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Final Report:
              precision    recall  f1-score   support<span class="sb">

         0.0       0.99      0.59      0.74     34658
         1.0       0.02      0.58      0.03       436

    accuracy                           0.59     35094
</span>   macro avg       0.50      0.58      0.39     35094
weighted avg       0.98      0.59      0.73     35094

Metric Score: 0.5825897065477098
</code></pre></div></div>

<p>The results of PCA transformed training set with BalancedRandomForestClassifier is worse than using the vanilla dataset.</p>

<h3 id="could-these-results-be-improved-with-other-algorithms">Could these results be improved with other algorithms?</h3>

<p>We could spot-check multiple algorithms to prove if this is the case.</p>

<p>Instead of implementing under-sampling or over-sampling in 
validation, we can use BalancedBaggingClassifier to wrap the 
classfication algortithms that we are interested in using. But we’ll 
need to scale the data because algorithms like Logistic Regression, KNN 
and SVM can perform better after scaling the data.</p>

<h3 id="note-that-when-comparing-different-models-result-ill-not-increase">Note that when comparing different models result, I’ll not increase</h3>
<p>n_estimators to save time, as the results are just for comparison.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">evaluate_models</span><span class="p">(</span><span class="n">models_dict</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">scoring</span><span class="o">=</span><span class="s">"recallWe can see that these features have definetly improved upon our PCA results, and even made it better than the original dataset score.
Right now what if we add the PCA features to all features?_macro"</span><span class="p">):</span>
    <span class="s">"""Evaluate several models using sklearn's cross_val_score."""</span>
    <span class="n">model_scores</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">model</span> <span class="ow">in</span> <span class="n">models_dict</span><span class="p">.</span><span class="n">items</span><span class="p">():</span>
        <span class="n">skf</span> <span class="o">=</span> <span class="n">StratifiedKFold</span><span class="p">(</span><span class="n">n_splits</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="n">seed</span><span class="p">)</span>
        <span class="n">scores</span> <span class="o">=</span> <span class="n">cross_val_score</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="n">skf</span><span class="p">,</span> <span class="n">scoring</span><span class="o">=</span><span class="n">scoring</span><span class="p">)</span>
        <span class="n">model_scores</span><span class="p">[</span><span class="n">name</span><span class="p">]</span> <span class="o">=</span> <span class="n">scores</span>
        <span class="k">print</span><span class="p">(</span><span class="s">"Model:%s, Score:%.3f (+/- %.3f)"</span> <span class="o">%</span> <span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="n">np</span><span class="p">.</span><span class="n">mean</span><span class="p">(</span><span class="n">scores</span><span class="p">),</span> <span class="n">np</span><span class="p">.</span><span class="n">std</span><span class="p">(</span><span class="n">scores</span><span class="p">)))</span>
    <span class="k">return</span> <span class="n">model_scores</span>
</code></pre></div></div>

<p>Then I selected the models we were going to use and passed them into the function to see how they fare against RandomForests.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">cross_val_score</span>
<span class="kn">from</span> <span class="nn">imblearn.ensemble</span> <span class="kn">import</span> <span class="n">BalancedBaggingClassifier</span>
<span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LogisticRegression</span>
<span class="kn">from</span> <span class="nn">sklearn.neighbors</span> <span class="kn">import</span> <span class="n">KNeighborsClassifier</span>
<span class="kn">from</span> <span class="nn">sklearn.svm</span> <span class="kn">import</span> <span class="n">SVC</span>
<span class="kn">from</span> <span class="nn">sklearn.pipeline</span> <span class="kn">import</span> <span class="n">Pipeline</span>
<span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">StandardScaler</span>

<span class="n">bagging_model</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">model</span><span class="p">:</span> <span class="n">BalancedBaggingClassifier</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>

<span class="n">models</span> <span class="o">=</span> <span class="p">{</span><span class="s">"RF"</span><span class="p">:</span> <span class="n">BalancedRandomForestClassifier</span><span class="p">(</span><span class="n">random_state</span><span class="o">=</span><span class="n">seed</span><span class="p">),</span>
          <span class="s">"LR"</span><span class="p">:</span> <span class="n">bagging_model</span><span class="p">(</span><span class="n">LogisticRegression</span><span class="p">(</span><span class="n">max_iter</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="n">seed</span><span class="p">)),</span>
          <span class="s">"KNN"</span><span class="p">:</span> <span class="n">bagging_model</span><span class="p">(</span><span class="n">KNeighborsClassifier</span><span class="p">()),</span>
          <span class="s">"Linear SVM"</span><span class="p">:</span> <span class="n">bagging_model</span><span class="p">(</span><span class="n">SVC</span><span class="p">(</span><span class="n">kernel</span><span class="o">=</span><span class="s">"linear"</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="n">seed</span><span class="p">)),</span>
          <span class="s">"RBF SVM"</span><span class="p">:</span> <span class="n">bagging_model</span><span class="p">(</span><span class="n">SVC</span><span class="p">(</span><span class="n">kernel</span><span class="o">=</span><span class="s">"rbf"</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="n">seed</span><span class="p">))}</span>

<span class="n">pca_bagging_scores</span> <span class="o">=</span> <span class="n">evaluate_models</span><span class="p">(</span><span class="n">models</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="s">"recall_macro"</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-markdown highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Model:RF, Score:0.549 (+/- 0.026)
Model:LR, Score:0.558 (+/- 0.006)
Model:KNN, Score:0.539 (+/- 0.023)
Model:Linear SVM, Score:0.536 (+/- 0.009)
Model:RBF SVM, Score:0.571 (+/- 0.016)
</code></pre></div></div>

<p>We can see  that Bagging using LogisitcRegression and RBF SVM exceeded the recall of BalancedRandomForestClassifier. However we have to note that we didn’t  increase the number of estimators, and we still don’t know the exact recall of the responsive class.</p>

<h3 id="what-if-we-trained-them-on-the-original-data">What if we trained them on the original data?</h3>

<p>But we’ll need to scale the data because algorithms like Logistic 
Regression, KNN and SVM can perform better after scaling the data.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Split training data into X and y
</span><span class="n">X</span> <span class="o">=</span> <span class="n">mailout_train_clean</span><span class="p">.</span><span class="n">loc</span><span class="p">[:,</span> <span class="n">mailout_train_clean</span><span class="p">.</span><span class="n">columns</span> <span class="o">!=</span> <span class="s">"RESPONSE"</span><span class="p">]</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">mailout_train_clean</span><span class="p">.</span><span class="n">loc</span><span class="p">[:,</span> <span class="s">"RESPONSE"</span><span class="p">]</span>

<span class="n">scaled_pipeline</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">model</span><span class="p">:</span> <span class="n">make_pipeline</span><span class="p">(</span><span class="n">StandardScaler</span><span class="p">(),</span> <span class="n">model</span><span class="p">)</span>

<span class="n">models</span> <span class="o">=</span> <span class="p">{</span><span class="s">"RF"</span><span class="p">:</span> <span class="n">BalancedRandomForestClassifier</span><span class="p">(</span><span class="n">random_state</span><span class="o">=</span><span class="n">seed</span><span class="p">),</span>
          <span class="s">"LR"</span><span class="p">:</span> <span class="n">scaled_pipeline</span><span class="p">(</span><span class="n">bagging_model</span><span class="p">(</span><span class="n">LogisticRegression</span><span class="p">(</span><span class="n">max_iter</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="n">seed</span><span class="p">))),</span>
          <span class="s">"KNN"</span><span class="p">:</span> <span class="n">scaled_pipeline</span><span class="p">(</span><span class="n">bagging_model</span><span class="p">(</span><span class="n">KNeighborsClassifier</span><span class="p">())),</span>
          <span class="s">"Linear SVM"</span><span class="p">:</span> <span class="n">scaled_pipeline</span><span class="p">(</span><span class="n">bagging_model</span><span class="p">(</span><span class="n">SVC</span><span class="p">(</span><span class="n">kernel</span><span class="o">=</span><span class="s">"linear"</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="n">seed</span><span class="p">))),</span>
          <span class="s">"RBF SVM"</span><span class="p">:</span> <span class="n">scaled_pipeline</span><span class="p">(</span><span class="n">bagging_model</span><span class="p">(</span><span class="n">SVC</span><span class="p">(</span><span class="n">kernel</span><span class="o">=</span><span class="s">"rbf"</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="n">seed</span><span class="p">)))}</span>

<span class="n">original_bagging_scores</span> <span class="o">=</span> <span class="n">evaluate_models</span><span class="p">(</span><span class="n">models</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="s">"recall_macro"</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-markdown highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Model:RF, Score:0.673 (+/- 0.013)
Model:LR, Score:0.585 (+/- 0.016)
Model:KNN, Score:0.533 (+/- 0.028)
Model:Linear SVM, Score:0.576 (+/- 0.013)
Model:RBF SVM, Score:0.573 (+/- 0.006)
</code></pre></div></div>

<p>By comparing the results, we can see that all of the algorithms (except Bagged KNN) perform better using the original data. However the improvement in all of them compared to BalancedRandomForestClassifier is tiny, so we need not continue in exploring them.</p>

<p>In terms of the significant improvement in BalancedRandomForestClassifier, this could be for two reasons:</p>

<ol>
  <li>The features that aren’t included in the PCA could add important information to the model</li>
  <li>The PCA transformed data isn’t particularly good for the task that we need</li>
</ol>

<h3 id="we-can-test-this-by-adding-all-of-the-features-that-were-left-our-from-pca-transformation-to-the-transformed-data-to-test-the-performance-of-the">We can test this by adding all of the features that were left our from PCA transformation to the transformed data to test the performance of the</h3>
<p>algorithm again.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Make new dataframe with only new feats
</span><span class="n">mailout_train_new_feats</span> <span class="o">=</span> <span class="n">mailout_train_clean</span><span class="p">.</span><span class="n">loc</span><span class="p">[:,</span> <span class="n">new_feats</span><span class="p">]</span>

<span class="c1"># Concatenate PCA transformed dataframe with new feats dataframe
</span><span class="n">X</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">concat</span><span class="p">([</span><span class="n">pd</span><span class="p">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">pca</span><span class="p">.</span><span class="n">transform</span><span class="p">(</span><span class="n">mailout_train_pre_pca</span><span class="p">)[:,</span> <span class="p">:</span><span class="n">d</span><span class="p">]),</span> <span class="n">mailout_train_new_feats</span><span class="p">.</span><span class="n">reset_index</span><span class="p">(</span><span class="n">drop</span><span class="o">=</span><span class="bp">True</span><span class="p">)],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="c1"># Drop RESPONSE
</span><span class="n">X</span> <span class="o">=</span> <span class="n">X</span><span class="p">.</span><span class="n">drop</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s">"RESPONSE"</span><span class="p">])</span>

<span class="n">model_validation</span><span class="p">(</span><span class="n">BalancedRandomForestClassifier</span><span class="p">(</span><span class="n">n_estimators</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="n">seed</span><span class="p">),</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-markdown highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Final Report:
              precision    recall  f1-score   support<span class="sb">

         0.0       1.00      0.70      0.82     34658
         1.0       0.03      0.74      0.06       436

    accuracy                           0.70     35094
</span>   macro avg       0.51      0.72      0.44     35094
weighted avg       0.98      0.70      0.81     35094

Metric Score: 0.7209044081958259
</code></pre></div></div>

<p>We can see that these features have definetly improved upon our PCA 
results, and even made it better than the original dataset score.</p>

<h3 id="right-now-what-if-we-add-the-pca-features-to-all-features">Right now what if we add the PCA features to all features?</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Concatenate PCA transformed dataframe with new feats dataframe
</span><span class="n">X</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">concat</span><span class="p">([</span><span class="n">pd</span><span class="p">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">pca</span><span class="p">.</span><span class="n">transform</span><span class="p">(</span><span class="n">mailout_train_pre_pca</span><span class="p">)[:,</span> <span class="p">:</span><span class="n">d</span><span class="p">]),</span> <span class="n">mailout_train_clean</span><span class="p">.</span><span class="n">reset_index</span><span class="p">(</span><span class="n">drop</span><span class="o">=</span><span class="bp">True</span><span class="p">)],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="c1"># Drop RESPONSE
</span><span class="n">X</span> <span class="o">=</span> <span class="n">X</span><span class="p">.</span><span class="n">drop</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s">"RESPONSE"</span><span class="p">])</span>

<span class="n">model_validation</span><span class="p">(</span><span class="n">BalancedRandomForestClassifier</span><span class="p">(</span><span class="n">n_estimators</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="n">seed</span><span class="p">),</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-markdown highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Final Report:
              precision    recall  f1-score   support<span class="sb">

         0.0       0.99      0.67      0.80     34658
         1.0       0.03      0.72      0.05       436

    accuracy                           0.67     35094
</span>   macro avg       0.51      0.70      0.43     35094
weighted avg       0.98      0.67      0.79     35094

Metric Score: 0.6977696440858011
</code></pre></div></div>

<p>We can see that this has worsened the results. It seems that the PCA transformer 
features are able to provide info that is better than the original 
features before transformation, and the presence of both at the same 
time doesn’t improve the model performance at all.</p>

<h3 id="what-if-we-add-the-clusters-distances-as-features-to-pca-features--new-features-combo">What if we add the clusters distances as features to PCA features + new features combo?</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># transform mailout train data using PCA
</span><span class="n">mailout_train_pca</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">pca</span><span class="p">.</span><span class="n">transform</span><span class="p">(</span><span class="n">mailout_train_pre_pca</span><span class="p">)[:,</span> <span class="p">:</span><span class="n">d</span><span class="p">],</span> 
                                 <span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="sa">f</span><span class="s">"pca_</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s">"</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">d</span><span class="p">)])</span>

<span class="c1"># predict mailout cluster distances
</span><span class="n">mailout_distances</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">kmeans</span><span class="p">.</span><span class="n">transform</span><span class="p">(</span><span class="n">mailout_train_pca</span><span class="p">),</span>
                                 <span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="sa">f</span><span class="s">"cluster_</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s">"</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">kmeans</span><span class="p">.</span><span class="n">cluster_centers_</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])])</span>

<span class="c1"># predict mailout cluster
</span><span class="n">mailout_clusters</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">Series</span><span class="p">(</span><span class="n">kmeans</span><span class="p">.</span><span class="n">predict</span><span class="p">(</span><span class="n">mailout_train_pca</span><span class="p">),</span> <span class="n">name</span><span class="o">=</span><span class="s">'label'</span><span class="p">)</span>

<span class="c1"># visualize mailout clusters
</span><span class="n">mailout_clusters</span><span class="p">.</span><span class="n">value_counts</span><span class="p">().</span><span class="n">plot</span><span class="p">(</span><span class="n">kind</span><span class="o">=</span><span class="s">"bar"</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="s">"How Are Clusters Distributed in MAILOUT Data?"</span><span class="p">);</span>
</code></pre></div></div>

<p><img src="/images/arvato/mailout_clusters.png" alt="Mailout Clusters" /></p>

<p>If we were to use these clusters directly, we would predict that the majority of the individuals in the <code class="language-plaintext highlighter-rouge">MAILOUT</code> data would respond, but we know from the labels than only a very small portion of them actually responded.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># concatenate cluster labels to the clean dataset
</span><span class="n">X</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">concat</span><span class="p">([</span><span class="n">mailout_train_pca</span><span class="p">,</span> <span class="n">mailout_train_new_feats</span><span class="p">.</span><span class="n">reset_index</span><span class="p">(</span><span class="n">drop</span><span class="o">=</span><span class="bp">True</span><span class="p">),</span> <span class="n">mailout_distances</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="c1"># drop RESPONSE
</span><span class="n">X</span> <span class="o">=</span> <span class="n">X</span><span class="p">.</span><span class="n">drop</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s">"RESPONSE"</span><span class="p">])</span>

<span class="n">model_validation</span><span class="p">(</span><span class="n">BalancedRandomForestClassifier</span><span class="p">(</span><span class="n">n_estimators</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="n">seed</span><span class="p">),</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-markdown highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Final Report:
              precision    recall  f1-score   support<span class="sb">

         0.0       0.99      0.69      0.81     34658
         1.0       0.03      0.72      0.05       436

    accuracy                           0.69     35094
</span>   macro avg       0.51      0.70      0.43     35094
weighted avg       0.98      0.69      0.80     35094

Metric Score: 0.7035868915429383
</code></pre></div></div>

<p>The results are worse than just using PCA features and new features.</p>

<h3 id="what-if-we-just-use-cluster-distances-and-new-features">What if we just use cluster distances and new features?</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># concatenate cluster labels to the clean dataset
</span><span class="n">X</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">concat</span><span class="p">([</span><span class="n">mailout_train_new_feats</span><span class="p">.</span><span class="n">reset_index</span><span class="p">(</span><span class="n">drop</span><span class="o">=</span><span class="bp">True</span><span class="p">),</span> <span class="n">mailout_distances</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="c1"># drop RESPONSE
</span><span class="n">X</span> <span class="o">=</span> <span class="n">X</span><span class="p">.</span><span class="n">drop</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s">"RESPONSE"</span><span class="p">])</span>

<span class="n">model_validation</span><span class="p">(</span><span class="n">BalancedRandomForestClassifier</span><span class="p">(</span><span class="n">n_estimators</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="n">seed</span><span class="p">),</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-markdown highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Final Report:
              precision    recall  f1-score   support<span class="sb">

         0.0       1.00      0.68      0.81     34658
         1.0       0.03      0.82      0.06       436

    accuracy                           0.68     35094
</span>   macro avg       0.51      0.75      0.43     35094
weighted avg       0.98      0.68      0.80     35094

Metric Score: 0.7474211491200037
</code></pre></div></div>

<p>I want to go more in depth with SupportVectorMachines, and look into their classification report with the original data.</p>

<p>We
 can see that the results have significantly improved, which indicates 
that the presence of all original features isn’t useful for the model.</p>

<h3 id="so-we-might-want-to-test-automatic-feature-selection-to-see-if-we-can">So we might want to test automatic feature selection to see if we can</h3>
<p>still use some of them and the PCA features to imporve the final
results.</p>

<h3 id="automatic-feature-selection">Automatic Feature Selection</h3>

<p>There are several methods present in scikit-learn for feature selection, but I’ll use <strong>SelectKBest</strong> which removes all but the highest scoring K features.</p>

<h3 id="lets-just-test-selectkbest-on-all-of-the-features-we-had-so-far-to-see-if-it-can-get-better-results-that-our-best-trial-so-far-recall-074">Let’s just test SelectKBest on all of the features we had so far to see if it can get better results that our best trial so far (Recall 0.74)</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Concatenate all features
</span><span class="n">X</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">concat</span><span class="p">([</span><span class="n">mailout_train_pca</span><span class="p">,</span> <span class="n">mailout_train_clean</span><span class="p">.</span><span class="n">reset_index</span><span class="p">(</span><span class="n">drop</span><span class="o">=</span><span class="bp">True</span><span class="p">),</span> <span class="n">mailout_distances</span><span class="p">,</span> <span class="n">mailout_clusters</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="c1"># Drop RESPONSE
</span><span class="n">X</span> <span class="o">=</span> <span class="n">X</span><span class="p">.</span><span class="n">drop</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s">"RESPONSE"</span><span class="p">])</span>

<span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="p">[</span><span class="mi">10</span><span class="p">,</span> <span class="mi">30</span><span class="p">,</span> <span class="mi">50</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">300</span><span class="p">,</span> <span class="mi">500</span><span class="p">]:</span>
    <span class="k">print</span><span class="p">(</span><span class="s">"K:"</span><span class="p">,</span> <span class="n">k</span><span class="p">)</span>
    
    <span class="c1"># make pipeline
</span>    <span class="n">pipeline</span> <span class="o">=</span> <span class="n">make_pipeline</span><span class="p">(</span><span class="n">SelectKBest</span><span class="p">(</span><span class="n">k</span><span class="o">=</span><span class="n">k</span><span class="p">),</span>
                             <span class="n">BalancedRandomForestClassifier</span><span class="p">(</span><span class="n">n_estimators</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="n">seed</span><span class="p">))</span>
    
    <span class="n">model_validation</span><span class="p">(</span><span class="n">pipeline</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
    <span class="k">print</span><span class="p">()</span>
</code></pre></div></div>

<div class="language-markdown highlighter-rouge"><div class="highlight"><pre class="highlight"><code>K: 10
Final Report:
              precision    recall  f1-score   support<span class="sb">

         0.0       1.00      0.70      0.83     34657
         1.0       0.03      0.83      0.07       436

    accuracy                           0.71     35093
</span>   macro avg       0.52      0.77      0.45     35093
weighted avg       0.99      0.71      0.82     35093

Metric Score: 0.7671317445321728

K: 30
Final Report:
              precision    recall  f1-score   support<span class="sb">

         0.0       1.00      0.71      0.83     34657
         1.0       0.03      0.82      0.07       436

    accuracy                           0.71     35093
</span>   macro avg       0.52      0.76      0.45     35093
weighted avg       0.98      0.71      0.82     35093

Metric Score: 0.7620556696904557

K: 50
Final Report:
              precision    recall  f1-score   support<span class="sb">

         0.0       1.00      0.71      0.83     34657
         1.0       0.03      0.80      0.06       436

    accuracy                           0.71     35093
</span>   macro avg       0.51      0.75      0.45     35093
weighted avg       0.98      0.71      0.82     35093

Metric Score: 0.7526066873716092

K: 100
Final Report:
              precision    recall  f1-score   support<span class="sb">

         0.0       1.00      0.71      0.83     34657
         1.0       0.03      0.79      0.06       436

    accuracy                           0.71     35093
</span>   macro avg       0.51      0.75      0.45     35093
weighted avg       0.98      0.71      0.82     35093

Metric Score: 0.7497644021549252

K: 300
Final Report:
              precision    recall  f1-score   support<span class="sb">

         0.0       1.00      0.70      0.83     34657
         1.0       0.03      0.77      0.06       436

    accuracy                           0.71     35093
</span>   macro avg       0.51      0.74      0.44     35093
weighted avg       0.98      0.71      0.82     35093

Metric Score: 0.7353716289811935

K: 500
Final Report:
              precision    recall  f1-score   support<span class="sb">

         0.0       1.00      0.70      0.82     34657
         1.0       0.03      0.79      0.06       436

    accuracy                           0.70     35093
</span>   macro avg       0.51      0.74      0.44     35093
weighted avg       0.98      0.70      0.81     35093

Metric Score: 0.744009369775735
</code></pre></div></div>

<p><strong>We can see that selecting that using the top 10 features is enough to improve the Macro Recall to 0.77. I’m still interested to see if we only use the feature selection on the PCA and old features only, while keeping the new feature and distances untouched.</strong></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">pca_feats</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">mailout_train_pca</span><span class="p">.</span><span class="n">columns</span><span class="p">)</span>
<span class="n">azdias_feats</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">clean_azdias</span><span class="p">.</span><span class="n">columns</span><span class="p">)</span>

<span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="p">[</span><span class="mi">10</span><span class="p">,</span> <span class="mi">30</span><span class="p">,</span> <span class="mi">50</span><span class="p">]:</span>
    <span class="k">print</span><span class="p">(</span><span class="s">"K:"</span><span class="p">,</span> <span class="n">k</span><span class="p">)</span>
   
    <span class="n">pipeline</span> <span class="o">=</span> <span class="n">Pipeline</span><span class="p">([</span>
        <span class="p">(</span><span class="s">'feature_selection'</span><span class="p">,</span> <span class="n">ColumnTransformer</span><span class="p">([</span>
            <span class="p">(</span><span class="s">'kbest'</span><span class="p">,</span> <span class="n">SelectKBest</span><span class="p">(</span><span class="n">k</span><span class="o">=</span><span class="n">k</span><span class="p">),</span> <span class="n">pca_feats</span><span class="o">+</span><span class="n">azdias_feats</span><span class="p">),</span>
        <span class="p">],</span> <span class="n">remainder</span><span class="o">=</span><span class="s">'passthrough'</span><span class="p">)),</span>
        <span class="p">(</span><span class="s">'clf'</span><span class="p">,</span> <span class="n">BalancedRandomForestClassifier</span><span class="p">(</span><span class="n">n_estimators</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="n">seed</span><span class="p">))</span>
    <span class="p">])</span>
    
    <span class="n">model_validation</span><span class="p">(</span><span class="n">pipeline</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
    <span class="k">print</span><span class="p">()</span>
</code></pre></div></div>

<div class="language-markdown highlighter-rouge"><div class="highlight"><pre class="highlight"><code>K: 10
Final Report:
              precision    recall  f1-score   support<span class="sb">

         0.0       1.00      0.70      0.82     34657
         1.0       0.03      0.83      0.06       436

    accuracy                           0.70     35093
</span>   macro avg       0.52      0.77      0.44     35093
weighted avg       0.99      0.70      0.81     35093

Metric Score: 0.7663125665425543

K: 30
Final Report:
              precision    recall  f1-score   support<span class="sb">

         0.0       1.00      0.70      0.83     34657
         1.0       0.03      0.82      0.06       436

    accuracy                           0.71     35093
</span>   macro avg       0.52      0.76      0.45     35093
weighted avg       0.98      0.71      0.82     35093

Metric Score: 0.7617978118285937

K: 50
Final Report:
              precision    recall  f1-score   support<span class="sb">

         0.0       1.00      0.71      0.83     34657
         1.0       0.03      0.81      0.06       436

    accuracy                           0.71     35093
</span>   macro avg       0.51      0.76      0.45     35093
weighted avg       0.98      0.71      0.82     35093

Metric Score: 0.7555872211446162
</code></pre></div></div>

<h3 id="now-we-know-its-better-to-just-pass-all-features-for-automatic-feature">Now we know it’s better to just pass all features for automatic feature</h3>
<p>selection, as the performance didn’t improve over the best score.<a href="https://viewqk2nn0jlef.udacity-student-workspaces.com/notebooks/Arvato%20Project%20Workbook.ipynb#Now-we-know-it's-better-to-just-pass-all-features-for-automatic-feature-selection,-as-the-performance-didn't-improve-over-the-best-score.">¶</a></p>

<p>Now I’ll make a function to document all of the steps we did for 
preparing the dataset, so we’ll be able to do the same in testing.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">prepare_mailout</span><span class="p">(</span><span class="n">df</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">test</span><span class="o">=</span><span class="bp">False</span><span class="p">):</span>
    <span class="s">"""Prepare MAILOUT training and testing dataset for ML Pipeline."""</span>
    <span class="c1"># Set dropping threshold to 1.0 for test set
</span>    <span class="k">if</span> <span class="n">test</span><span class="p">:</span>
        <span class="n">p</span> <span class="o">=</span> <span class="mf">1.0</span>
        
    <span class="c1"># Clean the dataset
</span>    <span class="n">df_clean</span> <span class="o">=</span> <span class="n">clean_dataset</span><span class="p">(</span><span class="n">df</span><span class="p">,</span> <span class="n">p_row</span><span class="o">=</span><span class="n">p</span><span class="p">,</span> <span class="n">p_col</span><span class="o">=</span><span class="n">p</span><span class="p">,</span> <span class="n">keep_features</span><span class="o">=</span><span class="p">[</span><span class="s">"KBA13_ANTG4"</span><span class="p">])</span>
    
    <span class="c1"># Drop RESPONSE if train set
</span>    <span class="k">if</span> <span class="n">test</span><span class="p">:</span>
        <span class="n">y</span> <span class="o">=</span> <span class="bp">None</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">y</span> <span class="o">=</span> <span class="n">df_clean</span><span class="p">[</span><span class="s">"RESPONSE"</span><span class="p">]</span>
        <span class="n">df_clean</span><span class="p">.</span><span class="n">drop</span><span class="p">(</span><span class="s">"RESPONSE"</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
        
    <span class="c1"># Filter features used in train set only
</span>    <span class="k">if</span> <span class="n">test</span><span class="p">:</span>
        <span class="n">train_feats</span> <span class="o">=</span> <span class="n">pickle</span><span class="p">.</span><span class="n">load</span><span class="p">(</span><span class="nb">open</span><span class="p">(</span><span class="s">"mailout_train_feats.pkl"</span><span class="p">,</span> <span class="s">"rb"</span><span class="p">))</span>
        <span class="n">df_clean</span> <span class="o">=</span> <span class="n">df_clean</span><span class="p">.</span><span class="n">loc</span><span class="p">[:,</span> <span class="n">train_feats</span><span class="p">]</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">train_feats</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">df_clean</span><span class="p">.</span><span class="n">columns</span><span class="p">)</span>
        <span class="n">pickle</span><span class="p">.</span><span class="n">dump</span><span class="p">(</span><span class="n">train_feats</span><span class="p">,</span> <span class="nb">open</span><span class="p">(</span><span class="s">"mailout_train_feats.pkl"</span><span class="p">,</span> <span class="s">"wb"</span><span class="p">))</span>
    
    <span class="c1"># Missing values
</span>    <span class="c1"># Impute clean AZDIAS features using old imputer
</span>    <span class="n">azdias_imputer</span> <span class="o">=</span> <span class="n">pickle</span><span class="p">.</span><span class="n">load</span><span class="p">(</span><span class="nb">open</span><span class="p">(</span><span class="s">"imputer.pkl"</span><span class="p">,</span> <span class="s">"rb"</span><span class="p">))</span>
    <span class="n">df_clean</span><span class="p">.</span><span class="n">loc</span><span class="p">[:,</span> <span class="n">clean_azdias</span><span class="p">.</span><span class="n">columns</span><span class="p">]</span> <span class="o">=</span> <span class="n">azdias_imputer</span><span class="p">.</span><span class="n">transform</span><span class="p">(</span><span class="n">df_clean</span><span class="p">[</span><span class="n">clean_azdias</span><span class="p">.</span><span class="n">columns</span><span class="p">])</span>
    
    <span class="c1"># Impute remaning features
</span>    <span class="k">if</span> <span class="n">test</span><span class="p">:</span>
        <span class="c1"># Load mailout imputer for test set
</span>        <span class="n">mailout_imputer</span> <span class="o">=</span> <span class="n">pickle</span><span class="p">.</span><span class="n">load</span><span class="p">(</span><span class="nb">open</span><span class="p">(</span><span class="s">"mailout_imputer.pkl"</span><span class="p">,</span> <span class="s">"rb"</span><span class="p">))</span>
        <span class="n">df_clean</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">mailout_imputer</span><span class="p">.</span><span class="n">transform</span><span class="p">(</span><span class="n">df_clean</span><span class="p">),</span> <span class="n">columns</span><span class="o">=</span><span class="n">df_clean</span><span class="p">.</span><span class="n">columns</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="c1"># Fit imputer for train set and pickle to load with test set
</span>        <span class="n">mailout_imputer</span> <span class="o">=</span> <span class="n">SimpleImputer</span><span class="p">(</span><span class="n">strategy</span><span class="o">=</span><span class="s">"mean"</span><span class="p">)</span>
        <span class="n">df_clean</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">mailout_imputer</span><span class="p">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">df_clean</span><span class="p">),</span> <span class="n">columns</span><span class="o">=</span><span class="n">df_clean</span><span class="p">.</span><span class="n">columns</span><span class="p">)</span>
        <span class="n">pickle</span><span class="p">.</span><span class="n">dump</span><span class="p">(</span><span class="n">mailout_imputer</span><span class="p">,</span> <span class="nb">open</span><span class="p">(</span><span class="s">"mailout_imputer.pkl"</span><span class="p">,</span> <span class="s">"wb"</span><span class="p">))</span>
    
    <span class="c1"># PCA features
</span>    <span class="n">df_pre_pca</span> <span class="o">=</span> <span class="n">df_clean</span><span class="p">[</span><span class="n">clean_azdias</span><span class="p">.</span><span class="n">columns</span><span class="p">]</span>
    <span class="n">df_pre_pca_scaled</span> <span class="o">=</span> <span class="n">scaler</span><span class="p">.</span><span class="n">transform</span><span class="p">(</span><span class="n">df_pre_pca</span><span class="p">)</span>
    <span class="n">df_pca</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">pca</span><span class="p">.</span><span class="n">transform</span><span class="p">(</span><span class="n">df_pre_pca_scaled</span><span class="p">)[:,</span> <span class="p">:</span><span class="n">d</span><span class="p">],</span> 
                          <span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="sa">f</span><span class="s">"pca_</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s">"</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">d</span><span class="p">)])</span>

    <span class="c1"># Cluster distances
</span>    <span class="n">df_distances</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">kmeans</span><span class="p">.</span><span class="n">transform</span><span class="p">(</span><span class="n">df_pca</span><span class="p">),</span>
                                <span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="sa">f</span><span class="s">"cluster_</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s">"</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">kmeans</span><span class="p">.</span><span class="n">cluster_centers_</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])])</span>

    <span class="c1"># Cluster labels
</span>    <span class="n">df_clusters</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">Series</span><span class="p">(</span><span class="n">kmeans</span><span class="p">.</span><span class="n">predict</span><span class="p">(</span><span class="n">df_pca</span><span class="p">),</span> <span class="n">name</span><span class="o">=</span><span class="s">'label'</span><span class="p">)</span>
    
    <span class="c1"># Concatenate all features
</span>    <span class="n">X</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">concat</span><span class="p">([</span><span class="n">df_clean</span><span class="p">,</span> <span class="n">df_pca</span><span class="p">,</span> <span class="n">df_distances</span><span class="p">,</span> <span class="n">df_clusters</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    
    <span class="k">return</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span>
</code></pre></div></div>

<h2 id="hyperparameter-tuning-the-final-pipeline">Hyperparameter Tuning The Final Pipeline</h2>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Prepare training set
</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span> <span class="o">=</span> <span class="n">prepare_mailout</span><span class="p">(</span><span class="n">mailout_train</span><span class="p">)</span>

<span class="n">pipeline</span> <span class="o">=</span> <span class="n">Pipeline</span><span class="p">([</span>
    <span class="p">(</span><span class="s">'k_best'</span><span class="p">,</span><span class="n">SelectKBest</span><span class="p">()),</span>
    <span class="p">(</span><span class="s">'clf'</span><span class="p">,</span> <span class="n">BalancedRandomForestClassifier</span><span class="p">(</span><span class="n">random_state</span><span class="o">=</span><span class="n">seed</span><span class="p">))</span>
<span class="p">])</span>

<span class="n">pipeline_grid</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s">"k_best__k"</span><span class="p">:</span> <span class="p">[</span><span class="nb">int</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">np</span><span class="p">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">start</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">stop</span><span class="o">=</span><span class="mi">30</span><span class="p">,</span> <span class="n">num</span><span class="o">=</span><span class="mi">5</span><span class="p">)],</span>
    <span class="s">"clf__n_estimators"</span><span class="p">:</span> <span class="p">[</span><span class="nb">int</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">np</span><span class="p">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">start</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">stop</span><span class="o">=</span><span class="mi">3000</span><span class="p">,</span> <span class="n">num</span><span class="o">=</span><span class="mi">5</span><span class="p">)],</span>
    <span class="s">"clf__max_depth"</span><span class="p">:</span> <span class="p">[</span><span class="nb">int</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">np</span><span class="p">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">110</span><span class="p">,</span> <span class="n">num</span> <span class="o">=</span> <span class="mi">5</span><span class="p">)]</span> <span class="o">+</span> <span class="p">[</span><span class="bp">None</span><span class="p">],</span>
    <span class="s">"clf__min_samples_split"</span><span class="p">:</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">10</span><span class="p">],</span>
    <span class="s">"clf__min_samples_leaf"</span><span class="p">:</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">],</span>
    <span class="s">"clf__bootstrap"</span><span class="p">:</span> <span class="p">[</span><span class="bp">True</span><span class="p">,</span> <span class="bp">False</span><span class="p">],</span>
<span class="p">}</span>

<span class="n">combs</span> <span class="o">=</span> <span class="mi">1</span>

<span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">params</span> <span class="ow">in</span> <span class="n">pipeline_grid</span><span class="p">.</span><span class="n">items</span><span class="p">():</span>
    <span class="n">combs</span> <span class="o">*=</span> <span class="nb">len</span><span class="p">(</span><span class="n">params</span><span class="p">)</span>
    
<span class="k">print</span><span class="p">(</span><span class="s">"Total number of combinations in parameters grid:"</span><span class="p">,</span> <span class="n">combs</span><span class="p">)</span>
</code></pre></div></div>

<p>There 2700 different combinations to the hyperparamter grid that we have set. Therefore it is only logical to use randomized grid search, as we don’t have the computational or time resources to find the best combination by brute force.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Instantiate StratifiedKFold object for CV
</span><span class="n">skf</span> <span class="o">=</span> <span class="n">StratifiedKFold</span><span class="p">(</span><span class="n">n_splits</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>

<span class="c1"># Use RandomizedSearch to find the best hyperparameters combination
</span><span class="n">pipeline_random</span> <span class="o">=</span> <span class="n">RandomizedSearchCV</span><span class="p">(</span><span class="n">estimator</span><span class="o">=</span><span class="n">pipeline</span><span class="p">,</span> <span class="n">param_distributions</span><span class="o">=</span><span class="n">pipeline_grid</span><span class="p">,</span>
                                     <span class="n">scoring</span><span class="o">=</span><span class="s">'recall_macro'</span><span class="p">,</span> <span class="n">n_iter</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="n">skf</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span>
                                     <span class="n">random_state</span><span class="o">=</span><span class="n">seed</span><span class="p">,</span> <span class="n">n_jobs</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>

<span class="c1"># Fit the RandomizedSearch model to the training data
</span><span class="n">pipeline_random</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">.</span><span class="n">values</span><span class="p">,</span> <span class="n">y_train</span><span class="p">.</span><span class="n">values</span><span class="p">)</span>
</code></pre></div></div>

<h2 id="randomizedsearchcv-results">RandomizedSearchCV Results</h2>

<div class="language-markdown highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Best parameters found: {'k_best__k': 5, 
                        'clf__n_estimators': 2500,
                        'clf__min_samples_split': 10,
                        'clf__min_samples_leaf': 2, 
                        'clf__max_depth': 35, 
                        'clf__bootstrap': True}
</code></pre></div></div>

<h2 id="checking-metrics-of-final-pipeline">Checking metrics of final pipeline</h2>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">pipeline</span><span class="p">.</span><span class="n">set_params</span><span class="p">(</span><span class="o">**</span><span class="n">best_params</span><span class="p">)</span>

<span class="n">model_validation</span><span class="p">(</span><span class="n">pipeline</span><span class="p">,</span> <span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">final_results</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">plot_confusion</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-markdown highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Final Report:
              precision    recall  f1-score   support<span class="sb">

           0       1.00      0.70      0.82     34657
           1       0.03      0.84      0.07       436

    accuracy                           0.70     35093
</span>   macro avg       0.52      0.77      0.44     35093
weighted avg       0.99      0.70      0.81     35093

Metric Score: 0.7696068445499852
</code></pre></div></div>

<p><img src="/images/arvato/confusion_matrix.png" alt="Confusion Matrix" /></p>

<p>The results aren’t state of the art, but we can see that we are able to predict 84% of the responsive individuals, and only 30% of non-responsive ones are false positives.</p>

<p>Comparing the results to the baseline model, the Macro Recall has improved by 8.62%</p>

<h2 id="tuning-the-decision-boundary">Tuning The Decision Boundary</h2>

<p>Now the final the thing that we can do is to tune the decision boundary to get the best results that we can get using this model. In order to do this, we need to predict the probability of classes first.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">skf</span> <span class="o">=</span> <span class="n">StratifiedKFold</span><span class="p">(</span><span class="n">n_splits</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="n">seed</span><span class="p">)</span>

<span class="c1"># Make empty y_pred to fill predictions of each fold
</span><span class="n">y_pred</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">y_train</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">2</span><span class="p">))</span>

<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="p">(</span><span class="n">train_idx</span><span class="p">,</span> <span class="n">test_idx</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">skf</span><span class="p">.</span><span class="n">split</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)):</span>
    <span class="n">X_tr</span><span class="p">,</span> <span class="n">X_test</span> <span class="o">=</span> <span class="n">X_train</span><span class="p">.</span><span class="n">iloc</span><span class="p">[</span><span class="n">train_idx</span><span class="p">],</span> <span class="n">X_train</span><span class="p">.</span><span class="n">iloc</span><span class="p">[</span><span class="n">test_idx</span><span class="p">]</span>
    <span class="n">y_tr</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">y_train</span><span class="p">.</span><span class="n">iloc</span><span class="p">[</span><span class="n">train_idx</span><span class="p">],</span> <span class="n">y_train</span><span class="p">.</span><span class="n">iloc</span><span class="p">[</span><span class="n">test_idx</span><span class="p">]</span>

    <span class="c1"># Make copy of model
</span>    <span class="n">model_clone</span> <span class="o">=</span> <span class="n">clone</span><span class="p">(</span><span class="n">pipeline</span><span class="p">)</span>

    <span class="c1"># Fit model
</span>    <span class="n">model_clone</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_tr</span><span class="p">,</span> <span class="n">y_tr</span><span class="p">)</span>

    <span class="c1"># Predict fold y_test and add in y_pred
</span>    <span class="n">fold_pred</span> <span class="o">=</span> <span class="n">model_clone</span><span class="p">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
    <span class="n">y_pred</span><span class="p">[</span><span class="n">test_idx</span><span class="p">,</span> <span class="p">:]</span> <span class="o">+=</span> <span class="n">fold_pred</span>

<span class="n">threshs</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>
<span class="n">f1_scores</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">recall_scores</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">precision_scores</span> <span class="o">=</span> <span class="p">[]</span>

<span class="k">for</span> <span class="n">thresh</span> <span class="ow">in</span> <span class="n">threshs</span><span class="p">:</span>
    <span class="n">thresh_pred</span> <span class="o">=</span> <span class="p">(</span><span class="n">y_pred</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">]</span> <span class="o">&gt;</span> <span class="n">thresh</span><span class="p">).</span><span class="n">astype</span><span class="p">(</span><span class="nb">int</span><span class="p">)</span>
    <span class="n">f1_scores</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">f1_score</span><span class="p">(</span><span class="n">y_train</span><span class="p">,</span> <span class="n">thresh_pred</span><span class="p">,</span> <span class="n">average</span><span class="o">=</span><span class="s">"macro"</span><span class="p">))</span>
    <span class="n">recall_scores</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">recall_score</span><span class="p">(</span><span class="n">y_train</span><span class="p">,</span> <span class="n">thresh_pred</span><span class="p">,</span> <span class="n">average</span><span class="o">=</span><span class="s">"macro"</span><span class="p">))</span>
    <span class="n">precision_scores</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">precision_score</span><span class="p">(</span><span class="n">y_train</span><span class="p">,</span> <span class="n">thresh_pred</span><span class="p">,</span> <span class="n">average</span><span class="o">=</span><span class="s">"macro"</span><span class="p">,</span> <span class="n">zero_division</span><span class="o">=</span><span class="mi">1</span><span class="p">))</span>
    
<span class="n">best_recall_thresh</span> <span class="o">=</span> <span class="n">threshs</span><span class="p">[</span><span class="n">np</span><span class="p">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">recall_scores</span><span class="p">)]</span>
<span class="k">print</span><span class="p">(</span><span class="s">"Threshold optimizing Recall: {:.3f}"</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="n">best_recall_thresh</span><span class="p">))</span>
</code></pre></div></div>

<div class="language-markdown highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Threshold optimizing Recall: 0.516
</code></pre></div></div>

<p><img src="/images/arvato/confusion_matrix1.png" alt="Confusion Matrix1" /></p>

<p>We can see that the best threshold slightly improved the precision of the model, by decreasing the number of false positives from 10327 to 10283.</p>

<h2 id="final-remarks">Final Remarks</h2>

<p>In each step we have done in the pipeline so far, there must have been other ways we could have achieved the same or even better results. But speaking from a business perspective, if this dataset provided the data one previous campaign were the conversion rate was minute as seen, using this model which isn’t particularly state of the art, would definitely decrease the costs and increase the conversion rate of the campaign due to the increased selectivity.</p>

<p>And now we can train the final model to predict the test set.</p>

<h2 id="training-final-model">Training Final Model</h2>

<div class="language-markdown highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pipeline.set_params(<span class="ge">**</span>best_params)
pipeline.fit(X_train, y_train)
</code></pre></div></div>

<h2 id="test-set-prediction">Test Set Prediction</h2>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Reading test set
</span><span class="n">mailout_test</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s">'../../data/Term2/capstone/arvato_data/Udacity_MAILOUT_052018_TEST.csv'</span><span class="p">,</span> <span class="n">sep</span><span class="o">=</span><span class="s">';'</span><span class="p">)</span>

<span class="c1"># Copying LNR column for Kaggle submission csv file
</span><span class="n">test_LNR</span> <span class="o">=</span> <span class="n">mailout_test</span><span class="p">[</span><span class="s">'LNR'</span><span class="p">]</span>

<span class="c1"># Data Preparation
</span><span class="n">X_test</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">prepare_mailout</span><span class="p">(</span><span class="n">mailout_test</span><span class="p">,</span> <span class="n">test</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

<span class="c1"># Prediction
</span><span class="n">test_preds</span> <span class="o">=</span> <span class="n">pipeline</span><span class="p">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>

<span class="c1"># Kaggle csv submission
</span><span class="n">submission</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">DataFrame</span><span class="p">({</span><span class="s">'LNR'</span><span class="p">:</span><span class="n">test_LNR</span><span class="p">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">int32</span><span class="p">),</span> <span class="s">'RESPONSE'</span><span class="p">:</span><span class="n">test_preds</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">]})</span>
<span class="n">submission</span><span class="p">.</span><span class="n">to_csv</span><span class="p">(</span><span class="s">'kaggle.csv'</span><span class="p">,</span> <span class="n">index</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
</code></pre></div></div>

<h2 id="conclusion">Conclusion</h2>

<p>To summarize what we have done in the project so far:</p>

<ol>
  <li>We explored the general population dataset to understand how it should be cleaned for our analysis</li>
  <li>We made a pipeline for cleaning the general population dataset and any dataset that has a similar structure</li>
  <li>We performed dimensionality reduction on the general population dataset followed by a clustering analysis of the population</li>
  <li>We cleaned the customers’ dataset using the pipeline we previously made and analyzed the clusters’ representation of the business’s customer base</li>
  <li>We analyzed the characteristics of our customer base and how they differ from non-customers</li>
  <li>We analyzed the differences between different clusters in the customer base</li>
  <li>We explored the <code class="language-plaintext highlighter-rouge">MAILOUT</code> dataset and made a pipeline to prepare the dataset for the supervised learning task</li>
  <li>We analyzed different algorithms and metrics then selected the best ones that suited the situation of the dataset we have which was Balanced Random Forests for the algorithm and Macro Recall for metric which deals with the target class imbalance</li>
  <li>We tested using feature selection in the pipeline to improve the results and found that it had indeed improved it</li>
  <li>We made a final pipeline and tuned it’s hyperparameters to predict individuals with high probability responding the mail-out campaign</li>
</ol>

<h2 id="reflection">Reflection</h2>

<p>Cleaning the general population dataset was really challenging for me, as it was the first time I’ve ever dealt with dataset this size, and I didn’t know where to begin in exploring it.</p>

<p>This has forced me to find some methods to be able to digest the data in smaller portions to get a general idea about how it should be dealt with, like to into categories separately.</p>

<h2 id="improvement">Improvement</h2>

<p>I really enjoyed the customer segmentation part, even though it could be improved since we only tried reducing dimensionality using PCA and clustered using K-Means, where we could have used different algorithms for clustering such as DBSCAN, Agglomerative Clustering or Gaussian Mixture Models.</p>

<p>Anyways I have learnt much through out this project and the Nano-degree in general, and I hope that you have enjoyed my capstone project’s write up.</p>

<h3 id="thanks-for-reading">Thanks for reading.</h3>

  </div><a class="u-url" href="/blog/2021/09/25/Arvato_Capstone_Project.html" hidden></a>
</article>

      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/blog/"></data>

  <div class="wrapper">

    <div class="footer-col-wrapper">
      <div class="footer-col">
        <p class="feed-subscribe">
          <a href="/blog/feed.xml">
            <svg class="svg-icon orange">
              <use xlink:href="/blog/assets/minima-social-icons.svg#rss"></use>
            </svg><span>Subscribe</span>
          </a>
        </p>
      </div>
      <div class="footer-col">
        <p>blog</p>
      </div>
    </div>

    <div class="social-links"><ul class="social-media-list"><li><a rel="me" href="https://github.com/ahmedsamirio" target="_blank" title="ahmedsamirio"><svg class="svg-icon grey"><use xlink:href="/blog/assets/minima-social-icons.svg#github"></use></svg></a></li><li><a rel="me" href="https://twitter.com/ahmedsamirio" target="_blank" title="ahmedsamirio"><svg class="svg-icon grey"><use xlink:href="/blog/assets/minima-social-icons.svg#twitter"></use></svg></a></li></ul>
</div>

  </div>

</footer>
</body>

</html>

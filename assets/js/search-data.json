{
  
    
        "post0": {
            "title": "What I Learned From My First Serious Kaggle Competition",
            "content": "Introduction . Recently I decided that I want to be a Kaggle grand-master, and so I set it out to achieve this goal by breaking it down into multiple steps, the first of which was to win 3 medals (not necessarily gold) by the end of this year. . I participated in UW-Madison GI Tract Segmentation competition without knowing the first thing about semantic segmentation, and through steady effort I was able to win a silver medal and reach the 23rd place (out of 1565 teams - Top 2%). . But through out this time, I often felt unorganized in my approach and distracted by . The shear amount of mistakes and bugs in my code that I want to fix | The amount of things and possible improvements that I want to implement | The amount of things that I want to learn | What I learned . So along this way, I came up with a system that isn’t perfect, but currently suits the job. . Also, once the competition finished, I reflected upon this experience to extract some lessons to help me win more competitions, and hopefully you too. . Make notebook templates to store your code in an organized way . Through out a competition you’ll need 4 kinds of notebooks . A notebook for data exploration | A notebook for developing ideas | A notebook for experimental runs | A notebook for inference | Why do I think you need 4 notebooks? Well not exactly 4, it’s not carved on stone, but you need to organize your code in way that enables you to iterate quickly, and if all of your work is in just one notebook, you’ll find yourself distracted by the different parts you have in there. And whenever you try to reach a part or track something that you’ve written before, it’ll be a nightmare. . Package the code that you have tested to enable using it in multiple places . Now say that you have written some code and organized it into functions or classes, does it make sense to keep copying it around all of your notebooks? . Or say that you are using something other than kaggle notebooks, does it make sense to copy all of your helper functions in your kaggle notebook? . That doesn’t make any sense. What makes sense is that you package any code that you have written and tested into a local package, and maybe you can even upload that package as a dataset, so whenever you are switching over to kaggle notebooks, you know that your functions will be there too without copying and pasting anything. . Develop a robust validation plan . . What good are your ideas if you can’t test them? If you don’t have good CV, then you are basically sacrificing your precious work down the drain. . If you only follow the public LB, you might get shaken up in the end. What you need to do instead is . Read the competition guidelines and try to replicate their validation strategy | Engage in the discussion forums with other participants and ask about CV if there is something that you don’t get | Take some time in the early days to validate the your validation plan by making arbitrary submissions and check CV - LB score correlation | Keep a log to track what you did on a daily basis . Some days you’ll feel a little bit under the weahter, and maybe you’ll go check the LB and see that your rank is low. . You’ll take a look at the top X% of the LB and compare yourself. . You might feel that it’s hopeless to even try, because look at these people, they must be geniuses, and I’m struggling to move up my rank by 1 position. . Please don’t do that. Don’t compare yourself to others, and to avoid that, I suggest keeping a log of everything you do in the competition everyday. . Log every idea you have, every little win you made, every bug you fixed, etc.. . . A log will help you see the trail of compound improvements that you have made since starting the competition, and by the end you’ll be astonished by what you have achieved. . It’s okay to check the leader-board, but don’t get sucked into it. Instead, check this log and be proud of yourself. Compare yourself only with your past self and just keep on making little steps and wins one day at a time. . Keep a list of any improvement and ideas that come to mind or that you have read which you can try . Often times you’ll get some new ideas that you want to implement, and you might be implementing something just to get a new idea while doing that or based on the results of that idea. . That’s why I suggest keeping a list with literally every idea that you get. . Sometimes you’ll have more ideas that you can implement, and sometimes you’ll have a shortage of ideas, which is why keeping a list is paramount to continuous improvement. . Keep a list of any problem that faces you and prioritize solving them . This goes without saying, once a problem occurs you have to make it top priority to solve. Once a problem occurs during submission, like a notebook timeout, or running out of memory, you need to prioritize fixing, or at least understanding why that happens. . Sometimes you can postpone fixing that, and then you might find later that your approach is incompatible and unsuitable for submission. . So to avoid wasting time and energy, find the root cause of any problem that occurs and prioritize fixing it. . Keep a list of the things that you need to educate yourself about . Often times you’ll find that you don’t understand something, and it can become overwhelming because this often times for me is like every 5 lines of code when I’m reading someone’s notebook. . So why not keep a list of everything you don’t understand? . Like when you are reading someone’s notebook, a blog post, or a previous competition winning solution. Whenever you encounter something that you don’t understand, add it to the the list, or a list particular to this thing you are reading, and then tackle it one by one. . You’ll find that it was easier than you expected, but the shear amount of things that you don’t know can sometimes be overwhelming. . And if there is something that I want you to take out of this post in general, it is to take everything one step at a time. . Make a plan based on the three lists that you have for what you should be doing in the next X days . In kaggle competitions (and maybe in life in general), the perfect is the enemy of the good. . Without a plan you’ll find that you are distracted, and whenever you gather the will required to do something, you might procrastinate while saying that maybe what you are trying to implement, learn or fix isn’t going to improve your results anyway. . That’s where a plan comes into place. By now if you have the lists we were talking about, you already know some stuff which you should work on. . So make a plan, and start working on it. Period. . What if that didn’t work out as planned? Well, I guarantee you that most of the time, it won’t work out as planned, but you never know what you don’t know unless you try it. . Sometimes one step of your plan might open up a road that you didn’t know exist, and sometimes a failure or a bug can teach you lessons that you might have not learned otherwise. . The point is, you will be the winner every single time. . So make a plan, follow it, update it based on what you find, and repeat. . Set a daily time to scan through new notebooks and discussions for any new ideas or information and jot them down in your log and lists accordingly . I can’t emphasize enough how important is this. The beautiful thing about these competitions is that you’ll often find people think a little bit different than you. . That’s why keeping an open mind while checking the discussion and notebooks is an amazing way to generate new ideas, and maybe be borrow some. . Sometimes you’ll find ideas so simple can provide tremendous improvements, and they haven’t even crossed your mind. . One thing that I found pretty useful too is to sift through similar old competitions notebooks and discussions for useful information, and often times you’ll find some hidden gems. . For example, while working on data augmentation, I didn’t know where to start, and I thought the augmentations used in the public notebook of the competition were either too basic, or too complicated. . So while looking into a previous competition winning solutions, I found a nice augmentation pipeline, so I tested that, and with a little tweaks I found that my models trained better. . Believe in yourself . . I personally find this step a little bit hard, and I don’t know about you. But if you believe that you can’t be winner, you probably won’t be. . That’s not because you can’t actually be a winner, but because you won’t see the ways that can make you one. . Over confidence is never good, but under selling yourself and having low self esteem won’t get you anywhere either. . Instead, understand your weaknesses and develop them. . Know your strengths and improve them. . Understand that now one is born with kaggle gold medal (of course some people find it easier that others), but most people start somewhere, and if they want to improve their positioning, they work on themselves and do it. . TL;DR . Make notebook templates to store your code in an organized way. | Package the code that you have tested to enable using it in multiple places. | Explore and develop and robust CV | Keep a log to track what you did on a daily basis | Keep a list of any improvement and ideas that come to mind or that you have read which you can try | Keep a list of any problem that faces you and prioritize solving them | Keep a list of the things that you need to educate yourself about | Begin everyday by reviewing the log to motivate yourself about the stuff that you have already worked on | Always make a plan based on the three lists that you have for what you should be doing in the next 3 days for example to avoid feeling distracted by all the information | Set a daily time to scan through new notebooks and discussions for any new ideas or information and jot them down in your log and lists accordingly | Don’t watch your models train | Last but not least, believe in yourself. |",
            "url": "https://ahmedsamirio.github.io/blog/markdown/2022/07/16/kaggle-tips.html",
            "relUrl": "/markdown/2022/07/16/kaggle-tips.html",
            "date": " • Jul 16, 2022"
        }
        
    
  
    
        ,"post1": {
            "title": "Bertelsmann Arvato Capstone Project",
            "content": ". First of all, I’m really happy to be writing this blog post as this signals the end of my Data Scientist Nano-degree journey generally, and the capstone project specifically. . I’d like to thank Udacity for providing us with this amazing dataset, which enabled me to test my abilities and to really learn some things that couldn’t be learned through lessons alone. . Projet Overview . In this project, I role-played that I was a Data Scientist at Bertelsmann Arvato Analytics, where I was handed over a project where the stakeholder was a mail-order sales company in Germany. . According to britannica, a mail order business is a . “method of merchandising in which the seller’s offer is made through mass mailing of a circular or catalog or through an advertisement placed in a newspaper or magazine and in which the buyer places an order by mail.” . And this stakeholder wanted two things from us: . First it wanted to understand which parts of the general population described the core customer base of the company. | Second it wanted a model which predicts which individuals are most likely to convert new campaigns. | The datasets that were provided with this project were: . A dataset which holds demographic information about the general population in Germany. | A dataset which holds demographic information about the customer base of the stakeholder. | A dataset which has the demographic information about the individuals involved in their campaigning, and whether they converted or not. | The datasets that we had were more than enough to begin addressing the requests of the stakeholder, but before we got to do these tasks we needed do some: . Data Exploration | Data Cleaning | The datasets that were provided were: . | Udacity_AZDIAS_052018.csv: Demographics data for the general population of Germany; 891 211 persons (rows) x 366 features (columns). | Udacity_CUSTOMERS_052018.csv: Demographics data for customers of a mail-order company; 191 652 persons (rows) x 369 features (columns). | Udacity_MAILOUT_052018_TRAIN.csv: Demographics data for individuals who were targets of a marketing campaign; 42 982 persons (rows) x 367 (columns). | Udacity_MAILOUT_052018_TEST.csv: Demographics data for individuals who were targets of a marketing campaign; 42 833 persons (rows) x 366 (columns). | DIAS Information Levels - Attributes 2017.xlsx: Information about features in demographic data. | DIAS Attributes - Values 2017.xlsx: Information about values of each feature in demographic data. | Problem Statement . We need to find the segments in the general population which constitute the core customer base of the business. | We need find out which targeted individuals in the new mailout campaign are more likely to convert. | How can we compare the populations of customers with the general population? . First we shall explore the data to get to know the features within it. | Then we shall segment the general population. | Finally we shall figure out which segments do our customers belong to. | How can we find out which targeted individuals in the new campaign are likely to convert? . We can use the provided features in addition to features we engineered from the unsupervised analysis to make a new dataset. | Then we can use supervised learning to train a model to predict which individuals are more likely to convert | Metrics . Customer Segmentation . We can use intertia to judge how many clusters we shall set the K-Means algorithm to find. Where inertia is the average distance between each data point and it’s cluster center. The smaller the intertia, the better the model. However, inertia tends to decrease as we increase the number of clusters, so we will use the elbow method to determine the optimal cluster number where the improvement in inertia starts slowing down. . Mailout Campaign . That depends on margin of error that we are willing to except with our model. So for example, we might want a model that is able to predict all individuals likely to be customers, despite predicting a large sum of individuals that won’t be customers. In this case we’d want a model with higher Recall. . On the other hand, sending out to a huge mass might be expensive and counter-intuitive business-wise in some case. In this case we’d want a model with higher Precision. . In order to get the best of both worlds, we can use the F1-Score, which calculate a harmonic average of Recall and Precision, penalizing a model that has bad scores for either one of the metrics. . Personally I prefer using F1-Score, specifically the macro-averaged F1-Score, which calculate the average of F1-Score for each class regardless the number of data points belonging to that class, as a weighted average F1-Score wouldn’t account the main class we are concerned with predicting due to it’s extremely low number. . However, I think that getting a model with high precision with this dataset would be a stretch, so my best guess right now is to use Recall, but the other metric to avoid a model with really bad precision. . So I’ll use Macro Recall, which calculates the recall of each class individually, then averages them, which is great for imbalanced classification. . Data Exploration . The two main datasets that we were dealing with had a setback, they were both in CSV format, and due to there size, they took a really long time to load (around 25 minutes). . Another hurdle was that we couldn’t just pickle them as the Udacity workspace memory couldn’t handle two copies of the two datasets at the same time. . So the first thing I did was to figure out a way to reduce the size of these datasets by manipulating the data-types of their features, and then pickling them. . read_pickle = False # csv data path data_path = &#39;../../data/Term2/capstone/arvato_data/&#39; # csv files azdias_csv = &#39;Udacity_AZDIAS_052018.csv&#39; customers_csv = &#39;Udacity_CUSTOMERS_052018.csv&#39; # csv file paths azdias_csv_path = os.path.join(data_path, azdias_csv) customers_csv_path = os.path.join(data_path, customers_csv) # pickle files azdias_pickle = &#39;Udacity_AZDIAS_052018.csv.pandas.pickle&#39; customers_pickle = &#39;Udacity_CUSTOMERS_052018.csv.pandas.pickle&#39; # load pickled datasets if exists if azdias_pickle in os.listdir() and customers_pickle in os.listdir(): print(&quot;Loading AZDIAS pickle...&quot;) azdias = pd.read_pickle(azdias_pickle) print(&quot;Loading CUSTOMERS pickle...&quot;) customers = pd.read_pickle(customers_pickle) read_pickle = True # else load csv and save pickles else: print(&quot;Loading AZDIAS csv...&quot;) azdias = pd.read_csv(azdias_csv_path, sep=&#39;;&#39;) print(&quot;Loading CUSTOMERS csv...&quot;) customers = pd.read_csv(customers_csv_path, sep=&#39;;&#39;) print(&quot;Loading Attributes Sheet...&quot;) dias_atts = pd.read_excel(&quot;DIAS Information Levels - Attributes 2017.xlsx&quot;) print(&quot;Loading Values Sheet...&quot;) dias_vals = pd.read_excel(&quot;DIAS Attributes - Values 2017.xlsx&quot;) print(&quot;Done.&quot;) if not read_pickle: azdias_p, azdias_na = reduce_mem_usage(azdias) if not read_pickle: customers_p, customers_na = reduce_mem_usage(customers) # save datasets in pickle format for later usage if not read_pickle: pd.to_pickle(azdias_p, azdias_pickle) pd.to_pickle(customers_p, customers_pickle) . Therefore, I significantly decreased their load time (practically made it in seconds instead of almost half an hour), and made it easy to save them in the workspace. . After taking a quick look into the dimensions of the demographic datasets I found the following: . # looking into the general population dataset print(&#39;Shape:&#39;, azdias.shape) # (891221, 366) # looking into the customers dataset print(&#39;Shape:&#39;, customers.shape) # (191652, 369) . 366 features were shared between these two datasets, and 3 additional features were present in CUSTOMERS relating only to customers which were: . CUSTOMER_GROUP | PRODUCT_GROUP | ONLINE_PURCHASE | Each row in the demographic data represented a single individual, but it also contained aggregated information about their household, their building, their neighborhood, their community, and more aggregations relating to different perspectives. . The thought of trying to understand these 366 features was overwhelming, as it was the first time I have ever tackled a dataset with this size. So I resorted to the sheets provided about the them. . Information about these features were included in DIAS Information Levels - Attributes 2017.xlsx and DIAS Attributes - Values 2017.xlsx, however after looking into the number of features included in these sheets and the features available in the two demographic datasets, I found some features to be missing from the sheets, and some features in the sheets that were missing from the demographic data. . print(&quot;Number of features in Values:&quot;, dias_vals.Attribute.dropna().nunique()) # 314 print(&quot;Number of features in Attributes:&quot;, dias_atts.Attribute.dropna().nunique()) # 313 . It turned out that the demographic data belonged to 2018, while these sheets belonged to the same data source, but one that was aggregated in 2017. . After taking a look at both sheets I found another thing, despite the dataframe having null values, most features had values in DIAS Attributes - Values 2017.xlsx that encoded unknown or missing values. So that urged me to explore missing values in the datasets, which could get me familiar with the data and break the ice between us before delving into each feature individually. . So I made a function that calculates the distribution of null percentages in a dataset in order to enables easy exploration before and after replacing the unknown values with null. . def get_null_prop(df, axis=0, plot=True): &quot;&quot;&quot;Calculates null proportions in dataframe columns or rows.&quot;&quot;&quot; # calculate null proportion of each column or row null_prop = (df.isnull().sum(axis=axis) / df.shape[axis]).sort_values() if plot: null_prop.hist(edgecolor=&#39;black&#39;, linewidth=1.2) return null_prop . azdias_null_cols = get_null_prop(azdias, plot=False) customers_null_cols = get_null_prop(customers, plot=False) azdias_null_cols.hist(bins=10, alpha=0.7, label=&#39;AZDIAS&#39;) customers_null_cols.hist(bins=10, alpha=0.7, label=&#39;CUSTOMERS&#39;) plt.title(&quot;Distribution of Null Values in Columns&quot;); plt.legend(); . . azdias_null_rows = get_null_prop(azdias, 1, False) customers_null_rows = get_null_prop(customers, 1, False) azdias_null_rows.hist(bins=10, alpha=0.7, label=&#39;AZDIAS&#39;) customers_null_rows.hist(bins=10, alpha=0.7, label=&#39;CUSTOMERS&#39;) plt.title(&quot;Distribution of Null Values in Rows&quot;); plt.legend(); . . From the above graphs we can see that the null percentages are definitely higher in AZDIAS , and since we will be modeling the clustering algorithm on it, we will focus our exploration on it, and only look into CUSTOMER if needed. . Then I made a function to replace the unknown values in each features with null, in order to look into the real percentage of missing values in rows and columns. . def replace_unknown_with_null(df): &quot;&quot;&quot;Replace unknown values with null in all features that have that info available.&quot;&quot;&quot; df_new = df.copy() feat_unknown_vals = dias_vals.query(&quot;Meaning == &#39;unknown&#39;&quot;) for feat in feat_unknown_vals.itertuples(): # check if feature in df if feat.Attribute in df_new: # if unknown values are more than one if &#39;,&#39; in str(feat.Value): # loop over unknown values for val in str(feat.Value).split(&#39;,&#39;): # replace unknown value with null df_new.loc[:, feat.Attribute] = df_new.loc[:, feat.Attribute].replace(eval(val), np.nan) else: # replace unknown value with null df_new.loc[:, feat.Attribute] = df_new.loc[:, feat.Attribute].replace(feat.Value, np.nan) return df_new # replace unknown values with null azdias_new = replace_unknown_with_null(azdias) . After replacing the unknown values with null, I visualized the null percentages in old and new AZDIAS in order to understand the consequences of this step. . . . After replacing the unknown values with null, we can see more features having higher percentage of null values than before. . The question then was how to deal with these missing values? . This will depend completely on the type of feature we are dealing with. As missing values can be divided into 3 categories: . Missing completely at random (MCAR): where the absence of such data is completely unrelated to other observed data and unobserved data, which means that there is no pattern to the missing data. | Missing at random (MAR): where the absence of such data is related to other observed data but not unobserved data, which means that there is a pattern to the missing data. | Missing not at random: where the missing data is related to unobserved data and it signifies something, like a column about age of first child while the person related to the data point doesn’t have a child. | We have a lot of features, and fortunately DIAS Information Levels - Attributes 2017.xlsx had them divided into categories. . So I decided that the best way is to skim each category’s features with their description, value counts and percentage of null values, and take notes along the way. . print(&quot;Number of feature categories:&quot;, dias_atts[&quot;Information level&quot;].nunique()) print(&quot; nFeature Categories:&quot;) print(dias_atts[&quot;Information level&quot;].value_counts()) Number of feature categories: 9 Feature Categories: PLZ8 112 Microcell (RR3_ID) 54 Person 42 Household 25 Microcell (RR4_ID) 11 Building 9 RR1_ID 5 Postcode 3 Community 3 Name: Information level, dtype: int64 . And so I made a function that let’s us inspect the features of each category, and get information about it’s description if it was available. . def explore_category(category, azdias): &quot;&quot;&quot;Prints description, null percentage and value counts of each feature in a specified category.&quot;&quot;&quot; # query only features in category cat_feats = dias_atts[dias_atts[&quot;Information level&quot;] == category] # calcualte null percentage of each features cat_feats[&quot;null_percentage&quot;] = cat_feats[&quot;Attribute&quot;].apply(lambda feat: azdias[feat].isna().sum()/azdias.shape[0]) # sort by null percentage cat_feats = cat_feats.sort_values(&quot;null_percentage&quot;) print(f&quot;Number of features in {category}: {len(cat_feats)} n n&quot;) for i, row in cat_feats.iterrows(): feat = row[&quot;Attribute&quot;] print(feat) print(row[&quot;Description&quot;]) print(&quot;Null percentage:&quot;, row[&quot;null_percentage&quot;]) print(azdias[feat].value_counts()) print() . The output of each feature was similar to this . KBA13_ALTERHALTER_30 share of car owners below 31 within the PLZ8 Null percentage: 0.11871354018812394 3.0 333405 2.0 160653 4.0 147128 1.0 72911 5.0 71324 Name: KBA13_ALTERHALTER_30, dtype: int64 . Which enabled us to skim the features in a given category easily and write down notes and thoughts about these features. . PLZ8 Features . According to https://datarade.ai/data-products/plz8-germany-and-plz8-germany-xxl, Germany has been divided into 84,000 PLZ8 boundaries, so this category contains socio-economic data related to each PLZ8 boundary which helps in optimizing distribution of promotional materials, and in our case the mail-order sales. . By skimming through the results I found that that: . All features are ordinal categorical except KBA13_ANZAHL_PKW. | 105 features have the same null percentage which is 11.87%, while the remaining 7 have 13.07% | . This rings the bell for data MCAR or MNAR, as there is a pattern that is most likely unrelated to observed data, but can or can’t be related to unobserved data, which is why some persons just don’t have PLZ8 data collected. . KBA13_ANZAHL_PKW is supposed to encode the number of cars in the PLZ8, but it has high values of peculiar number which are 1400, 1500, 1300, etc. | . We can see that the bins starts getting less granular is we exceed 1200. My guess is that this data was spread between 1300 and the max values, but the granularity of these section was decreased, which explains why the distribution is right skewed but then we start seeing bumps near the end. . We could leave it as is, as I don’t think it would make much difference. Or we can follow the lead of the last section and reduce the granularity of the whole feature. . Microcell (RR3_ID) Features . By skimming through the results I can see that: . The most prominent null percentage is 16.6%, while a few have less and only has 53.46% which is KBA05_BAUMAX (most common building-type within the cell). | Why is this data missing? . This is data about a collective of individuals, and it should mean that we have any indicator about this collective, like an identifier for the microcell of this person, or the plz8. . I looked for any feature that has the postcode or the PLZ8 area or anything related, but I didn’t find one. Therefore I thought that in handling missing data we should do the following: . First we should look for rows that have high percentage of missing values across all feature categories | We should then drop these rows as we can’t use other feature categories to infer them (if we used a method like KNN imputation) | We shall then impute the missing values only when the rows are missing from a feature category, but not from the other. That’s because the data is extremely related to each other (we have information about the person, their household, their community, their area, their microcell and their plz8, and all of these are related to each other, so we can use them to infer missing information about each other.) | Person Features . Notes about the features: . GEBURTSJAHR (year of birth) has 44% missing values (encoded as 0), which need to be dropped, as we can’t infer year of birth, and I don’t think it would be that indicative of anything when we have much more deep information about each individual. | . GEBURTSJAHR year of birth Null percentage: 0.0 0 392318 1967 11183 1965 11090 1966 10933 1970 10883 ... . Some features have 0.5% missing values which are mostly related to social status, and it’s peculiar why this data is missing about these individuals. If these individuals have more missing data in the other feature categories then we can safely drop them and not worry about their missing data type. | PRAEGENDE_JUGENDJAHRE (dominating movement in the person’s youth (avantgarde or mainstream)), NATIONALITAET_KZ (nationaltity), VERS_TYP (insurance typology), HEALTH_TYP (health typology) and SHOPPER_TYP (shopping typology) have around 12% missing values. | AGER_TYP (best-ager typology) has 76% missing values, in addition to around 1% of individuals that couldn’t be classified (encode as 0). | TITEL_KZ (flag whether this person holds an academic title) has 99% missing values, and it could be just that only 1% hold academic titles and hence the data is correct, and it could be that the data isn’t complete and hence we should drop it. | . At this point in the exploration, I was curious to find how are the missing values in current features related with the previous categories’ ones? . So I determined a threshold for the percentage of missing values in a feature to include this test, and that should be the highest repeating missing values percentage we have seen so far which is 16.6%. . # all features with missing percentage less than 17% and higher than 0% features_missing = azdias.columns[((azdias_new.isnull().sum() / azdias.shape[0]) &lt; 0.17) &amp; ((azdias_new.isnull().sum() / azdias.shape[0])) &gt; 0] # narrow down to only features of the explored categories plz8_feats = dias_atts[dias_atts[&quot;Information level&quot;] == &quot;PLZ8&quot;][&quot;Attribute&quot;].unique() rr3_feats = dias_atts[dias_atts[&quot;Information level&quot;] == &quot;Microcell (RR3_ID)&quot;][&quot;Attribute&quot;].unique() person_feats = dias_atts[dias_atts[&quot;Information level&quot;] == &quot;Person&quot;][&quot;Attribute&quot;].unique() features_missing = list(set(plz8_feats).union(rr3_feats).union(person_feats).intersection(features_missing)) . Then I looked into the percentage of rows with 100% missing values. . # azdias with only features that have missing values azdias_missing = azdias[features_missing] # flag rows with all missing values rows_all_missing = azdias_missing.isna().sum(axis=1) == azdias_missing.shape[1] print(&quot;Number of rows with all missing values:&quot;, rows_all_missing.sum()) print(&quot;Percentage of rows with all missing values:&quot;, rows_all_missing.sum()/azdias_missing.shape[0]) Number of rows with all missing values: 7 Percentage of rows with all missing values: 7.854393018117841e-06 . We can see that there are 7 rows that have all the values for the features explored missing, and we already know that there are rows with high percentage of missing features. But how many are there exactly? . rows_missing_p = azdias_missing.isnull().sum(axis=1)/azdias_missing.shape[1] for i in np.arange(0, 0.8, 0.1): print(&quot;Percentage of rows with more than {:.2f}% values missing: {}&quot;.format(i*100, (rows_missing_p&gt;i).sum()/azdias.shape[0])) . Percentage of rows with more than 0.00% values missing: 0.22381092905126787 Percentage of rows with more than 10.00% values missing: 0.1727225906929931 Percentage of rows with more than 20.00% values missing: 0.17272146863684765 Percentage of rows with more than 30.00% values missing: 0.1315577168850375 Percentage of rows with more than 40.00% values missing: 0.11871690635656026 Percentage of rows with more than 50.00% values missing: 0.11871354018812394 Percentage of rows with more than 60.00% values missing: 0.11799766836732976 Percentage of rows with more than 70.00% values missing: 0.11217980725319533 . We can see the there is almost 12% of rows that more than 50% missing values. In order to figure out if these rows are problematic here only or overall in the dataset, we can look into the same information using all features in the dataset. . rows_missing_p = azdias_new.isnull().sum(axis=1)/azdias_new.shape[1] for i in np.arange(0, 0.8, 0.1): print(&quot;Percentage of rows with more than {:.2f}% values missing: {}&quot;.format(i*100, (rows_missing_p&gt;i).sum()/azdias.shape[0])) . Percentage of rows with more than 0.00% values missing: 1.0 Percentage of rows with more than 10.00% values missing: 0.17350802999480489 Percentage of rows with more than 20.00% values missing: 0.14647545333873416 Percentage of rows with more than 30.00% values missing: 0.11872139458114205 Percentage of rows with more than 40.00% values missing: 0.11250408147922905 Percentage of rows with more than 50.00% values missing: 0.11216746463559543 Percentage of rows with more than 60.00% values missing: 0.10454645929573024 Percentage of rows with more than 70.00% values missing: 0.0824902016447099 . We can see that in the whole the dataset 11.2% of rows have more than 50% missing values. This part of the data seemed problematic, and it suggested that these rows should be dropped. But first I wanted to explore them with relationship to all features that have missing values. . And what I did was this, I look for the percentage of each feature’s missing values in these rows compared the total number of missing values available in all rows. . # calculate features missing values feat_missing_count = azdias_new.isna().sum() # filter out rows with more than 50% missing values half_missing_rows = azdias_new[rows_missing_p &gt; 0.5] # transpose the dataframe to make the features as index half_missing_rows_t = half_missing_rows.T # calculate the percentage of null values in these rows for each feature half_missing_rows_t[&quot;null_percentage&quot;] = half_missing_rows_t.isna().sum(axis=1)/feat_missing_count # sort values by null_percentage half_missing_rows_t = half_missing_rows_t.sort_values(&quot;null_percentage&quot;, ascending=False) # select features that have null values half_missing_rows_t = half_missing_rows_t.query(&quot;null_percentage &gt; 0&quot;) # print each feature, category and percent of missing values category_missing = defaultdict(list) category_missing_p = defaultdict(list) category_missing_count = defaultdict(int) for feat, p in half_missing_rows_t[&quot;null_percentage&quot;].iteritems(): if feat in set(dias_atts.Attribute): category = dias_atts.query(&quot;Attribute == @feat&quot;)[&quot;Information level&quot;].item() else: category = &quot;Unknown&quot; category_missing_p[category].append(p) if p &gt; 0.9: category_missing[category].append(feat) category_missing_count[category] += 1 print(feat, category, p) . WOHNLAGE Building 1.0 ANZ_TITEL Household 1.0 EINGEFUEGT_AM Unknown 1.0 DSL_FLAG Unknown 1.0 GEBAEUDETYP Building 1.0 AKT_DAT_KL Unknown 1.0 MIN_GEBAEUDEJAHR Building 1.0 MOBI_RASTER Unknown 1.0 OST_WEST_KZ Building 1.0 SOHO_KZ Unknown 1.0 HH_EINKOMMEN_SCORE Household 1.0 UNGLEICHENN_FLAG Unknown 1.0 KBA05_MODTEMP Building 1.0 EINGEZOGENAM_HH_JAHR Unknown 1.0 ANZ_HAUSHALTE_AKTIV Building 1.0 ALTER_HH Household 1.0 ANZ_STATISTISCHE_HAUSHALTE Unknown 1.0 WOHNDAUER_2008 Household 1.0 ANZ_PERSONEN Household 1.0 ANZ_KINDER Unknown 1.0 ... . for category, p in category_missing_p.items(): plt.hist(p, alpha=0.5, label=category) plt.title(&quot;Distribution of Null Values Ratio between Data with &gt;= 50% Missing Values and Full Data by Category&quot;) plt.xlabel(&quot;Null Ratio&quot;) plt.legend() . . category_mean_p = pd.Series({category: np.mean(p) for category, p in category_missing_p.items()}).sort_values(ascending=False) category_mean_p.plot(kind=&quot;bar&quot;, title=&quot;Mean Null Values Ratio between Data with &gt;= 50% Missing Values and Full Data&quot;); plt.xlabel(&quot;Categories&quot;) plt.ylabel(&quot;Mean Null Ratio&quot;) plt.savefig(&quot;null_ratio_2.png&quot;); . . Using these plots we can see that most of the features will benefit from dropping these rows to get rid of most of the missing values in the data, and the rest can be imputed using the method of our choice. . So I asked myself, if I dropped these rows completely, how many features would have more than 90% of their missing values removed? . # first we need to add the original count of Unknown features original_category_count = dias_atts[&quot;Information level&quot;].value_counts() original_category_count[&quot;Unknown&quot;] = len(azdias_feats.difference(dias_atts_feats)) original_category_count = original_category_count.sort_values(ascending=False) category_half_missing_count = pd.Series(category_missing_count).sort_values(ascending=False) category_half_missing_count[original_category_count.index].plot(kind=&quot;bar&quot;, alpha=0.5, color=&#39;blue&#39;, label=&quot;Missing rows&quot;) original_category_count.plot(kind=&quot;bar&quot;, alpha=0.5, color=&#39;green&#39;, label=&quot;All&quot;) plt.xticks(rotation=90); plt.legend(); plt.title(&quot;Number of Features with More than 90% Missing Values in Rows with More than 50% Missing Values&quot;) . . This graph gives a much clearer picture of what is going on, so let me explain: . First this graph has the count of features that have more than 90% missing values only in the rows with more than 50% missing values, so it’s basically a win-win if we dropped these rows as it will automatically fix the problem of these features missing values, and the rest can be imputed. | The majority of PLZ8, Building, Postcode and Community features are completely missing in rows with more than 50% missing values so if we drop these rows we won’t need to impute them. | Around 25% of the features with no known category won’t need imputation if we dropped rows with more than 50% missing values. | Some features from Household, RR4 and RR1 categories will be fixed. | . So now that I was satisfied with the results of this detour, I continued exploring the feature in the remaining categories. . Household Features . ALTER_HH (main age within the household) has missing values encoded as 0. | WOHNDAUER_2008 (length of residence) won’t need imputation as 100% of missing values are present in the rows that we are dropping. | D19_GESAMT_ONLINE_QUOTE_12 and similar features encode whether a person has no transaction in the previous 12 months, but there are some null values of which only 30% are present in the rows we are going to drop. | W_KEIT_KIND_HH (likelihood of a child present in this household (can be specified in child age groups)) has 8% missing values of which 60% are in rows that we are dropping. | Microcell (RR4_ID) Features . CAMEO_DEU_2015 will need one hot encoding and has 11% missing values which should be imputed. | CAMEO_DEUG_2015 is a less detailed CAMEO_DEU_2015 and also has 11% missing values that should be imputed. | KBA05_ANTG1 (number of 1-2 family houses in the cell), KBA05_ANTG2, KBA05_ANTG3 and KBA05_ANTG4 have 15% missing values of which 74% are present in rows we are going to drop. | KBA05_ANHANG (share of trailers in the microcell) has 16.5% of which 67% missing values in rows we are going to drop. | KBA05_ALTER1, KBA05_ALTER2, KBA05_ALTER3 and KBA05_ALTER4 (share of car owners between X and Y years old) has 16.6% of which 67.4% missing values are in rows we are going to drop. | Building Features . KONSUMNAEHE (distance from a building to PoS (Point of Sale)) has 8% missing values that can’t be imputed, and fortunately 99% of these missing values are in the rows we are dropping. | The rest of the features have 100% of it’s missing values in the rows we are dropping, except KBA05_HERSTTEMP (Development of the most common car manufacturers in the neighbourhood) which has 85%. | RR1_ID Features . ONLINE_AFFINITAET (online affinity) has only 0.5% missing values. | GEBAEUDETYP_RASTER (industrial areas) has 10.4% missing values of which 99% exist in the rows we are dropping. | MOBI_REGIO (moving patterns) has 15% missing values of which 74% exist in the rows we are dropping. | KKK (purchasing power) and REGIOTYP (AZ neighbourhood typology) have 17.7% missing values of which 61% exist in the rows we are dropping. | Postcode Features . All features have 10.5% missing values of which 99% exist in the rows that we are dropping. | Community Features . All features have 10.9% missing values of which 95.8% are in rows that we are dropping. | Unknown Features . Since we have no information about the feature description, I looked through them looking for what they mean and tried to find which features are worth keeping and which are not. . I replaced 0 and -1 values in features with null as I did with other features, however in other features knew that these values were actually missing from DIAS Attributes - Values 2017.xlsx , but in this case it was just an assumption based on our knowledge with the data so far. . # find features that don&#39;t have categories no_cat_feats = azdias_new[list(set(azdias_feats).difference(dias_atts_feats))].copy() # change 0 and -1 to null in non-binary features for feat in no_cat_feats.columns: if no_cat_feats[feat].nunique() &gt; 2: no_cat_feats[feat].replace(-1, np.nan, inplace=True) no_cat_feats[feat].replace(0, np.nan, inplace=True) # transpose df so that features are index no_cat_feats = no_cat_feats.T # calculate null_percentage of features no_cat_feats[&quot;null_percentage&quot;] = no_cat_feats.isna().sum(axis=1)/no_cat_feats.shape[1] # sort by null percentage no_cat_feats.sort_values(&quot;null_percentage&quot;, inplace=True) . KOMBIALTER translates to combial, which I have no idea what it means, but it has no missing values. However, it’s values are 1-2-3-4 and then 9, which could mean that 9 is the missing value. | D19_KONSUMTYP_MAX (D19 CONSUMPTION TYPE MAX) and it also has no missing values. | LNR has no missing values, and it looks like an individual ID so it should be dropped. | CJT_TYP_(1-5), CJT_KATALOGNUTZER, RT_SCHNAEPPCHEN (RT Bargain) and RT_KEIN_ANREIZ (RT No Incentive) have 0.5% missing values which is trivial and I think it is going to be removed with dropped rows. | UNGLEICHENN_FLAG (Inequality flag) has 8% missing values, and I’m not sure if this feature has missing values encoded as 0 or not, but either way all of it’s missing values will be dropped with the rows. | EINGEZOGENAM_HH_JAHR (RECOVERED HH YEAR) has 8% missing values which will all be dropped with the rows we are dropping, and it encodes information related to year. | SOHO_KZ has 8% missing values, in addition to a more than 90% 0, and I don’t understand what it means. | AKT_DAT_KL, VK_DHT4A, VK_ZG11 and VK_DISTANZ, have 8% missing values, and I couldn’t find anything related to it’s translation. | RT_UEBERGROESSE (RT OVER-SIZE) has 8% missing values. | EINGEFUEGT_AM (INSERTED_AM) is the timestamp of insertion, but which data exactly? It has 10.5% missing values. | DSL_FLAG and MOBI_RASTER have 10.5% missing values with no translation available. | KONSUMZELLE (Consumer Cell) has 10.5% missing values and only 0-1 values, so I don’t understand what does this feature mean exactly. | FIRMENDICHTE (COMPANY DENSITY) has 10.5% missing values, and I think that all of the featuers so far that have 10.5% missing values come from the same source, which I don’t know exactly what it is so far. | ANZ_STATISTISCHE_HAUSHALTE (ANZ STATISTICAL BUDGETS) has 10.5% missing values. It’s distribution seems to be right skewed. | STRUKTURTYP (STRUCTURE TYPE) has 10.9% missing values. | GEMEINDETYP (COMMUNITY TYPE) has 10.9% missing values, and it has weird values that don’t make sense (not ordinal or nominal). I could further investigate if it has any relation with other community level features. | UMFELD_JUNG (ENVIRONMENT YOUNG) and UMFELD_ALT (ENVIRONMENT OLD) has 10.9% missing values. | CAMEO_INTL_2015 CAMEO is a consumer segmentation system linking address information to demographic, lifestyle and socio-economic insight. We could use KNN imputation to predict the missing values in CAMEO features since they are related to demographic data. | All KB18 features are PLZ8 features. | D19_LETZTER_KAUF_BRANCHE (D19 LAST PURCHASE SECTOR) has 28% missing values, and I think that it has valuable information, so it shouldn’t be dropped with other D19 features. | ALTERSKATEGORIE FEIN (AGE CATEGORY FINE) has 34% missing values (which are 0), and we know that they are missing because category 1 only has one data point, and 0 has 41188. | The rest of the features have more than 50% missing values, so I’m going to drop them except ANZ_KINDER (probably the number of children), ALTER_KINDX (age of Xth child), as I understand their meaning and know that there missing values are not missing at random. | Data Preprocessing . After the previous exploration, I was able to make some final decisions about how to clean the data for the tasks at hand. . Steps . Clean columns with mixed types | Drop columns with more than 50% missing values | Replace encoded unknown values with null from Values sheet and Replace ALTER_HH 0 to null | Replace KOMBIALTER 9 to null | | Features to drop: GEBURTSJAHR (year of birth) has 44% missing values | AGER_TYP (best-ager typology) has 76% missing values | CAMEO_DEU_2015 as it is categorical and needs one-hot-encoding while CAMEO_DEUG_2015 is ordinal and can be better used with PCA. | LNR as it is an individual identifier | All features with more than 50% missing values | | Feature engineering: D19_LETZTER_KAUF_BRANCHE need one hot encoding | MIN_GEBAEUDEJAHR should be changed to number of years between 2017 and date | EINGEFUEGT_AM should be changed to time between 2017 and timestamp | Change ALTER_KINDX and ANZ_KINDER null values to 0 | Convert OST_WEST_KZ to binary labels | Convert CAMEO_INTL_2015 and CAMEO_DEUG_2015 to int | | Impute missing values | Uncertainties . KBA13_ANZAHL_PKW is supposed to encode the number of cars in the PLZ8, but it has high values of peculiar number which are 1400, 1500, 1300, etc. but I’ll keep it. | KONSUMZELLE (Consumer Cell) has 10.5% missing values and only 0-1 values, where 99% of these missing values will be dropped with rows we are dropping first. I don’t understand what does this feature mean exactly. but I’ll keep it. | GEMEINDETYP (COMMUNITY TYPE) has 10.9% missing values, and it has weird values that don’t make sense (not ordinal or nominal), so I’ll drop it. | RT_UEBERGROESSE (RT OVER-SIZE) has small percentage of values encoded as 0, which I don’t know if they are missing or not. 89% of it’s missing values will be dropped with rows, so I’ll keep it. | def clean_dataset(df, p_row=0.5, p_col=0.5, drop_uncertain=True, keep_features=[]): &quot;&quot;&quot; Clean dataset using insights gained during EDA. inputs 1. df (pandas dataframe) 2. p_thresh (float) - maximum threshold of null values in columns 3. drop_uncertain (bool) - drop features we are uncertain from &quot;&quot;&quot; # Make a new copy of the dataframe clean_df = df.copy() # Clean columns with mixed dtypes clean_df[&quot;CAMEO_DEUG_2015&quot;].replace(&#39;X&#39;, np.nan, inplace=True) clean_df[&quot;CAMEO_INTL_2015&quot;].replace(&#39;XX&#39;, np.nan, inplace=True) # Replace unknown values with missing clean_df = replace_unknown_with_null(clean_df) # Drop rows with more than 50% missing values min_count = int(((1 - p_row))*clean_df.shape[1] + 1) clean_df.dropna(axis=0, thresh=min_count, inplace=True) # Drop duplicated rows clean_df.drop_duplicates(inplace=True) # Drop GEBURTSJAHR (year of birth) that has 44% missing values clean_df.drop(&#39;GEBURTSJAHR&#39;, axis=1, inplace=True) # Drop LNR which is a unique indentifier clean_df.drop(&#39;LNR&#39;, axis=1, inplace=True) # Drop CAMEO_DEU_2015 as it&#39;s not suitable for PCA clean_df.drop(&#39;CAMEO_DEU_2015&#39;, axis=1, inplace=True) # Drop features with more than p_thresh missing values features_missing_p = clean_df.isna().sum() / clean_df.shape[0] features_above_p_thresh = clean_df.columns[features_missing_p &gt; p_col] features_to_keep = [&quot;ALTER_KIND1&quot;, &quot;ALTER_KIND2&quot;, &quot;ALTER_KIND3&quot;, &quot;ALTER_KIND4&quot;, &quot;ANZ_KINDER&quot;] + keep_features features_to_remove = [feat for feat in features_above_p_thresh if feat not in features_to_keep] clean_df.drop(features_to_remove, axis=1, inplace=True) # Drop uncertain features if drop_uncertain: uncertain_features = [&quot;GEMEINDETYP&quot;] clean_df.drop(uncertain_features, axis=1, inplace=True) # Feature Engineering # One Hot Encoding D19_LETZTER_KAUF_BRANCHE dummies = pd.get_dummies(clean_df[&quot;D19_LETZTER_KAUF_BRANCHE&quot;], prefix=&quot;D19_LETZTER_KAUF_BRANCHE&quot;) clean_df = pd.concat([clean_df, dummies], axis=1) clean_df.drop(&quot;D19_LETZTER_KAUF_BRANCHE&quot;, axis=1, inplace=True) # Calculate year difference in MIN_GEBAEUDEJAHR clean_df[&quot;MIN_GEBAEUDEJAHR_ENG&quot;] = (2017 - clean_df[&quot;MIN_GEBAEUDEJAHR&quot;]) clean_df.drop(&quot;MIN_GEBAEUDEJAHR&quot;, axis=1, inplace=True) # Calculate days difference in EINGEFUEGT_AM current = datetime.strptime(&quot;2017-01-01&quot;, &quot;%Y-%m-%d&quot;) clean_df[&quot;EINGEFUEGT_AM_DAY&quot;] = (current - pd.to_datetime(clean_df[&quot;EINGEFUEGT_AM&quot;])).dt.days clean_df.drop(&quot;EINGEFUEGT_AM&quot;, axis=1, inplace=True) # Replace null values in ALTER_KIND and ANZ_KINDER with 0 to avoid imputation for feat in clean_df.columns[clean_df.columns.str.startswith(&quot;ALTER_KIND&quot;)]: clean_df[feat].replace(np.nan, 0, inplace=True) clean_df[&quot;ANZ_KINDER&quot;].replace(np.nan, 0, inplace=True) # Convert OST_WEST_KZ to binary labels clean_df[&quot;OST_WEST_KZ&quot;] = (clean_df[&quot;OST_WEST_KZ&quot;] == &quot;W&quot;).astype(np.uint8) # Convert CAMEO_INTL_2015 and CAMEO_DEUG_2015 to float32 CAMEO_feats = [&quot;CAMEO_INTL_2015&quot;, &quot;CAMEO_DEUG_2015&quot;] clean_df[CAMEO_feats] = clean_df[CAMEO_feats].astype(np.float32) # Convert float16 features to float32 to enable arithmetic operations float_feats = clean_df.select_dtypes(np.float16).columns clean_df[float_feats] = clean_df[float_feats].astype(np.float32) return clean_df . The last step remaining was imputing the data after this cleaning pipeline, so I checked the percentage of missing values in features after it. . . We can see now that imputations won’t be a big deal, and that there is very small number of features that exceed 10% missing values. . But how shall we impute that data? There are multiple ways we could that including: . Imputing missing values with 0 or -1 which was used originally in the dataset | Imputing missing values with mean, median or mode and adding and indicator (or not) for missing data | KNN imputation | Iterative imputation using linear regression | My gut feeling says that I should go with KNN imputation, as I think that features of the same category could be of huge aid in determining which value to impute with, instead of imputing with 0 or mode (since we are dealing with ordinal data). . But KNN imputation will be time and memory intensive, so we can’t do on all features. So we probably should stick with an easy strategy like mode imputation for features below a threshold and KNN for features above. . But what that shall that threshold be? . Since KNN is memory and time intensive, we need the majority of features to be imputed using mode values, and only small number of features shall be imputed using KNN. . . Using this box plot we can see that we can only impute features above 10% using KNN. So in order to do that we need to first impute features lower than 10% using mode imputation, then impute the rest of the features using KNN. . After several trials, it seemed that KNN takes a really long time. . So instead, I imputed the remaining 10 features using mean imputation to avoid introducing any spike in the data since we are dealing with features that might have more than 20% missing values. . I have used sklearn’s ColumnTransformer to impute each set of features using their specified method. . # Features below 10% null percentage features_below_10 = na_features[na_features &lt; 0.1].index # Features above 10% null percentage features_above_10 = na_features[na_features &gt; 0.1].index imputer = ColumnTransformer([ (&#39;mode&#39;, SimpleImputer(strategy=&quot;most_frequent&quot;), features_below_10), (&#39;mean&#39;, SimpleImputer(strategy=&quot;mean&quot;), features_above_10) ], remainder=&quot;passthrough&quot;) imputed_azdias = imputer.fit_transform(clean_azdias).astype(np.float32) . After finally imputing AZDIAS we were ready to go through with the first task which customer segmentation. . Customer Segmentation . As I mentioned in the beginning, in this part we are supposed to find the part of the general population that represents the core customer base of the business. . In order to do that we need to the following: . Cluster the general population dataset (AZDIAS) | Predict the clusters of our customers dataset (CUSTOMERS) | Find the clusters that are over represented in CUSTOMERS | Analyze these cluster in AZDIAS | And in order to cluster either of the datasets we need to reduce the dimensions of the datasets using PCA, as it could improve the results of K-Means clustering algorithm. . This was done by first training the PCA on AZDIAS, and then reduce CUSTOMERS using the trained algorithm. . The data needs to be scaled for PCA to obtain good results, so I decided to use sklearn’s StandardScaler. However, since the data was huge, I decided to incrementally fit both the StandardScaler and PCA, as fitting them normally took a long time. . def batch_fit_scaler(scaler, data, n_batches=100): &quot;&quot;&quot; Fit a scaler to data through n batches. Input: transfromer (scikit-learn Scaler object) data (numpy array or pandas dataframe) n_batches (int) - number of batches for fitting the scaler Output: Fitted Scaler &quot;&quot;&quot; for X_batch in tqdm(np.array_split(data, n_batches), total=n_batches): scaler.partial_fit(X_batch) return scaler def batch_fit_pca(pca, scaler, data, n_batches=100): &quot;&quot;&quot; Fit an Incremental PCA to scaled data through n batches. Input: pca (scikit-learn IncrementalPCA object) scaler (scikit-learn Scaler object) data (numpy array or pandas dataframe) n_batches (int) - number of batches for fitting the transformer Output: Fitted IncrementalPCA &quot;&quot;&quot; for X_batch in tqdm(np.array_split(data, n_batches), total=n_batches): scaled_X_batch = scaler.transform(X_batch) pca.partial_fit(scaled_X_batch) return pca def batch_transform_pca(pca, scaler, data, n_batches=100): &quot;&quot;&quot; Transform large data using fitted pca. Input: pca (Fitted IncrementalPCA) scaler (Fitted Scaler) data (numpy array or pandas dataframe) n_batches (int) - number of batches for fitting the transformer Output: Transformed data &quot;&quot;&quot; pca_data = None for X_batch in tqdm(np.array_split(data, n_batches), total=n_batches): scaled_X_batch = scaler.transform(X_batch) pca_X_batch = pca.transform(scaled_X_batch) if pca_data is None: pca_data = pca_X_batch else: pca_data = np.vstack([pca_data, pca_X_batch]) return pca_data scaler = batch_fit_scaler(scaler, imputed_azdias) pca = batch_fit_pca(pca, scaler, imputed_azdias) cumsum = np.cumsum(pca.explained_variance_ratio_) d = np.argmax(cumsum &gt;= 0.95) + 1 print(&quot;Minimum number of dimensions that have 95% of original data variance:&quot;, d) . Minimum number of dimensions that have 95% of original data variance: 224 . After fitting them, I used the cumulative explained variance of each principal component to find the number of them which explains 95% of variability in the dataset, and I found that to be 224. . Then I proceeded with clustering using sklearn’s MiniBatchKMeans, and I tested several values for the number of clusters to use and compared the results of their Inertia (Average distance between each point and it’s cluster center). . . As we can see, the more clusters we set, the better the score we get. But this keeps going forever, so we need to find the elbow point, and I think that is 7. . k = 7 init = &quot;k-means++&quot; n_init = 10 n_batches = 100 batch_size = pca_azdias.shape[0]//n_batches seed = 42 kmeans = MiniBatchKMeans(n_clusters=k, init=init, n_init=n_init, batch_size=batch_size, random_state=seed) kmeans.fit(pca_azdias) # Calculate percentage of each cluster in azdias azdias_clusters_p = pd.Series(kmeans.labels_).value_counts()/len(kmeans.labels_) . After fitting, it was time to do the same, but for CUSTOMERS . After that, we shall compare each cluster representation in both populations to finally find out the core customer base of the business. . # Clean the customers dataset using the pipeline we made earlier clean_customers = clean_dataset(customers, keep_features=[&quot;KBA13_ANTG4&quot;]) # Re-arrange columns to be just as clean_azdias to avoid any problems with imputer clean_customers = clean_customers[clean_azdias.columns] # Impute missing values using imputer fitted on clean_azdias imputed_customers = imputer.transform(clean_customers) # transform using scaler scaled_customers = scaler.transform(imputed_customers) # transform using pca pca_customers = pca.transform(scaled_customers)[:, :d] # Predict KMeans labels customers_clusters = kmeans.predict(pca_customers) # Calculate percentage of each cluster in customers customers_clusters_p = pd.Series(customers_clusters).value_counts()/len(customers_clusters) . . We can see an over representation of cluster 0 in CUSTOMERS when compared to AZDIAS, with over 40% of CUSTOMERS being in this cluster. | Clusters 4 and 6 percentages in CUSTOMERS also exceed their counterparts in AZDIAS. | The most rare cluster in CUSTOMERS is cluster 5, followed by 3, 2 and 1. | Therefore, the clusters that are more generally inclined to be customers are 0, 4 and 6. | While the clusters that are less inclined to be customers are 1, 2, 3, and 5. | We have a lot features, so instead of looking into the difference between customers and non-customers in all of them, I skimmed the difference between cluster 0 (the most frequent customer cluster) and cluster 5 (the most frequent non-customer cluster) in all of the features to find the ones which we can use to understand the difference between the whole customer base and the rest of the population. . def compare_features(dfs=[], labels=[]): &quot;&quot;&quot;Plots all features of passes dataframes for comparison&quot;&quot;&quot; # parameters of subplot nrows = 1 ncols = 2 # number of features nfeats = feat_cat_df.shape[0] # colors colors = &quot;bgrcmy&quot; # loop over 4 features at a time to plot them in a row for i in range(0, nfeats, ncols): # make subplots fig, axes = plt.subplots(nrows=nrows, ncols=ncols, figsize=(16, 3)); # loop over each of 4 features for j, feat_idx in enumerate(range(i, i+ncols)): # feature name and category feat = feat_cat_df.iloc[feat_idx].feature cat = feat_cat_df.iloc[feat_idx].category # ldict with label as key and df feature as value dfs_feat = {} for df, label in zip(dfs, labels): dfs_feat[label] = df.loc[:, feat] # plot using histogram if unique values exceeds 11 if dfs_feat[label].nunique() &gt; 16: for (label, df_feat), color in zip(dfs_feat.items(), colors): df_feat.hist(bins=20, color=color, density=True, alpha=0.4, ax=axes[j], label=label) axes[j].legend() # plot using bar chart else: columns = [] feats_p = None # concatenate all df features values counts in one dataframe and plot bar plot for label, df_feat in dfs_feat.items(): columns.append(label) feat_p = df_feat.value_counts()/df_feat.shape[0] feats_p = pd.concat([feats_p, feat_p], axis=1) feats_p.columns = columns feats_p.sort_index().plot(kind=&quot;bar&quot;, ax=axes[j]) # assign unknown feature description feat_desc = &#39;Unknown&#39; # if feature description exists include instead of unknown if feat in known_feats: feat_desc = dias_atts[dias_atts[&quot;Attribute&quot;] == feat][&quot;Description&quot;].item() # set title as feature, category and description axes[j].set_title(f&quot;{feat} n{cat} n{feat_desc}&quot;) # Filtering individuals from cluster 0 in AZDIAS cluster_0 = clean_azdias[kmeans.labels_ == 0] # Filtering individuals from cluster 5 in AZDIAS cluster_5 = clean_azdias[kmeans.labels_ == 5] compare_features([cluster_0, cluster_5], [&quot;Cluster 0&quot;, &quot;Cluster 5&quot;]) . Through skimming over the plots of all of the features in these two clusters, I selected the following features to explore the difference between the full customer base and the rest of the population. . ALTERSKATEGORIE_GROB (Age through prename analysis) | ANREDE_KZ (Gender) | CJT_GESMATTYP (Preferred information and buying channels) | FINANZTYP (Financial type) | LP_LEBENSPHASE_FEIN (Lifestage) | RETOURTYP_BK_S (Return type) | ALTER_HH (Main age within household) | HH_EINKOMMEN_SCORE (Estimated household net income) | WOHNLAGE (Neighbourhood area) | MOBI_REGIO (Moving patterns) | There are more features that also emphasize differences between customers and non-customers, however I found that they offer redundant information. . There are also some features that shows no difference between the two groups, specifically features that are related to motor vehicles information in PLZ8 areas or microcells. Which indicated that we might want to use some sort of feature selection in the final machine learning pipeline. . My intuition is that we should visualize the differences between cluster 0-4-6 and clusters 2-3-5 leaving out cluster 1, as individuals in this cluster have really similar tendencies of being customers or non-customers. So that would decrease the sharpness of differences between the two groups. . I shall also visualize the difference between the clusters of the customer base in order to further understand them. . customers = clean_azdias[np.in1d(kmeans.labels_, [0, 4, 6])] cluster_0 = clean_azdias[kmeans.labels_ == 0] cluster_4 = clean_azdias[kmeans.labels_ == 4] cluster_6 = clean_azdias[kmeans.labels_ == 6] non_customers = clean_azdias[np.in1d(kmeans.labels_, [2, 3, 5])] cluster_2 = clean_azdias[kmeans.labels_ == 2] cluster_3 = clean_azdias[kmeans.labels_ == 3] cluster_5 = clean_azdias[kmeans.labels_ == 5] . Age . . Customers and Non-Customers . We can see that customers have greater probability of being older, with almost 80% being above 45 years old. On the other hand non-customers tend have more than 50% of less than 46 years old. The age groups that is mostly shared between the two groups is 46-460 years group. . Customer Clusters . We can see that cluster 4 stand out with higher percentage of indiviudals less than 46 years old, while cluster 0 has more than 90% of it’s population above 46 years old. . So cluster 0 includes is mostly elders with the majority being above 60 years old, while cluster 4 has the majority above 45 but also has higher than average percentage of younger indivduals, and cluster 6 is similar to cluster 0 except that the percentage of 46-60 years indivduals is larger than cluster 0. . Gender . . Customers and Non-Customers . The percentage of males in customers is higher than that in non-customers, while the percentage of females in both is higher than males. . Customer Clusters . Cluster 0 has an over representation of males, where males is higher than all clusters and higher than female percentage in the same cluster. While cluster 4 and 6 have higher female percentages than cluster 0. . Preferred Information and Buying Channels . . Customer and Non-Customers . We can see than customers exceed non-customers in percentages of advertising and consumption minamilists and traditionalists, while non-customers tend to be more open in that spectrum. . Customer Clusters . Since cluster 0 mostly represents elderly individuals, it’s expected that they will be over represented in the minimalists and traditionlists. And also since cluster 4 represents the younger customers, we don’t see alot of them as minimalist and traditionalists. And finally we can see that cluster 6 has the most uniform distribtution across the spectrum. . Financial Type . . Customers and Non-Customers . 20% of customers are money savers, while another 20% are inverstors, and around 35% are unremarkable which I guess means that they have no specific financial type. On the other hand, non-customers tend to have low financial interest. . Customer Clusters . We can that the majority of cluster 0 with distinguished financial type are money savers, while in cluster 6 they are investors. Cluster 4 doesn’t show a specific type. . Life Stage . . Customers and Non-Customers . The most frequent non-customer type is single low-income and average earners of younger age, while customers’ most frequent type is singe high-income earner. However, there is no great difference between the most frequent value of customers and two next most frequent values, indicating the difference between clusters. . Customer Clusters . Around 70% of cluster 6 are single, with the majority of them being single low-income average earners of higher age., while the most frequent type in cluster 0 is single high-income earners, while cluster 4’s most frequent type is high income earner of higher age from multiperson households. However, the remaining majority of cluster 4 types falls in younger aged families with different types of income. . Return Type . . Customers and Non-Customers . The most frequent type in customers is determined minimal returner, which I think means that these individuals aren’t the shopping type. They only buy what they need when they need it. The second frequent type in incentive receptive normal returner. While in non-customers, we can see that the most frequent type is influencable crazy shopper, and these wouldn’t definetly be interested in mail-order cataloges. . Customers Clusters . First off we can see the cluster 0 and 6 are the only populating most of the customers belonging to the determined minimal returner category, and that makes sense since they are older individuals, and we have found that they are consumption minimalists and traditionalists. On the other hand, cluster 4 populates every other category with frequency higher than the determined minmal returner one, with them most frequent being demanding heavy returner. . Main Age within Household . . Customers and Non-Customers . We have already investigated the age difference between customers and non-customers, and we can see that the main age within the household is also different between the two groups, where customers households tend be also older in age, while non-customers households tend to be younger. . Customer Clusters . We can see that cluster 4 is the main cluster populating younger ages in customers clusters, while cluster 0 and 6 have nearly identical distributions representing the elderly segments of the customers. . Estimated Net Household Income . . Customers and Non-Customers . We can see a huge difference between the distribution of customers and non-customers among estimated net household income, where more than 50% of non-customers come from very low income households, and only around 15% of customers do. The most frequent in customers is average income, and the second most is lower income. However, the total percentage of customers whose income is average or above exceeds 50%. . Customers Clusters . Now we can see a difference between the two older segments, which are cluster 0 and 6. We can see that over 60% of cluster 6 households have either lower or very low income, while more than 70% of cluster 0 has average or higher income. Similarily cluster 4 also has around 70% of it’s households having average or higher income. . Does this mean that cluster 6 is poorer than cluster 0? . Will that would be the case if this feature indicated the income of the individual, however since this feature indicates the net household income, this doesn’t say anything about the specific individuals in cluster 0 and 6. Since cluster 6 had more tendency to be single, it makes sense that cluster 0 household income would be higher, because if cluster 0 is financially above average, it’s safe to say that probably the rest of their family is the same, and that would make their net income larger than the same individual if he was single, and that’s the situation for cluster 6. . Neighborhood Area . . Customers and Non-Customers . The most frequent neighborhood area in both customers and non-customers is average neighborhoods, however the next most frequent for customers is rural neighborhoods, while that of non-customers is poor neighboorhoods. We can also see that the percentage of customers occupying above average neighborhood areas is larger than non-customers’. . Customers Clusters . We can see that our remark about the household income difference between cluster 0 and 6 has been useful, because cluster 6 to have the highest percentage occupying average and above neighborhood areas, while the most frequent neighborhood area for cluster 0 is rural areas, since they are mostly families. Cluster 4 is extremely similar to cluster 0 in this attribute. . Moving Patterns . . Customers and Non-Customers . 50% of customers are classified as having low or very mobility, while more than 60% of non-customers are the extreme opposite, with classification of either high or very high mobility. . Customers Clusters . Once again we can see some of the differing factors between cluster 0 and 6, since cluster 6 are mostly single individuals, more than 60% of their moving pattern is high or very high and 25% have middle mobility. On the other hand, since cluster 0 and 4 tend to in families, their mobility is much lower than cluster 6, with almost 75% of cluster 0 having low or very low mobility, and 65% of cluster 4 having the same. . Now that I solidly understood the customer base through the previous analysis, I proceed to the next part. . Mailout Campaign . As I previously said, in this part we are supposed to make a supervised learning model based on campaign data provided in Udacity_MAILOUT_052018_TRAIN.csv which hold demographic data for individuals targeted in marketing campaigns paired with whether they have became customers or not. . Since I already had a cleaning pipeline for the previous datasets, all I needed to do was to check whether this dataset conforms to what we already know in terms of null percentages in features. . # Read mailout training data mailout_train = pd.read_csv(&#39;../../data/Term2/capstone/arvato_data/Udacity_MAILOUT_052018_TRAIN.csv&#39;, sep=&#39;;&#39;) # Replace unknown values with null mailout_train_new = replace_unknown_with_null(mailout_train) # Check null percentages in columns and rows fig = plt.figure(figsize=(14, 4)) plt.subplot(1, 2, 1) plt.title(&quot;Null Percentages in Columns&quot;) mailout_null_columns = get_null_prop(mailout_train_new, axis=0, plot=True) plt.subplot(1, 2, 2) plt.title(&quot;Null Percentages in Rows&quot;) mailout_null_rows = get_null_prop(mailout_train_new, axis=1, plot=True) . . The null percentages somehow conform to the what we saw in AZDIAS and CUSTOMERS , so I proceeded with cleaning them using the cleaning function, then checking whether they have features that weren’t dropped compared to clean_azdias . . # Test cleaning the dataset using cleaning function mailout_train_clean = clean_dataset(mailout_train_new) print(&quot;Shape before cleaning:&quot;, mailout_train.shape) print(&quot;Shape after cleaning:&quot;, mailout_train_clean.shape) # Check if feature set is the same as clean AZDIAS set(mailout_train_clean.columns) == set(clean_azdias.columns) . Shape before cleaning: (42962, 367) Shape after cleaning: (35093, 361) False . # Check for difference in features in both datasets print(&quot;Features in clean MAILOUT not in clean AZDIAS:&quot;, set(mailout_train_clean.columns).difference(clean_azdias.columns)) print(&quot;Features in clean AZDIAS not in clean MAILOUT:&quot;, set(clean_azdias.columns).difference(mailout_train_clean.columns)) . Features in clean MAILOUT not in clean AZDIAS: {&#39;AGER_TYP&#39;, &#39;D19_SONSTIGE&#39;, &#39;D19_VOLLSORTIMENT&#39;, &#39;EXTSEL992&#39;, &#39;VHA&#39;, &#39;RESPONSE&#39;, &#39;D19_BUCH_CD&#39;, &#39;D19_SOZIALES&#39;} Features in clean AZDIAS not in clean MAILOUT: {&#39;KBA13_ANTG4&#39;} . So there are some features that we have in MAILOUT that weren’t dropped, so if we opted to use these there are some points that we need to take care of: . They aren’t included in imputation pipeline | They aren’t included in scaling pipeline | They aren’t included in PCA transformer | They aren’t included in clustering algorithm | So they could be concatenated with the results of the original pipeline and cleaned separately if I wanted to use them, or we could just drop them. . Since at that moment, my main goal was to make a baseline to build upon, I didn’t to use the previous customer segmentation pipeline at all, and all I need was clean data to pass to a supervised machine learning algorithm. So I left them as they are. . I added the feature KBA13_ANTG4 that was dropped during cleaning so that we are able to use the customer segmentation pipeline if we needed. . mailout_train_clean = clean_dataset(mailout_train_new, keep_features=[&quot;KBA13_ANTG4&quot;]) . After that I need to see these features that we didn’t include in the customer segmentation, and I also needed to look at the RESPONSE target feature to see how it is distributed. . new_feats = [&#39;D19_BUCH_CD&#39;, &#39;VHA&#39;, &#39;EXTSEL992&#39;, &#39;D19_SOZIALES&#39;, &#39;D19_VOLLSORTIMENT&#39;, &#39;RESPONSE&#39;, &#39;AGER_TYP&#39;, &#39;D19_SONSTIGE&#39;] mailout_train_clean[new_feats].hist(figsize=(14, 14)); . . All of the features shall be imputed using mean values since all of their missing percentages exceed 10%. . D19_BUCH_CD 0.427863 VHA 0.447326 EXTSEL992 0.247884 D19_SOZIALES 0.282592 D19_VOLLSORTIMENT 0.378623 RESPONSE 0.000000 AGER_TYP 0.305047 D19_SONSTIGE 0.228963 dtype: float64 . However, I have noticed that RESPONSE is extremely imbalanced, which means that results of the regular baseline will probably be really bad, and we’ll have to use certain techniques to handle that situation. . # Impute clean AZDIAS features using old imputer mailout_train_clean.loc[:, clean_azdias.columns] = imputer.transform(mailout_train_clean[clean_azdias.columns]) # Ad-hoc impute EXTSEL992 mailout_train_clean[&quot;EXTSEL992&quot;].fillna(value=mailout_train_clean[&quot;EXTSEL992&quot;].mean(), inplace=True) # Make new imputer for remaining features mailout_imputer = SimpleImputer(strategy=&quot;most_frequent&quot;) # Fit imputer and impute the remaining columns in place mailout_train_clean.loc[:, :] = mailout_imputer.fit_transform(mailout_train_clean) # Split training data into X and y X = mailout_train_clean.loc[:, mailout_train_clean.columns != &quot;RESPONSE&quot;] y = mailout_train_clean.loc[:, &quot;RESPONSE&quot;] . Now that we have a clean dataset it’s time to train a model . The model that I have in mind is RandomForestClassifier. . In order to evaluate this baseline, we need to have some sort of validation set in order to score our results. . For validation I’ll use Stratified KFold cross validation (in order to account for RESPONSE imbalance), and I’ll use sklearn’s classfication report which shows recall, precision and f1-score for each label. . I’ll also use ROC AUC score since it’s the score of the final competition. . The advantage of classification report is that it shows us the whole picutre, so we don’t get decieved if the model is performing poorly on the under-represented class, and performing better on the over-represented one. . def model_validation(model, X, y, fold_results=False, final_results=True, metric=False): &quot;&quot;&quot;Evaluate model using sklearn&#39;s classification report.&quot;&quot;&quot; # Instantiate StratifiedKFold skf = StratifiedKFold(n_splits=3, shuffle=True, random_state=seed) # Make empty y_pred to fill predictions of each fold y_pred = np.zeros(y.shape) # Initialize empty list for scores scores = [] for i, (train_idx, test_idx) in enumerate(skf.split(X, y)): X_train, X_test = X.iloc[train_idx], X.iloc[test_idx] y_train, y_test = y.iloc[train_idx], y.iloc[test_idx] # Fit model model.fit(X_train, y_train) # Predict fold y_test and add in y_pred fold_pred = model.predict(X_test) y_pred[test_idx] += fold_pred # Print classification report if fold_results: print(&quot;Fold:&quot;, i+1) print(classification_report(y_test, fold_pred)) # Calculate fold macro f1 scores scores = metric(y_test, fold_pred) # Print final classification report if final_results: print(&quot;Final Report:&quot;) print(classification_report(y, y_pred)) # Return metric scores if passed to the function if metric != None: return scores . from imblearn.ensemble import BalancedRandomForestClassifier model_validation(BalancedRandomForestClassifier(n_estimators=1000, random_state=seed), X, y) . Final Report: precision recall f1-score support 0.0 1.00 0.69 0.81 34658 1.0 0.03 0.73 0.06 436 accuracy 0.69 35094 macro avg 0.51 0.71 0.44 35094 weighted avg 0.98 0.69 0.81 35094 Metric Score: 0.7085381814866175 . We can see that this model still doesn’t have amazing performance, but it’s significantly better than the regular RandomForestClassifier. . Final Report: precision recall f1-score support 0.0 0.99 1.00 0.99 34658 1.0 0.00 0.00 0.00 436 accuracy 0.99 35094 macro avg 0.49 0.50 0.50 35094 weighted avg 0.98 0.99 0.98 35094 Metric Score: 0.5 . We can see that if we use the weighted average of any of the 3 metric used (precision, recall or f1-score) that the RandomForestClassifier wins, however, if we look into the metrics for the responsive class, we can see that the classifier completely missed them, and due to their lower percentage the classifier decided that the best strategy is to predict that all of the data points were non-responsive. . So as we can see, the best thing about classification_report is that it gives us a variety of metrics that we can use to judge our model. . Baseline Results . We can see that this model still has bad performance, but it’s siginifcantly better than the regular RandomForestClassifier. . The best thing about classification_report is that it gives us a variety of metrics that we can use to judge our model. . Therefore, we can see the results are: . Recall (Macro): 0.71 | Precision (Macro): 0.51 | F1-Score (Macro): 0.44 | The first imporvement that we can make is to utilize the dimensionality reduction that we have made earlier. . Select only features in clean_azdias | Scale the features and reduce dimensions using PCA | Predict using PCA | Predicting using PCA reduced dataset . # Filter for features used in clean_azdias mailout_train_pre_pca = mailout_train_clean[clean_azdias.columns] # Scale the features mailout_train_pre_pca = scaler.transform(mailout_train_pre_pca) # Reduce dimensions using pca to d dimensions X = pd.DataFrame(pca.transform(mailout_train_pre_pca)[:, :d]) model_validation(BalancedRandomForestClassifier(random_state=seed)) . Final Report: precision recall f1-score support 0.0 0.99 0.59 0.74 34658 1.0 0.02 0.58 0.03 436 accuracy 0.59 35094 macro avg 0.50 0.58 0.39 35094 weighted avg 0.98 0.59 0.73 35094 Metric Score: 0.5825897065477098 . The results of PCA transformed training set with BalancedRandomForestClassifier is worse than using the vanilla dataset. . Could these results be improved with other algorithms? . We could spot-check multiple algorithms to prove if this is the case. . Instead of implementing under-sampling or over-sampling in validation, we can use BalancedBaggingClassifier to wrap the classfication algortithms that we are interested in using. But we’ll need to scale the data because algorithms like Logistic Regression, KNN and SVM can perform better after scaling the data. . Note that when comparing different models result, I’ll not increase . n_estimators to save time, as the results are just for comparison. . def evaluate_models(models_dict, X, y, scoring=&quot;recallWe can see that these features have definetly improved upon our PCA results, and even made it better than the original dataset score. Right now what if we add the PCA features to all features?_macro&quot;): &quot;&quot;&quot;Evaluate several models using sklearn&#39;s cross_val_score.&quot;&quot;&quot; model_scores = {} for name, model in models_dict.items(): skf = StratifiedKFold(n_splits=3, shuffle=True, random_state=seed) scores = cross_val_score(model, X, y, cv=skf, scoring=scoring) model_scores[name] = scores print(&quot;Model:%s, Score:%.3f (+/- %.3f)&quot; % (name, np.mean(scores), np.std(scores))) return model_scores . Then I selected the models we were going to use and passed them into the function to see how they fare against RandomForests. . from sklearn.model_selection import cross_val_score from imblearn.ensemble import BalancedBaggingClassifier from sklearn.linear_model import LogisticRegression from sklearn.neighbors import KNeighborsClassifier from sklearn.svm import SVC from sklearn.pipeline import Pipeline from sklearn.preprocessing import StandardScaler bagging_model = lambda model: BalancedBaggingClassifier(model) models = {&quot;RF&quot;: BalancedRandomForestClassifier(random_state=seed), &quot;LR&quot;: bagging_model(LogisticRegression(max_iter=1000, random_state=seed)), &quot;KNN&quot;: bagging_model(KNeighborsClassifier()), &quot;Linear SVM&quot;: bagging_model(SVC(kernel=&quot;linear&quot;, random_state=seed)), &quot;RBF SVM&quot;: bagging_model(SVC(kernel=&quot;rbf&quot;, random_state=seed))} pca_bagging_scores = evaluate_models(models, X, y, &quot;recall_macro&quot;) . Model:RF, Score:0.549 (+/- 0.026) Model:LR, Score:0.558 (+/- 0.006) Model:KNN, Score:0.539 (+/- 0.023) Model:Linear SVM, Score:0.536 (+/- 0.009) Model:RBF SVM, Score:0.571 (+/- 0.016) . We can see that Bagging using LogisitcRegression and RBF SVM exceeded the recall of BalancedRandomForestClassifier. However we have to note that we didn’t increase the number of estimators, and we still don’t know the exact recall of the responsive class. . What if we trained them on the original data? . But we’ll need to scale the data because algorithms like Logistic Regression, KNN and SVM can perform better after scaling the data. . # Split training data into X and y X = mailout_train_clean.loc[:, mailout_train_clean.columns != &quot;RESPONSE&quot;] y = mailout_train_clean.loc[:, &quot;RESPONSE&quot;] scaled_pipeline = lambda model: make_pipeline(StandardScaler(), model) models = {&quot;RF&quot;: BalancedRandomForestClassifier(random_state=seed), &quot;LR&quot;: scaled_pipeline(bagging_model(LogisticRegression(max_iter=1000, random_state=seed))), &quot;KNN&quot;: scaled_pipeline(bagging_model(KNeighborsClassifier())), &quot;Linear SVM&quot;: scaled_pipeline(bagging_model(SVC(kernel=&quot;linear&quot;, random_state=seed))), &quot;RBF SVM&quot;: scaled_pipeline(bagging_model(SVC(kernel=&quot;rbf&quot;, random_state=seed)))} original_bagging_scores = evaluate_models(models, X, y, &quot;recall_macro&quot;) . Model:RF, Score:0.673 (+/- 0.013) Model:LR, Score:0.585 (+/- 0.016) Model:KNN, Score:0.533 (+/- 0.028) Model:Linear SVM, Score:0.576 (+/- 0.013) Model:RBF SVM, Score:0.573 (+/- 0.006) . By comparing the results, we can see that all of the algorithms (except Bagged KNN) perform better using the original data. However the improvement in all of them compared to BalancedRandomForestClassifier is tiny, so we need not continue in exploring them. . In terms of the significant improvement in BalancedRandomForestClassifier, this could be for two reasons: . The features that aren’t included in the PCA could add important information to the model | The PCA transformed data isn’t particularly good for the task that we need | We can test this by adding all of the features that were left our from PCA transformation to the transformed data to test the performance of the . algorithm again. . # Make new dataframe with only new feats mailout_train_new_feats = mailout_train_clean.loc[:, new_feats] # Concatenate PCA transformed dataframe with new feats dataframe X = pd.concat([pd.DataFrame(pca.transform(mailout_train_pre_pca)[:, :d]), mailout_train_new_feats.reset_index(drop=True)], axis=1) # Drop RESPONSE X = X.drop(columns=[&quot;RESPONSE&quot;]) model_validation(BalancedRandomForestClassifier(n_estimators=1000, random_state=seed), X, y) . Final Report: precision recall f1-score support 0.0 1.00 0.70 0.82 34658 1.0 0.03 0.74 0.06 436 accuracy 0.70 35094 macro avg 0.51 0.72 0.44 35094 weighted avg 0.98 0.70 0.81 35094 Metric Score: 0.7209044081958259 . We can see that these features have definetly improved upon our PCA results, and even made it better than the original dataset score. . Right now what if we add the PCA features to all features? . # Concatenate PCA transformed dataframe with new feats dataframe X = pd.concat([pd.DataFrame(pca.transform(mailout_train_pre_pca)[:, :d]), mailout_train_clean.reset_index(drop=True)], axis=1) # Drop RESPONSE X = X.drop(columns=[&quot;RESPONSE&quot;]) model_validation(BalancedRandomForestClassifier(n_estimators=1000, random_state=seed), X, y) . Final Report: precision recall f1-score support 0.0 0.99 0.67 0.80 34658 1.0 0.03 0.72 0.05 436 accuracy 0.67 35094 macro avg 0.51 0.70 0.43 35094 weighted avg 0.98 0.67 0.79 35094 Metric Score: 0.6977696440858011 . We can see that this has worsened the results. It seems that the PCA transformer features are able to provide info that is better than the original features before transformation, and the presence of both at the same time doesn’t improve the model performance at all. . What if we add the clusters distances as features to PCA features + new features combo? . # transform mailout train data using PCA mailout_train_pca = pd.DataFrame(pca.transform(mailout_train_pre_pca)[:, :d], columns=[f&quot;pca_{i}&quot; for i in range(d)]) # predict mailout cluster distances mailout_distances = pd.DataFrame(kmeans.transform(mailout_train_pca), columns=[f&quot;cluster_{i}&quot; for i in range(kmeans.cluster_centers_.shape[0])]) # predict mailout cluster mailout_clusters = pd.Series(kmeans.predict(mailout_train_pca), name=&#39;label&#39;) # visualize mailout clusters mailout_clusters.value_counts().plot(kind=&quot;bar&quot;, title=&quot;How Are Clusters Distributed in MAILOUT Data?&quot;); . . If we were to use these clusters directly, we would predict that the majority of the individuals in the MAILOUT data would respond, but we know from the labels than only a very small portion of them actually responded. . # concatenate cluster labels to the clean dataset X = pd.concat([mailout_train_pca, mailout_train_new_feats.reset_index(drop=True), mailout_distances], axis=1) # drop RESPONSE X = X.drop(columns=[&quot;RESPONSE&quot;]) model_validation(BalancedRandomForestClassifier(n_estimators=1000, random_state=seed), X, y) . Final Report: precision recall f1-score support 0.0 0.99 0.69 0.81 34658 1.0 0.03 0.72 0.05 436 accuracy 0.69 35094 macro avg 0.51 0.70 0.43 35094 weighted avg 0.98 0.69 0.80 35094 Metric Score: 0.7035868915429383 . The results are worse than just using PCA features and new features. . What if we just use cluster distances and new features? . # concatenate cluster labels to the clean dataset X = pd.concat([mailout_train_new_feats.reset_index(drop=True), mailout_distances], axis=1) # drop RESPONSE X = X.drop(columns=[&quot;RESPONSE&quot;]) model_validation(BalancedRandomForestClassifier(n_estimators=1000, random_state=seed), X, y) . Final Report: precision recall f1-score support 0.0 1.00 0.68 0.81 34658 1.0 0.03 0.82 0.06 436 accuracy 0.68 35094 macro avg 0.51 0.75 0.43 35094 weighted avg 0.98 0.68 0.80 35094 Metric Score: 0.7474211491200037 . I want to go more in depth with SupportVectorMachines, and look into their classification report with the original data. . We can see that the results have significantly improved, which indicates that the presence of all original features isn’t useful for the model. . So we might want to test automatic feature selection to see if we can . still use some of them and the PCA features to imporve the final results. . Automatic Feature Selection . There are several methods present in scikit-learn for feature selection, but I’ll use SelectKBest which removes all but the highest scoring K features. . Let’s just test SelectKBest on all of the features we had so far to see if it can get better results that our best trial so far (Recall 0.74) . # Concatenate all features X = pd.concat([mailout_train_pca, mailout_train_clean.reset_index(drop=True), mailout_distances, mailout_clusters], axis=1) # Drop RESPONSE X = X.drop(columns=[&quot;RESPONSE&quot;]) for k in [10, 30, 50, 100, 300, 500]: print(&quot;K:&quot;, k) # make pipeline pipeline = make_pipeline(SelectKBest(k=k), BalancedRandomForestClassifier(n_estimators=1000, random_state=seed)) model_validation(pipeline, X, y) print() . K: 10 Final Report: precision recall f1-score support 0.0 1.00 0.70 0.83 34657 1.0 0.03 0.83 0.07 436 accuracy 0.71 35093 macro avg 0.52 0.77 0.45 35093 weighted avg 0.99 0.71 0.82 35093 Metric Score: 0.7671317445321728 K: 30 Final Report: precision recall f1-score support 0.0 1.00 0.71 0.83 34657 1.0 0.03 0.82 0.07 436 accuracy 0.71 35093 macro avg 0.52 0.76 0.45 35093 weighted avg 0.98 0.71 0.82 35093 Metric Score: 0.7620556696904557 K: 50 Final Report: precision recall f1-score support 0.0 1.00 0.71 0.83 34657 1.0 0.03 0.80 0.06 436 accuracy 0.71 35093 macro avg 0.51 0.75 0.45 35093 weighted avg 0.98 0.71 0.82 35093 Metric Score: 0.7526066873716092 K: 100 Final Report: precision recall f1-score support 0.0 1.00 0.71 0.83 34657 1.0 0.03 0.79 0.06 436 accuracy 0.71 35093 macro avg 0.51 0.75 0.45 35093 weighted avg 0.98 0.71 0.82 35093 Metric Score: 0.7497644021549252 K: 300 Final Report: precision recall f1-score support 0.0 1.00 0.70 0.83 34657 1.0 0.03 0.77 0.06 436 accuracy 0.71 35093 macro avg 0.51 0.74 0.44 35093 weighted avg 0.98 0.71 0.82 35093 Metric Score: 0.7353716289811935 K: 500 Final Report: precision recall f1-score support 0.0 1.00 0.70 0.82 34657 1.0 0.03 0.79 0.06 436 accuracy 0.70 35093 macro avg 0.51 0.74 0.44 35093 weighted avg 0.98 0.70 0.81 35093 Metric Score: 0.744009369775735 . We can see that selecting that using the top 10 features is enough to improve the Macro Recall to 0.77. I’m still interested to see if we only use the feature selection on the PCA and old features only, while keeping the new feature and distances untouched. . pca_feats = list(mailout_train_pca.columns) azdias_feats = list(clean_azdias.columns) for k in [10, 30, 50]: print(&quot;K:&quot;, k) pipeline = Pipeline([ (&#39;feature_selection&#39;, ColumnTransformer([ (&#39;kbest&#39;, SelectKBest(k=k), pca_feats+azdias_feats), ], remainder=&#39;passthrough&#39;)), (&#39;clf&#39;, BalancedRandomForestClassifier(n_estimators=1000, random_state=seed)) ]) model_validation(pipeline, X, y) print() . K: 10 Final Report: precision recall f1-score support 0.0 1.00 0.70 0.82 34657 1.0 0.03 0.83 0.06 436 accuracy 0.70 35093 macro avg 0.52 0.77 0.44 35093 weighted avg 0.99 0.70 0.81 35093 Metric Score: 0.7663125665425543 K: 30 Final Report: precision recall f1-score support 0.0 1.00 0.70 0.83 34657 1.0 0.03 0.82 0.06 436 accuracy 0.71 35093 macro avg 0.52 0.76 0.45 35093 weighted avg 0.98 0.71 0.82 35093 Metric Score: 0.7617978118285937 K: 50 Final Report: precision recall f1-score support 0.0 1.00 0.71 0.83 34657 1.0 0.03 0.81 0.06 436 accuracy 0.71 35093 macro avg 0.51 0.76 0.45 35093 weighted avg 0.98 0.71 0.82 35093 Metric Score: 0.7555872211446162 . Now we know it’s better to just pass all features for automatic feature . selection, as the performance didn’t improve over the best score.¶ . Now I’ll make a function to document all of the steps we did for preparing the dataset, so we’ll be able to do the same in testing. . def prepare_mailout(df, p=0.5, test=False): &quot;&quot;&quot;Prepare MAILOUT training and testing dataset for ML Pipeline.&quot;&quot;&quot; # Set dropping threshold to 1.0 for test set if test: p = 1.0 # Clean the dataset df_clean = clean_dataset(df, p_row=p, p_col=p, keep_features=[&quot;KBA13_ANTG4&quot;]) # Drop RESPONSE if train set if test: y = None else: y = df_clean[&quot;RESPONSE&quot;] df_clean.drop(&quot;RESPONSE&quot;, axis=1, inplace=True) # Filter features used in train set only if test: train_feats = pickle.load(open(&quot;mailout_train_feats.pkl&quot;, &quot;rb&quot;)) df_clean = df_clean.loc[:, train_feats] else: train_feats = list(df_clean.columns) pickle.dump(train_feats, open(&quot;mailout_train_feats.pkl&quot;, &quot;wb&quot;)) # Missing values # Impute clean AZDIAS features using old imputer azdias_imputer = pickle.load(open(&quot;imputer.pkl&quot;, &quot;rb&quot;)) df_clean.loc[:, clean_azdias.columns] = azdias_imputer.transform(df_clean[clean_azdias.columns]) # Impute remaning features if test: # Load mailout imputer for test set mailout_imputer = pickle.load(open(&quot;mailout_imputer.pkl&quot;, &quot;rb&quot;)) df_clean = pd.DataFrame(mailout_imputer.transform(df_clean), columns=df_clean.columns) else: # Fit imputer for train set and pickle to load with test set mailout_imputer = SimpleImputer(strategy=&quot;mean&quot;) df_clean = pd.DataFrame(mailout_imputer.fit_transform(df_clean), columns=df_clean.columns) pickle.dump(mailout_imputer, open(&quot;mailout_imputer.pkl&quot;, &quot;wb&quot;)) # PCA features df_pre_pca = df_clean[clean_azdias.columns] df_pre_pca_scaled = scaler.transform(df_pre_pca) df_pca = pd.DataFrame(pca.transform(df_pre_pca_scaled)[:, :d], columns=[f&quot;pca_{i}&quot; for i in range(d)]) # Cluster distances df_distances = pd.DataFrame(kmeans.transform(df_pca), columns=[f&quot;cluster_{i}&quot; for i in range(kmeans.cluster_centers_.shape[0])]) # Cluster labels df_clusters = pd.Series(kmeans.predict(df_pca), name=&#39;label&#39;) # Concatenate all features X = pd.concat([df_clean, df_pca, df_distances, df_clusters], axis=1) return X, y . Hyperparameter Tuning The Final Pipeline . # Prepare training set X_train, y_train = prepare_mailout(mailout_train) pipeline = Pipeline([ (&#39;k_best&#39;,SelectKBest()), (&#39;clf&#39;, BalancedRandomForestClassifier(random_state=seed)) ]) pipeline_grid = { &quot;k_best__k&quot;: [int(x) for x in np.linspace(start=5, stop=30, num=5)], &quot;clf__n_estimators&quot;: [int(x) for x in np.linspace(start=1000, stop=3000, num=5)], &quot;clf__max_depth&quot;: [int(x) for x in np.linspace(10, 110, num = 5)] + [None], &quot;clf__min_samples_split&quot;: [2, 5, 10], &quot;clf__min_samples_leaf&quot;: [1, 2, 4], &quot;clf__bootstrap&quot;: [True, False], } combs = 1 for name, params in pipeline_grid.items(): combs *= len(params) print(&quot;Total number of combinations in parameters grid:&quot;, combs) . There 2700 different combinations to the hyperparamter grid that we have set. Therefore it is only logical to use randomized grid search, as we don’t have the computational or time resources to find the best combination by brute force. . # Instantiate StratifiedKFold object for CV skf = StratifiedKFold(n_splits=3) # Use RandomizedSearch to find the best hyperparameters combination pipeline_random = RandomizedSearchCV(estimator=pipeline, param_distributions=pipeline_grid, scoring=&#39;recall_macro&#39;, n_iter=50, cv=skf, verbose=3, random_state=seed, n_jobs=-1) # Fit the RandomizedSearch model to the training data pipeline_random.fit(X_train.values, y_train.values) . RandomizedSearchCV Results . Best parameters found: {&#39;k_best__k&#39;: 5, &#39;clf__n_estimators&#39;: 2500, &#39;clf__min_samples_split&#39;: 10, &#39;clf__min_samples_leaf&#39;: 2, &#39;clf__max_depth&#39;: 35, &#39;clf__bootstrap&#39;: True} . Checking metrics of final pipeline . pipeline.set_params(**best_params) model_validation(pipeline, X_train, y_train, final_results=True, plot_confusion=True) . Final Report: precision recall f1-score support 0 1.00 0.70 0.82 34657 1 0.03 0.84 0.07 436 accuracy 0.70 35093 macro avg 0.52 0.77 0.44 35093 weighted avg 0.99 0.70 0.81 35093 Metric Score: 0.7696068445499852 . . The results aren’t state of the art, but we can see that we are able to predict 84% of the responsive individuals, and only 30% of non-responsive ones are false positives. . Comparing the results to the baseline model, the Macro Recall has improved by 8.62% . Tuning The Decision Boundary . Now the final the thing that we can do is to tune the decision boundary to get the best results that we can get using this model. In order to do this, we need to predict the probability of classes first. . skf = StratifiedKFold(n_splits=3, shuffle=True, random_state=seed) # Make empty y_pred to fill predictions of each fold y_pred = np.zeros((y_train.shape[0], 2)) for i, (train_idx, test_idx) in enumerate(skf.split(X_train, y_train)): X_tr, X_test = X_train.iloc[train_idx], X_train.iloc[test_idx] y_tr, y_test = y_train.iloc[train_idx], y_train.iloc[test_idx] # Make copy of model model_clone = clone(pipeline) # Fit model model_clone.fit(X_tr, y_tr) # Predict fold y_test and add in y_pred fold_pred = model_clone.predict_proba(X_test) y_pred[test_idx, :] += fold_pred threshs = np.linspace(0, 1, 100) f1_scores = [] recall_scores = [] precision_scores = [] for thresh in threshs: thresh_pred = (y_pred[:, 1] &gt; thresh).astype(int) f1_scores.append(f1_score(y_train, thresh_pred, average=&quot;macro&quot;)) recall_scores.append(recall_score(y_train, thresh_pred, average=&quot;macro&quot;)) precision_scores.append(precision_score(y_train, thresh_pred, average=&quot;macro&quot;, zero_division=1)) best_recall_thresh = threshs[np.argmax(recall_scores)] print(&quot;Threshold optimizing Recall: {:.3f}&quot;.format(best_recall_thresh)) . Threshold optimizing Recall: 0.516 . . We can see that the best threshold slightly improved the precision of the model, by decreasing the number of false positives from 10327 to 10283. . Final Remarks . In each step we have done in the pipeline so far, there must have been other ways we could have achieved the same or even better results. But speaking from a business perspective, if this dataset provided the data one previous campaign were the conversion rate was minute as seen, using this model which isn’t particularly state of the art, would definitely decrease the costs and increase the conversion rate of the campaign due to the increased selectivity. . And now we can train the final model to predict the test set. . Training Final Model . pipeline.set_params(**best_params) pipeline.fit(X_train, y_train) . Test Set Prediction . # Reading test set mailout_test = pd.read_csv(&#39;../../data/Term2/capstone/arvato_data/Udacity_MAILOUT_052018_TEST.csv&#39;, sep=&#39;;&#39;) # Copying LNR column for Kaggle submission csv file test_LNR = mailout_test[&#39;LNR&#39;] # Data Preparation X_test, _ = prepare_mailout(mailout_test, test=True) # Prediction test_preds = pipeline.predict_proba(X_test) # Kaggle csv submission submission = pd.DataFrame({&#39;LNR&#39;:test_LNR.astype(np.int32), &#39;RESPONSE&#39;:test_preds[:, 1]}) submission.to_csv(&#39;kaggle.csv&#39;, index=False) . Conclusion . To summarize what we have done in the project so far: . We explored the general population dataset to understand how it should be cleaned for our analysis | We made a pipeline for cleaning the general population dataset and any dataset that has a similar structure | We performed dimensionality reduction on the general population dataset followed by a clustering analysis of the population | We cleaned the customers’ dataset using the pipeline we previously made and analyzed the clusters’ representation of the business’s customer base | We analyzed the characteristics of our customer base and how they differ from non-customers | We analyzed the differences between different clusters in the customer base | We explored the MAILOUT dataset and made a pipeline to prepare the dataset for the supervised learning task | We analyzed different algorithms and metrics then selected the best ones that suited the situation of the dataset we have which was Balanced Random Forests for the algorithm and Macro Recall for metric which deals with the target class imbalance | We tested using feature selection in the pipeline to improve the results and found that it had indeed improved it | We made a final pipeline and tuned it’s hyperparameters to predict individuals with high probability responding the mail-out campaign | Reflection . Cleaning the general population dataset was really challenging for me, as it was the first time I’ve ever dealt with dataset this size, and I didn’t know where to begin in exploring it. . This has forced me to find some methods to be able to digest the data in smaller portions to get a general idea about how it should be dealt with, like to into categories separately. . Improvement . I really enjoyed the customer segmentation part, even though it could be improved since we only tried reducing dimensionality using PCA and clustered using K-Means, where we could have used different algorithms for clustering such as DBSCAN, Agglomerative Clustering or Gaussian Mixture Models. . Anyways I have learnt much through out this project and the Nano-degree in general, and I hope that you have enjoyed my capstone project’s write up. . Thanks for reading. .",
            "url": "https://ahmedsamirio.github.io/blog/2021/09/25/Arvato_Capstone_Project.html",
            "relUrl": "/2021/09/25/Arvato_Capstone_Project.html",
            "date": " • Sep 25, 2021"
        }
        
    
  
    
        ,"post2": {
            "title": "Movie-Maps - A Webapp that Graphs Network of Characters in Movie Scripts",
            "content": ". The story of this project starts in the Udacity Data Scientist Nanodegree. The Nanodegree has a simple course about web development using Bootstrap, Flask and JQuery. . They provided an ungraded project in which you create a dashboard webapp using Flask that fetched data and visualizes it from the WorldBank API. . But I really wanted to do something unique in this project, and I thought that doing the project this way wouldn’t teach me anything worthwhile, so I kept digging and digging for ideas that might interest me and have the right mix of challenge and ease that won’t get me discouraged. . I had been interested in NLP for a while, but this interest didn’t produce any fruitful outcome until now, so I thought that this project should have some NLP in it. . And It finally occurred to me to analyze movie scripts and compare them with each other, specifically of my favorite directors Christopher Nolan and Quentin Tarantino, since they both write their movies’ screenplays. . So I started preparing for this project and searching for ways to get the data, and while exploring the things that can be done, the most interesting thing that I found was analyzing the network of interactions between the characters in the script. . A network graph consists of nodes that have various sizes, which are connected by edges. What I had in mind was to make a node for each character with its size varying according to the number of dialogues they have in the script, and to trace the edges between these nodes according to the number of interaction between these characters, and it’s illustrated in the following image. . . After successfully making the first network graph, I found that the method could generalize to any script (as all scripts almost have the same anatomy), and so I decided to start a stand-alone project which is a webapp that can show the character network of (almost) any movie. . The pipeline of the project consisted of the following steps: . Getting the data | Preparing an app that can make network graphs using movie scripts | Deploying the app on the web using heroku | Getting the data . This step was really easy as I found https://imsdb.com/ which is a database the contains numerous movie scripts in HTML format. . Using requests and BeautifulSoup I was able to: . Fetch and extract all movies relative links in imsdb | Loop over the movies and extract their script texts from HTML pages using BeautifulSoup | Save the scripts texts and saving a clean version of the movie name for further lookups | response = requests.get(&#39;https://imsdb.com/all-scripts.html&#39;) html = response.text soup = BeautifulSoup(html, &quot;html.parser&quot;) paragraphs = soup.find_all(&#39;p&#39;) movies_files = {} for p in paragraphs: relative_link = p.a[&#39;href&#39;] search_title, save_title, script = get_script(relative_link) if not script: continue # clean save title from any punctuations that prevent creating a filename save_title = save_title[:-5].translate(str.maketrans(&#39;&#39;, &#39;&#39;, string.punctuation)) + &#39;.txt&#39; script_file = os.path.join(SCRIPTS_DIR, save_title) # save script text with open(script_file, &#39;w&#39;, encoding=&#39;utf-8&#39;) as outfile: outfile.write(script) # save mapping of movie name with script text file movies_files[search_title] = script_file # save mapping to a picke file for easy loading movies_files_pkl = open(os.path.join(SCRIPTS_DIR, MOVIES), &#39;wb&#39;) pickle.dump(movies_files, movies_files_pkl) . For more information about cleaning the HTML you can check the github repo. . Preparing the visualization app . Now this part was the one that interested me the most, how could I extract characters interactions throughout the script? . I didn’t find a perfect answer, but I found one nonetheless. . Let’s take a look at the anatomy of a typical script. . . We can see that a character name is distinct as it’s always written in uppercase. We can also see that the only part of the script which is also written in uppercase is the scene heading. . So what if we split the script into sentences, then we looped over them and identified character names, then appended the following dialogue to them. . Using this approach we would be able to identify the dialogue belonging to each character in the script. And that’s exactly what I did. . The condition that I found out was most differentiating between scene headings and character names was the presence of dots and dashes in scene headings. So I used a regular expression that checks for the presence in this in the sentence, alongside being in upper case, to judge whether it’s a character name or not. . def extract_dialogues(script_sents): &quot;&quot;&quot;Function to extract dialogues from tokenized script sentences.&quot;&quot;&quot; pattern_compile = re.compile(r&#39;[.,–!:]&#39;) dialogues = [] add_dialogue = False for sent in script_sents: if not (sent.startswith(&#39;(&#39;) and sent.endswith(&#39;)&#39;)): if add_dialogue: if not (sent.strip().isupper() and not pattern_compile.search(sent)): dialogues.append((character, sent)) add_dialogue = False if (sent.strip().isupper() and not pattern_compile.search(sent)) and not (sent.strip().startswith(&#39;(&#39;) and sent.strip().endswith(&#39;)&#39;)): character = sent.strip() add_dialogue = True return dialogues . The next part was finding out through this extracted dialogue, who is talking to whom? . To be honest, I used the most simple approach that crossed my mind, and that was shifting the dialogue between characters one step backward, so this way if X said something followed by Y saying something, you’d get the X said something to Y, and if X said something after Y, you’d get that Y also said something to X. . text = open(movie_file, &#39;r&#39;).read() text = clean_script_text(text) sents = sent_tokenize_script(text) dialogue = extract_dialogues(sents) dialogue_df = pd.DataFrame(dialogue, columns=[&#39;character&#39;, &#39;text&#39;]) # Make list of dialogue exchanged dialogue_df[&#39;character_shifted&#39;] = dialogue_df.character.shift(-1) . But what if a scene ends with X saying something, then another scene starts with Z saying something, wouldn’t that mean that we’d get that X said something to Z? . That’s true, but it wouldn’t matter that much, because if X regularly interacted with Z throughout the whole script, a wrongly attributed interaction between them wouldn’t affect the results that much, and if they never interacted, then this wrongfully attributed interaction would be filtered out in the end of the visualization pipeline as we don’t want to show extremely rare interactions between characters, even if they were correct. . Also what if a scene ended with X, then another scene started with X? or what if a scene had X say two sentences back to back? . The easy solution to this was to drop any exchange from a character to itself all together, but still count it into their dialogue count as this would affect the sizes of their node in the network visualization. . # extract character pairs pairs = dialogue_df[[&#39;character&#39;, &#39;character_shifted&#39;]].values.tolist()[:-1] # remove dialogues from one character to themselves and sort exchanges pairs = [&#39;-&#39;.join(sorted(x)) for x in pairs if x[0] != x[1]] . Another shortcoming of this method is that it wouldn’t capture interaction between multiple characters, since it’s a linear approach that depends on which character speaks after another. . I originally had a solution for this which was to extract the scenes in the script, and use the appearances of characters together in scenes as interactions, but I honestly didn’t care that much to implement it as the current method yielded nice results. . Another important point was to remove characters which didn’t appear much throughout the script, as they cluttered the visualization and made it look bad for no reason. . # top characters lines count characters_lines = dialogue_df.character.value_counts() top_characters = characters_lines[characters_lines &gt; 5] # count exchanges pairs = pd.Series(pairs).value_counts() top_pairs = pairs[in_top_characters(pairs.index, top_characters)] . And now since I had information about the characters, and how many dialogues they have, and the interactions between these characters, I was ready to move on to making the network. . To remind you, a network graph consists of: . Nodes | Edges the connect nodes | In our case the nodes where the characters, and the size of these nodes was the dialogue count throughout the script. The edges where the interactions between these characters. . I used NetworkX and Plotly to create the graph using network graph. The code in here heavily relied on this repo. . def make_edge(x, y, text, width): &#39;&#39;&#39;Creates a scatter trace for the edge between x&#39;s and y&#39;s with given width Parameters - x : a tuple of the endpoints&#39; x-coordinates in the form, tuple([x0, x1, None]) y : a tuple of the endpoints&#39; y-coordinates in the form, tuple([y0, y1, None]) width: the width of the line Returns - An edge trace that goes between x0 and x1 with specified width. &#39;&#39;&#39; return go.Scatter(x=x, y=y, line=dict(width=width, color=&#39;cornflowerblue&#39;), hoverinfo=&#39;none&#39;, text=([text]), mode=&#39;lines&#39;) def get_network_traces(top_characters, top_pairs, pair_chars): dialogue = nx.Graph() # add node for each character for char, count in top_characters.iteritems(): if char in pair_chars: dialogue.add_node(char, size=count) # for each dialogue exchange between two character add an edge for pair, count in top_pairs.iteritems(): char1, char2 = pair.split(&#39;-&#39;) dialogue.add_edge(char1, char2, weight=count) # get positions for nodes pos_ = nx.spring_layout(dialogue) # for each edge make edge trace, append to list edge_trace = [] for edge in dialogue.edges(): if dialogue.edges()[edge][&#39;weight&#39;] &gt; 0: char1 = edge[0] char2 = edge[1] x0, y0 = pos_[char1] x1, y1 = pos_[char2] text = char1 + &#39;--&#39; + char2 + &#39;: &#39; + str(dialogue.edges()[edge][&#39;weight&#39;]) trace = make_edge([x0, x1, None], [y0, y1, None], text, 0.1*dialogue.edges()[edge][&#39;weight&#39;]**0.5) edge_trace.append(trace) # make a node trace node_trace = go.Scatter(x = [], y = [], text = [], textposition = &quot;top center&quot;, textfont_size = 10, mode = &#39;markers+text&#39;, hoverinfo = &#39;none&#39;, marker = dict(color = [], size = [], line = None, reversescale=True )) # For each node in dialogue, get the position and size and add to the node_trace for node in dialogue.nodes(): x, y = pos_[node] node_trace[&#39;x&#39;] += tuple([x]) node_trace[&#39;y&#39;] += tuple([y]) node_trace[&#39;marker&#39;][&#39;color&#39;] += tuple([&#39;cornflowerblue&#39;]) node_trace[&#39;marker&#39;][&#39;size&#39;] += tuple([dialogue.nodes()[node][&#39;size&#39;]**0.7]) node_trace[&#39;text&#39;] += tuple([&#39;&lt;b&gt;&#39; + node + &#39;&lt;/b&gt;&#39;]) return edge_trace, node_trace . Until I have created an app that can take a movie script and output a network graph that looks like this: . . Deploying the app using heroku . My knowledge about web development in general is really shallow, but using what I’ve learned in the small course provided by Udacity in the nanodegree and the template for their original project, I was able to hack together a web app that suited my needs. . I wanted the app to have a search bar with autocomplete which you can type the name of a movie in, and it would autocomplete based on the available scripts, with a button that loads the network graph on the same page. . The webapp is available at https://movie-maps.herokuapp.com/ . Final Remarks . I didn’t care much for design, as the perfect is definitely the enemy of the good here. . I definetly didn’t make a perfect application that correctly graphs interactions between characters in any movie script, and actually I didn’t aim to, nor do I think that it’s possible since there are many variations in any given script that reduces the ability of hard rules to infer such interactions. . That doesn’t mean that you can’t dissect a single movie script and graph the interactions in a very meticulous way, but it rather restricts any thought that this dissection would generalize to every other script out there, as evident by this web app. . This project is rather a fun way to look into the movies you love and see how they are different from each other using their network graphs, as some movies can have multiple storylines, which can be evident in their networks, like the movie “Babel” or “The Lord of The Rings: The Two Towers” . . And some movies can be really centralized around one character like “Thor: Ragnarok”. . . You can checkout the full code and understand more about deploying the webapp and running it in you environment in the github repo. . Thanks for reading. .",
            "url": "https://ahmedsamirio.github.io/blog/2021/08/09/Movie-Maps.html",
            "relUrl": "/2021/08/09/Movie-Maps.html",
            "date": " • Aug 9, 2021"
        }
        
    
  
    
        ,"post3": {
            "title": "Can You Still Get Hired as a Developer Without Having a CS Background?",
            "content": ". This question always bothers me when contemplating the career-shift I’m attempting to do. . And that’s why I started with this question in the first project of Udacity’s Data Scientist Nanodegree. . I chose to analyze the StackOverflow data from 2015 until 2020 to answer the question in the title and another 2 questions that also bother me, and they are: . How present are professional developers with no CS background in the market? | What non-degree education should a data scientist focus on? | Is job satisfaction related to salary? or are there other factors? | . How present are professional developers with no CS background in the market? . . To my surprise I found out that in the 2015 survey, developers with no CS background constitued about 41% of the respondents. . And this percentage remained the same through 2016, before the sudden drop of 2017. . When I examined why the percentage of professional respondents with no CS background decreased I found out the this was due to a continued increase in the number of CS fresh graduates which you can see in the next bar plot. . . Before 2017, the number of professional respondents with no CS background exceeded those with CS background, but we can see the shift that happened in 2017. . The number of CS fresh graduates suddenly increased, and kept increasing through out 2018, but by then the composition of the market stabilised with an average 25% of the professional respondents having no CS background. . So does this mean that it’s harder for developer with no CS background to break into the market? . Well, it’s not that easy to say from this data, because if the number of CS undergraduates is increasing, that means that there is an increasing demands of developers through new job oppurtunities. . And if the number of CS developers is increasing, and the number of non CS developers may not be increasing, or increasing but with a much smaller rate when compared to CS developers, then the percentage of non CS developers will remain decreasing in the market. . While it may actually be easier for them to get hired because of the abundance of job oppurtunities that the increasing amounts of CS developers can’t even fill. . So the picture might look like it’s getting harder, but what is actually happening is that CS developers are just more abundant than before, and non-CS developers are barely increasing. . What non-degree education should a data scientist focus on? . When I was first introduced to data science, I was torn between the two ways that people suggested for learning. . Some people suggested starting with books, online courses, part-time programs and mainly starting with the theoritical part. . While another groups insisted that the best way to learn data science is by doing, so pick a project to work on or participate in a compeitition in order to have something to show for the skills that companies need. . So first I wanted to look at the trends in data science education. . . Self-education . We can see the previous trends in alternative education dissolve into stable composition through 2017, 2018 and 2019. . Where earlier in 2015 and 2016 the self-taught data scientist was the prevelant one. Maybe that was due to the absence of other means of education, and that’s why it’s proportion kept decreasing every year with increasing educational sources. . MOOCs . MOOCs popularity fell down during 2016 and 2017, then it started gaining increasing again through 2018 and 2019. . On the job training . It siginificantly fell starting from 2017, and then stablizied. This might indicate a change in how data scientists were hired back then, as the data science sphere had data scientists who didn’t need much on the job training to learn how to do their jobs. . Open source contributions . The stability over the years signify that this method is only relevant to a certain percentage of data scientists who aren’t increasing or decreasing in the field, and maybe these are data scientsts who are more heavy on the software engineering side. . Hackathons and competitions . I originally thought that they would have increasing popularity over the years, but it seems that they are just as stable as open source contributions. . Part-time programs, Bootcamps and Industry Certifications . Part-time programs has gained popularity over the years, and that’s probably because they became more available, and more reliable. Also bootcamps and industry certifications popularity between respondents has zigzaged throughout the years. . And after that I wanted to understand how does each one relate to salary? . So what I did was aggregate the mean salary for the respondents who said they used each on of the education methods listed in the survey, and this is what I found out. . . From the get go we can see online classes and part-time programs are the last in line, which was kind of shocking for me because if we look into the percentage of each education method over the years, we see that the methods with the highest mean salaries aren’t that popular. . First of all we can see the education with the highest mean salary is Bootcamps. We know that bootcamps are expensive, and it makes sense that a developer with a high salary can get into a bootcamp. It might also mean that bootcamps set you up for better oppurtunities. But since there is great uncertainty to where the true mean lies, we can’t verify that and it seems to good to be true. . We can see right next to it is on-the-job training, which kind of makes sense as this indicates that data scientists in companies that pay top dollar would recieve training specific to the tools used within the company. . Then we have open source contributions, hackathons and coding competition right next to each other, with slightly decreasing means and slightly increasing uncertainty. This trio definetly speaks of the data scientists practical ability, and if you remember, their trends we going up, which indicates how important they are in determining the kind of job and salary you are going to land. . What seems interesting is that they are followed by industry certifications, and how uncertainty just increases beyond the last 3 educational methods. I think this speaks of how industry certifications aren’t really guranteed to get you a good salary. . And from there we how the remaining educational sources are rated, where self-education, online classes and part-time courses are in the bottom of the list. . So what can we learn from this? I think we can learn that one should focus the practical side more than the theoritical side. . Proving that you are competent through open source contributions, hackathons, competitions, projects portfolio (which is what you get from a bootcamp) are the way to go. . You should also utilize online and part-time courses, but no the point where they take up all of your time and you don’t have anything to show for it. . Is job satisfaction related to salary? or are there other factors? . I thought that there might be a relationship between salary and job satisfaction, and when I saw the next plot I laughed out loud. . . How can the median salary decrease when satisfaction decreases, but then increase when dissatisfaction decreases below neutral? . It paints a different picture of a developer who might have taken a new position at another company, or chosen a career path based on salary alone, and then got to the point of dissatisfaction with the job. . But what are these factors that make a developer dissatified with his job? . What are the challenges that he/she faces? . . The percentage of developers facing lack of support from management seems to decrease as satisfaction increases. Which indicates that lack of support from management defines a bad work experience, where the increase in answers from very dissatisfied developers differs from very satisfied by 53.5%. . Very satisfied developers list meetings as one the challenges of work, which means that they are more invloved, and this percentage of developers seeing this as a challenger decreases with decreased satisfaction, which doesn’t mean that meetings make employees satisfied per se, but means that dissatisfaction might be related to feeling left out from the work environment. . It also seems that very satisfied developers tend to have more non-work commitments than developers on the other side of the spectrum, with an increase of 49.5%. . Then we have toxic work environment, where very dissatified developers have a 56% increase over very satisfied developers. . What doesn’t seem to really matter is being tasked with non-development work, distracting work environment, inadequate access to tools, not enough people for the workload and time spent commuting. . Using these points we can illustrate a mental picture of a satisfied developer: . They tend to have some sort of competent management, and positive work envrionemnt. | They tend to have non-work commitments that challenge their work, which indicates that strive to achieve work-life balance. | They tend to be more involved in their work with their colleagues and other department through meetings, etc.. | . While unsatisfied developers have the following: . They tend to distrust management competence | They might not have good work-life balance, and might be actually overworked | They are more secluded in their work environment and feel out of the loop | . If you contemplate the difference between the two developers listed above, you’ll understand why the difference isn’t in salary. As the difference is related to the different experiences they both have in the job and their personal life. . Conclusion . We could sum up the findings in these three points: . It is still possible to get hire when you have no CS background, but it will be slightly challenging | In order to learn better as a developer or data scientist, you should give more time to practical education methods | Job dissatisfaction isn’t related to salary, but it is deeply intertwined between the work environment and your personal life | . You can check out the full analysis in this reposirtory. .",
            "url": "https://ahmedsamirio.github.io/blog/2021/06/20/StackOverflow-Analysis.html",
            "relUrl": "/2021/06/20/StackOverflow-Analysis.html",
            "date": " • Jun 20, 2021"
        }
        
    
  
    
        ,"post4": {
            "title": "How to Recite 1000 Quran Verses",
            "content": ". It was the end of Ramadan when I began thinking about how I shall address the habit of night prayers (qiyyam) after the holy month, and thinking to myself that I should specifiy a daily goal (werd). And then I remembered the hadith: . قال الرسول عليه الصلاة و السلام (من قامَ بعشرِ آياتٍ لم يُكتب منَ الغافلينَ، ومن قامَ بمائةِ آيةٍ كتبَ منَ القانتينَ، ومن قامَ بألفِ آيةٍ كتبَ منَ المقنطرينَ) الراوي: عبدالله بن عمرو المحدث: الألباني المصدر: صحيح أبي داود 1398 خلاصة حكم المحدث: صحيح . The Prophet (peace be upon him) said: If anyone prays at night reciting regularly ten verses, he will not be recorded among the negligent; if anyone prays at night and recites a hundred verses, he will be recorded among those who are obedient to Allah; and if anyone prays at night reciting one thousand verses, he will be recorded among those who receive huge rewards. . So then I contemplated how could one recite 1000 verses? I don’t remember any time during Ramadan where I could have reached that number. I talked about it with my wife and she said that she knew that this was possible, it was by reading the last two Juz’s of Quran that you would be able to finish the 1000 verses. . I thought that it was interesting, but I then remembered that I was a budding data scientist, and that I could easily answer this in a data-driven way. I challenged myself to find an easier way than this. . So this project is split into 4 parts: . Getting the data and processing it | Analyzing the data | Illustrating how to pray using either ways | Other Scenarios | Getting the data . I found this amazing github repo called quran-json which has quran text in JSON format, making it really easy to get the data that I had in mind. . The information that I wanted was all surahs, their total verses, words and characters. That’s why I downloaded a json file providing general info about each surah (it’s number, name, translation , revelation place and total verses) . url = &#39;https://unpkg.com/quran-json@latest/json/surahs.pretty.json&#39; response = urllib.request.urlopen(url) data = json.loads(response.read()) surahs = pd.DataFrame(data) . . url = &#39;https://unpkg.com/quran-json@latest/json/quran/text.pretty.json&#39; response = urllib.request.urlopen(url) data = json.loads(response.read()) text = pd.DataFrame(data) . . Analyzing the data . After aggregating the information that I needed, I set out to find the answer. In order to find the easiest way to pray with 1000 verses, I needed to find which surahs have the easiest verses to read, that is they contain the least amount of words or characters per verse. . The basic idea that I had was to rank the Surahs according to their word to verse, or char to verse ratio. After that I could figure out the cutoff point where the cumlative sum of the Surahs’ verses reaches 1000, then sort them according to their number in the Quran to find out how to read them. . But first we need to look at the baseline, in order to compare it with the new ways we are trying to find. And I summarized it into the following: . Total number of Surahs: 48 Total number of Verses: 995 Total number of Words: 5161 Total number of Characters: 23161 . Since we read Al-Fatihah at least one time in Qiyyam (In a one rakaa’ wetr prayer), that would decrease 14 verses from the total 1000 verses. Meaning the we don’t more than 993 verses to complete 1000 verses including the verses of Al-Fatihah. . Now that we have our baseline, let’s take a look into what the data dictates. . . We can see the word to verse ratio in all Surahs is bimodal. We can see that the a great proportion of the Surahs have low word to verse ratio. So how many of them would it take to complete 1000 verses if I we ranked the Surahs by word to verse in ascending order? . . So using this information, we know that it would take 34 surahs to complete 1000 verses. But Surahs in here are sorted by the word to verse ratio, how would we actually read them if they were sorted by their number? . . So by following this you wouldn’t start at Al-Mulk, but rather at As-Saaffaat, and then skipping to Ar-Rahmaan and Al-Waaqia, then skip to Al-Muddaththir and recite until the end while skipping some Surahs that have high word to verse ratio. . Summarizing this way like the old way would yield us: Total number of Surahs: 34 Total number of Verses: 1005 Total number of Words: 4272 Total number of Characters: 19764 . Word count difference with Baseline: 889 Character count difference with Baseline: 3397 . To better understand how does this difference fair out, let’s find out how many pages of Juz’ Amma (جزء عم) this way cuts out. . juz_amma = surahs.query(&#39;number &gt;= 78&#39;) page_word_count = juz_amma.total_words.sum() / 23 words_saved = data_old.total_words.sum() - first_way.total_words.sum() pages_saved = words_saved/page_word_count . And it turns out that the number of pages from Juz Amma this way saves is 8.325325732899023. But this bothered me, as I didn’t want to be always on the lookout for Surahs I was going to skip, whereas the old way provided a slick way to just go through the last two Juz’s, was there a way to find a middle ground? . I figured out that I should stick with the most important part, which are the parts that finish the bulk of the verses. The first 3 Surahs (As-Saaffaat, Ar-Rahmaan and Al-Waaqia) completed 356 verses, which is around 35% of the way. If we recited them and started reciting without any skips from Al-Muddaththir, when could we get the 1000 verses? . . Total number of Surahs: 29 Total number of Verses: 1006 Total number of Words: 4495 Total number of Characters: 20831 . Word count difference with Baseline: 666 Character count difference with Baseline: 2330 . he number of pages from Juz Amma this way saves is 6.236970684039088 . By comparing the character difference to Surahs char count, we find that the difference between the baseline in the new way which is similar in simplicity to it is around 6.25 pages of Juz’ Amma. . Now there are 2 choices for, so you either go with the easiest way to read 1000 verses which saves you around 8.25 pages, or you go with a still easy way that saves you 6.25 pages, or you go with the old way by reading the two Juz’s back to back. . . This question wasn’t intended as a call for laziness, afterall if you intended to pray with 1000 verses how dare anyone call you lazy, but it’s a way to decrease any deterrents or obstacles between intending it and actually achieving it. . Illustrating how to pray using either ways . How would you pray 2, 4 or 8 Rakaas using the easiest way? . I’ve illustrated how you would do that exactly in the next plots . . . . How would you pray 2, 4 or 8 Rakaas using the second easiest way? . . . . Other Scenarios . Now you might have started reading somewhere else, and you want to continue reciting a specific number of verses, but you are still looking for an easy way to do it. That’s why I’ve the following charts. . . . . Now you have all the tools to recite those 1000 verses, I pray that we’re all blessed to be from those who did this. . I hope that this post inspires those who read it to work on projects that they find interesting, and ask questions that can possibly optimize whatever good deeds they maybe doing. . Project Notebook: https://github.com/ahmedsamirio/1000-quran-verses .",
            "url": "https://ahmedsamirio.github.io/blog/2021/05/21/Recite-1000-Verses.html",
            "relUrl": "/2021/05/21/Recite-1000-Verses.html",
            "date": " • May 21, 2021"
        }
        
    
  
    
        ,"post5": {
            "title": "How I built a Twitter crawler",
            "content": ". I was curious to use twitter data for a project I had in mind, and basically it was a answering a bunch of question regarding the behavior of users on twitter and how that is correlated with their followers’ count. But when I got access to the developer’s API, I was lost on how to get the data that I want. . Of course there are a bunch of packages like Tweepy, Python-twitter, ptt (Python twitter tools) and even Twint (which doesn’t require access through Twitter’s API) which you can use to get data from twitter, but what kind of data did I want? . What I had in mind was to capture the recent behavior of a random group of users through mining their tweets, but the Twitter API requires searching by specific queries or hashtags, etc.. . What I wanted was a random sample of users that isn’t affected by users tweeting about a certain topic, and that’s when a thought occurred to me. Why don’t I select a bunch of seed users, collect their latest tweets and then select a random subset of their friends and followers and do the same? . So this is just randomly traversing over a tree of users branched from a seed user until a certain depth, not unlike a binary search tree. . . We can see an example of a traversed tree from a seed user, let’s suppose d stands for a the level of a given layer, where the first layer’s level equals 0, the maximum depth of layers l is 3, the new users recursed over from a seed user is n is 2, and the tweets to collect is t is 1. If user A is the seed user, then we collect the latest tweet and select 2 random users from his friends and followers, which turned out to be B and C. . Then for each of these users we do the same as user A. This results for a total of n^d users for each layer where the starting layer d is 0, therefore the total number of users and tweets collected equals 2^3 + 2^2 + 2^1 + 1 = 15 users and 15 tweets. . def stream_from_users(seed_user, tweets_per_user, users_per_user, depth): tweets = collect_tweets(seed_user) users = collect_random_users(seed_user) if depth &lt; limit_depth: for user in random_users: stream_from_users(seed_user, tweets_per_user, users_per_user, depth+1) . This code snippet would serve to be the backbone of mining this data. But then I was faced with another question, which package should I use to get the data? and how shall I store it? . What I had in mind was starting with 3 seed users, and then recursing over them for 2 levels down, with 100 users per user and collecting 200 tweets per user (which is the limit of one request per user). So that would result in an estimated 30K users and 6 million tweets, how could I efficiently store this data? . I thought SQLite was the answer, but it turned out that I couldn’t do bulk inserts of up to 200 tweets into a table, where I had to loop over every collected tweet and store it which was pretty inefficient. I ended up using MongoDB, which was extremely useful in that matter, as it enabled bulk inserts of tweets in JSON format directly. . And then came the next hurdle, using Tweepy or Python-twitter was really easy, but when used for mining tweets they returned Status objects, where each object had a JSON attribute, but the Status object itself wasn’t supported for bulk inserts in MongoDB, so I would have had to loop over each object and save it’s JSON into the database, just like SQLite. And that’s why I used ptt, as it directly returned the tweets in JSON format, enabling bulk inserts. . Now it was just a matter of running the script, and then preparing the data in a format which enables answering the questions the I had in mind, which will be reserved for another post. . To make a dataset of your own, you can use the script provided in the following repo. Github repo: https://github.com/ahmedsamirio/twitter-crawler .",
            "url": "https://ahmedsamirio.github.io/blog/2021/05/11/Twitter-Crawler.html",
            "relUrl": "/2021/05/11/Twitter-Crawler.html",
            "date": " • May 11, 2021"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "A budding data scientist, currently focused on deep learning (with even more focus on computer vision). . I’ve won a silver medal in UW-Madison GI Tract Segmentation competition and implemented some general projects in various domains while building my knowledge base. . I’m originally a pharmacist, so I still find myself more prone to work on competitions and problems rooted in the medical domain, but I’m open to work in any domain that I find interesting. . Contact me . ahmedsamirio95@gmail.com . This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://ahmedsamirio.github.io/blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://ahmedsamirio.github.io/blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}